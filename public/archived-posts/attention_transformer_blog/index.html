<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Transformers from Scratch: Implementing Attention Is All You Need | Anoop Maurya</title><meta name=keywords content="transformers,pytorch,nlp"><meta name=description content="Beginner-to-intermediate guide to the Transformer with PyTorch code: attention, multi-head attention, positional encoding, and a minimal encoder."><meta name=author content="Anoop Maurya"><link rel=canonical href=https://imanoop7.github.io/posts/attention_transformer_blog/><link crossorigin=anonymous href=/assets/css/stylesheet.36ae83e8569c7202b54e5e59985704e8f7cd8e356cc7148f2bddbe48acc8275c.css integrity="sha256-Nq6D6FaccgK1Tl5ZmFcE6PfNjjVsxxSPK92+SKzIJ1w=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/archived-posts/attention_transformer_blog/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="http://localhost:1313/archived-posts/attention_transformer_blog/"><meta property="og:site_name" content="Anoop Maurya"><meta property="og:title" content="Transformers from Scratch: Implementing Attention Is All You Need"><meta property="og:description" content="Beginner-to-intermediate guide to the Transformer with PyTorch code: attention, multi-head attention, positional encoding, and a minimal encoder."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="archived-posts"><meta property="article:published_time" content="2025-09-15T12:00:00+00:00"><meta property="article:modified_time" content="2025-09-15T12:00:00+00:00"><meta property="article:tag" content="Transformers"><meta property="article:tag" content="Pytorch"><meta property="article:tag" content="Nlp"><meta name=twitter:card content="summary"><meta name=twitter:title content="Transformers from Scratch: Implementing Attention Is All You Need"><meta name=twitter:description content="Beginner-to-intermediate guide to the Transformer with PyTorch code: attention, multi-head attention, positional encoding, and a minimal encoder."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Archived Posts","item":"http://localhost:1313/archived-posts/"},{"@type":"ListItem","position":2,"name":"Transformers from Scratch: Implementing Attention Is All You Need","item":"http://localhost:1313/archived-posts/attention_transformer_blog/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Transformers from Scratch: Implementing Attention Is All You Need","name":"Transformers from Scratch: Implementing Attention Is All You Need","description":"Beginner-to-intermediate guide to the Transformer with PyTorch code: attention, multi-head attention, positional encoding, and a minimal encoder.","keywords":["transformers","pytorch","nlp"],"articleBody":"Transformers have transformed the field of sequence modeling by replacing recurrent nets with self-attention. I was curious about how this works under the hood, so I dove into the “Attention Is All You Need” paper and built the components from scratch in PyTorch. In this post I share my understanding and hands-on insights. I’ll cover the motivation behind Transformers, break down the core ideas, and show PyTorch snippets for the key modules: scaled dot-product attention, multi-head attention, positional encoding, and an encoder layer.\nAlong the way I’ll mention a few hiccups and “aha!” moments I had – hopefully making this journey relatable to anyone wrapping their head around this for the first time.\nMotivation Before Transformers, sequence models like RNNs or LSTMs were standard for machine translation or language modeling. These models process data step-by-step, which is hard to parallelize and can struggle with long-range dependencies. The Transformer relies entirely on attention, allowing information from all time steps to be used at once.\nParallelism and efficiency: Self-attention lets us process all tokens at once, so training is much faster on modern hardware. Long-range context: Every token can attend to any other token, capturing long-distance relationships without vanishing gradients. Simplicity of components: The building blocks (attention, feed-forward layers, etc.) are relatively simple operations—feasible to code and inspect directly. Practical success: Many cutting-edge models (BERT, GPT) are based on Transformers, so learning it deeply pays off. Core Concepts The Transformer encoder is built from a few key ideas:\nScaled Dot-Product Attention: Computes attention scores between queries and keys, scales them, and uses softmax to weight values. Multi-Head Attention: Runs several attention “heads” in parallel so the model can jointly attend to different representation subspaces. Positional Encoding: Injects information about token positions, since attention alone is order-agnostic. Feed-Forward Network: A two-layer MLP applied to each position separately and identically. Add \u0026 Norm (Residual + LayerNorm): Residual connections and layer normalization for stable training and easy gradient flow. Stacking several encoder layers (typically 6–12) gives you a Transformer encoder.\nScaled Dot-Product Attention Given queries Q, keys K, and values V, scaled dot-product attention computes a weighted sum of the values, where weights come from the similarity of queries with keys. The scores are divided by (\\sqrt{d_k}) to keep values in a reasonable range.\nHigh level steps for a single query (\\mathbf{q}) and keys (\\mathbf{k}_j):\nCompute raw scores: (\\text{score}_j = \\mathbf{q} \\cdot \\mathbf{k}_j^\\top) Scale: (\\text{score}_j / \\sqrt{d_k}) Optionally mask some scores to (-\\infty) Softmax to probabilities: (\\alpha_j) Weighted sum of values: (\\sum_j \\alpha_j \\mathbf{v}_j) PyTorch implementation:\nimport torch import math def scaled_dot_product_attention(Q, K, V, mask=None): \"\"\" Q, K, V: shape (batch, seq_len, d_k) mask: optional mask tensor of shape (batch, seq_len_q, seq_len_k) Returns: (output, attention_weights) \"\"\" d_k = Q.size(-1) # Compute scaled dot products scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k) # Apply the mask (if provided) by setting scores to large negative where mask==0 if mask is not None: scores = scores.masked_fill(mask == 0, float('-inf')) # Softmax to get attention weights attn_weights = torch.softmax(scores, dim=-1) # Multiply by values output = torch.matmul(attn_weights, V) return output, attn_weights Here, scores has shape (batch, seq_q, seq_k) after the matmul. Multiplying by V then gives an output of shape (batch, seq_q, d_k). For masks (e.g., padding or causal), use masked_fill(-inf) before softmax so those positions get effectively zero weight.\nMulti-Head Attention One attention head gives one context representation. Multi-head attention runs several heads in parallel with separate linear projections, letting each head focus on different aspects (e.g., local vs. long-range patterns).\nIf d_model is the model dimension and we have h heads, each head uses d_k = d_model / h. There are learned projections Wq, Wk, Wv and an output projection Wo.\nimport torch import torch.nn as nn class MultiHeadAttention(nn.Module): def __init__(self, d_model, num_heads): super().__init__() assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\" self.num_heads = num_heads self.d_k = d_model // num_heads # Linear layers to project inputs to queries, keys, values self.Wq = nn.Linear(d_model, d_model) self.Wk = nn.Linear(d_model, d_model) self.Wv = nn.Linear(d_model, d_model) # Output linear layer self.Wo = nn.Linear(d_model, d_model) def forward(self, Q, K, V, mask=None): batch_size = Q.size(0) # 1) Linear projections Q = self.Wq(Q) # (batch, seq_len, d_model) K = self.Wk(K) V = self.Wv(V) # 2) Split into heads by reshaping Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) # Now Q, K, V are (batch, heads, seq_len, d_k) # 3) Apply scaled dot-product attention on each head # Combine batch and heads dims for efficiency Q_flat = Q.reshape(-1, Q.size(2), self.d_k) # (batch*heads, seq_q, d_k) K_flat = K.reshape(-1, K.size(2), self.d_k) V_flat = V.reshape(-1, V.size(2), self.d_k) if mask is not None: # Expand mask to match the number of heads mask = mask.unsqueeze(1).repeat(1, self.num_heads, 1, 1) mask = mask.reshape(-1, mask.size(-2), mask.size(-1)) context, attn_weights = scaled_dot_product_attention(Q_flat, K_flat, V_flat, mask) # 4) Concatenate heads context = context.view(batch_size, self.num_heads, -1, self.d_k) context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k) # 5) Final linear layer output = self.Wo(context) # (batch, seq_len, d_model) return output, attn_weights The tensor reshaping is the trickiest part. After splitting into heads, I flatten (batch, heads) to reuse scaled_dot_product_attention, then reshape back and concatenate. Use .contiguous() before .view after a transpose.\nPositional Encoding Pure attention has no notion of order. We add positional encodings to token embeddings. The original paper used fixed sinusoidal encodings:\n[\\text{PE}(pos, 2i) = \\sin\\bigl(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\bigr),\\quad \\text{PE}(pos, 2i+1) = \\cos\\bigl(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\bigr)]\nimport torch import torch.nn as nn import math class PositionalEncoding(nn.Module): def __init__(self, d_model, max_len=5000): super().__init__() pe = torch.zeros(max_len, d_model) position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # (max_len, 1) div_term = torch.exp(torch.arange(0, d_model, 2).float() * (math.log(10000.0) / d_model)) pe[:, 0::2] = torch.sin(position * div_term) # even dims pe[:, 1::2] = torch.cos(position * div_term) # odd dims pe = pe.unsqueeze(0) # (1, max_len, d_model) # Register as buffer so it’s saved with the model but not trainable self.register_buffer('pe', pe) def forward(self, x): # x: (batch, seq_len, d_model) seq_len = x.size(1) return x + self.pe[:, :seq_len, :] Shaping pe as (1, max_len, d_model) lets it broadcast-add to a batch of embeddings. Register it as a buffer so it moves with model.to(device) but isn’t updated by the optimizer.\nPutting It Together: Encoder Layer Each encoder layer does:\nSelf-Attention + Add \u0026 Norm: x \u003c- LayerNorm(x + MHA(x, x, x)) Feed-Forward + Add \u0026 Norm: x \u003c- LayerNorm(x + FFN(x)) import torch.nn as nn class EncoderLayer(nn.Module): def __init__(self, d_model, num_heads, dim_ff, dropout=0.1): super().__init__() self.self_attn = MultiHeadAttention(d_model, num_heads) self.norm1 = nn.LayerNorm(d_model) self.norm2 = nn.LayerNorm(d_model) self.dropout = nn.Dropout(dropout) # Feed-forward network (position-wise) self.fc1 = nn.Linear(d_model, dim_ff) self.fc2 = nn.Linear(dim_ff, d_model) def forward(self, x, mask=None): # 1) Multi-head self-attention attn_output, _ = self.self_attn(x, x, x, mask) x = x + self.dropout(attn_output) # Residual x = self.norm1(x) # 2) Feed-forward network ff_output = self.fc2(self.dropout(torch.relu(self.fc1(x)))) x = x + self.dropout(ff_output) # Residual x = self.norm2(x) return x If x is (batch, seq_len, d_model), the module preserves shapes. Dropout is applied after attention and inside the feed-forward path.\nReflections Understand by implementing: Writing code forces you to handle details (tensor shapes, masking). Quick dummy tests are invaluable. Scaling matters: Dividing by (\\sqrt{d_k}) keeps softmax well-behaved; omitting it caused overly peaky distributions in my tests. Reshaping is tricky but logical: Splitting/combining heads is the most error-prone part; draw shapes while coding. Positional encoding is neat: The sine/cosine patterns give a smooth notion of distance; the model infers relative order. Modularity: Once MultiHeadAttention and EncoderLayer work, stacking them is straightforward and encourages experimentation. Overall, the magic is that self-attention lets each position flexibly gather information from anywhere else, and multi-head attention enriches that process. If you’re learning this, don’t just read the paper—try coding the pieces. Small confusions often turn into big insights once resolved.\n","wordCount":"1284","inLanguage":"en","datePublished":"2025-09-15T12:00:00Z","dateModified":"2025-09-15T12:00:00Z","author":{"@type":"Person","name":"Anoop Maurya"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/archived-posts/attention_transformer_blog/"},"publisher":{"@type":"Organization","name":"Anoop Maurya","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Anoop Maurya (Alt + H)">Anoop Maurya</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/ title=Posts><span>Posts</span></a></li><li><a href=http://localhost:1313/archives/ title=Archives><span>Archives</span></a></li><li><a href=http://localhost:1313/tags/ title=Tags><span>Tags</span></a></li><li><a href=http://localhost:1313/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=http://localhost:1313/faq/ title=FAQ><span>FAQ</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/archived-posts/>Archived Posts</a></div><h1 class="post-title entry-hint-parent">Transformers from Scratch: Implementing Attention Is All You Need</h1><div class=post-description>Beginner-to-intermediate guide to the Transformer with PyTorch code: attention, multi-head attention, positional encoding, and a minimal encoder.</div><div class=post-meta><span title='2025-09-15 12:00:00 +0000 +0000'>September 15, 2025</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;1284 words&nbsp;·&nbsp;Anoop Maurya</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><ul><li><a href=#motivation>Motivation</a></li><li><a href=#core-concepts>Core Concepts</a></li><li><a href=#scaled-dot-product-attention>Scaled Dot-Product Attention</a></li><li><a href=#multi-head-attention>Multi-Head Attention</a></li><li><a href=#positional-encoding>Positional Encoding</a></li><li><a href=#putting-it-together-encoder-layer>Putting It Together: Encoder Layer</a></li><li><a href=#reflections>Reflections</a></li></ul></li></ul></nav></div></details></div><div class=post-content><p>Transformers have transformed the field of sequence modeling by replacing recurrent nets with self-attention. I was curious about how this works under the hood, so I dove into the “Attention Is All You Need” paper and built the components from scratch in PyTorch. In this post I share my understanding and hands-on insights. I’ll cover the motivation behind Transformers, break down the core ideas, and show PyTorch snippets for the key modules: scaled dot-product attention, multi-head attention, positional encoding, and an encoder layer.</p><p>Along the way I’ll mention a few hiccups and “aha!” moments I had – hopefully making this journey relatable to anyone wrapping their head around this for the first time.</p><h3 id=motivation>Motivation<a hidden class=anchor aria-hidden=true href=#motivation>#</a></h3><p>Before Transformers, sequence models like RNNs or LSTMs were standard for machine translation or language modeling. These models process data step-by-step, which is hard to parallelize and can struggle with long-range dependencies. The Transformer relies entirely on attention, allowing information from all time steps to be used at once.</p><ul><li><strong>Parallelism and efficiency</strong>: Self-attention lets us process all tokens at once, so training is much faster on modern hardware.</li><li><strong>Long-range context</strong>: Every token can attend to any other token, capturing long-distance relationships without vanishing gradients.</li><li><strong>Simplicity of components</strong>: The building blocks (attention, feed-forward layers, etc.) are relatively simple operations—feasible to code and inspect directly.</li><li><strong>Practical success</strong>: Many cutting-edge models (BERT, GPT) are based on Transformers, so learning it deeply pays off.</li></ul><h3 id=core-concepts>Core Concepts<a hidden class=anchor aria-hidden=true href=#core-concepts>#</a></h3><p>The Transformer encoder is built from a few key ideas:</p><ul><li><strong>Scaled Dot-Product Attention</strong>: Computes attention scores between queries and keys, scales them, and uses softmax to weight values.</li><li><strong>Multi-Head Attention</strong>: Runs several attention “heads” in parallel so the model can jointly attend to different representation subspaces.</li><li><strong>Positional Encoding</strong>: Injects information about token positions, since attention alone is order-agnostic.</li><li><strong>Feed-Forward Network</strong>: A two-layer MLP applied to each position separately and identically.</li><li><strong>Add & Norm (Residual + LayerNorm)</strong>: Residual connections and layer normalization for stable training and easy gradient flow.</li></ul><p>Stacking several encoder layers (typically 6–12) gives you a Transformer encoder.</p><h3 id=scaled-dot-product-attention>Scaled Dot-Product Attention<a hidden class=anchor aria-hidden=true href=#scaled-dot-product-attention>#</a></h3><p>Given queries Q, keys K, and values V, scaled dot-product attention computes a weighted sum of the values, where weights come from the similarity of queries with keys. The scores are divided by (\sqrt{d_k}) to keep values in a reasonable range.</p><p>High level steps for a single query (\mathbf{q}) and keys (\mathbf{k}_j):</p><ol><li>Compute raw scores: (\text{score}_j = \mathbf{q} \cdot \mathbf{k}_j^\top)</li><li>Scale: (\text{score}_j / \sqrt{d_k})</li><li>Optionally mask some scores to (-\infty)</li><li>Softmax to probabilities: (\alpha_j)</li><li>Weighted sum of values: (\sum_j \alpha_j \mathbf{v}_j)</li></ol><p>PyTorch implementation:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> math
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>scaled_dot_product_attention</span>(Q, K, V, mask<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Q, K, V: shape (batch, seq_len, d_k)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    mask: optional mask tensor of shape (batch, seq_len_q, seq_len_k)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Returns: (output, attention_weights)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    d_k <span style=color:#f92672>=</span> Q<span style=color:#f92672>.</span>size(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    <span style=color:#75715e># Compute scaled dot products</span>
</span></span><span style=display:flex><span>    scores <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>matmul(Q, K<span style=color:#f92672>.</span>transpose(<span style=color:#f92672>-</span><span style=color:#ae81ff>2</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)) <span style=color:#f92672>/</span> math<span style=color:#f92672>.</span>sqrt(d_k)
</span></span><span style=display:flex><span>    <span style=color:#75715e># Apply the mask (if provided) by setting scores to large negative where mask==0</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> mask <span style=color:#f92672>is</span> <span style=color:#f92672>not</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>        scores <span style=color:#f92672>=</span> scores<span style=color:#f92672>.</span>masked_fill(mask <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>, float(<span style=color:#e6db74>&#39;-inf&#39;</span>))
</span></span><span style=display:flex><span>    <span style=color:#75715e># Softmax to get attention weights</span>
</span></span><span style=display:flex><span>    attn_weights <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>softmax(scores, dim<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    <span style=color:#75715e># Multiply by values</span>
</span></span><span style=display:flex><span>    output <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>matmul(attn_weights, V)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> output, attn_weights
</span></span></code></pre></div><p>Here, <code>scores</code> has shape <code>(batch, seq_q, seq_k)</code> after the matmul. Multiplying by <code>V</code> then gives an output of shape <code>(batch, seq_q, d_k)</code>. For masks (e.g., padding or causal), use <code>masked_fill(-inf)</code> before softmax so those positions get effectively zero weight.</p><h3 id=multi-head-attention>Multi-Head Attention<a hidden class=anchor aria-hidden=true href=#multi-head-attention>#</a></h3><p>One attention head gives one context representation. Multi-head attention runs several heads in parallel with separate linear projections, letting each head focus on different aspects (e.g., local vs. long-range patterns).</p><p>If <code>d_model</code> is the model dimension and we have <code>h</code> heads, each head uses <code>d_k = d_model / h</code>. There are learned projections <code>Wq, Wk, Wv</code> and an output projection <code>Wo</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>MultiHeadAttention</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self, d_model, num_heads):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span><span style=color:#a6e22e>__init__</span>()
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>assert</span> d_model <span style=color:#f92672>%</span> num_heads <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>, <span style=color:#e6db74>&#34;d_model must be divisible by num_heads&#34;</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>num_heads <span style=color:#f92672>=</span> num_heads
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>d_k <span style=color:#f92672>=</span> d_model <span style=color:#f92672>//</span> num_heads
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Linear layers to project inputs to queries, keys, values</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>Wq <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(d_model, d_model)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>Wk <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(d_model, d_model)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>Wv <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(d_model, d_model)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Output linear layer</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>Wo <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(d_model, d_model)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, Q, K, V, mask<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>        batch_size <span style=color:#f92672>=</span> Q<span style=color:#f92672>.</span>size(<span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 1) Linear projections</span>
</span></span><span style=display:flex><span>        Q <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>Wq(Q)  <span style=color:#75715e># (batch, seq_len, d_model)</span>
</span></span><span style=display:flex><span>        K <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>Wk(K)
</span></span><span style=display:flex><span>        V <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>Wv(V)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 2) Split into heads by reshaping</span>
</span></span><span style=display:flex><span>        Q <span style=color:#f92672>=</span> Q<span style=color:#f92672>.</span>view(batch_size, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, self<span style=color:#f92672>.</span>num_heads, self<span style=color:#f92672>.</span>d_k)<span style=color:#f92672>.</span>transpose(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>        K <span style=color:#f92672>=</span> K<span style=color:#f92672>.</span>view(batch_size, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, self<span style=color:#f92672>.</span>num_heads, self<span style=color:#f92672>.</span>d_k)<span style=color:#f92672>.</span>transpose(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>        V <span style=color:#f92672>=</span> V<span style=color:#f92672>.</span>view(batch_size, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, self<span style=color:#f92672>.</span>num_heads, self<span style=color:#f92672>.</span>d_k)<span style=color:#f92672>.</span>transpose(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>        <span style=color:#75715e># Now Q, K, V are (batch, heads, seq_len, d_k)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 3) Apply scaled dot-product attention on each head</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Combine batch and heads dims for efficiency</span>
</span></span><span style=display:flex><span>        Q_flat <span style=color:#f92672>=</span> Q<span style=color:#f92672>.</span>reshape(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, Q<span style=color:#f92672>.</span>size(<span style=color:#ae81ff>2</span>), self<span style=color:#f92672>.</span>d_k)  <span style=color:#75715e># (batch*heads, seq_q, d_k)</span>
</span></span><span style=display:flex><span>        K_flat <span style=color:#f92672>=</span> K<span style=color:#f92672>.</span>reshape(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, K<span style=color:#f92672>.</span>size(<span style=color:#ae81ff>2</span>), self<span style=color:#f92672>.</span>d_k)
</span></span><span style=display:flex><span>        V_flat <span style=color:#f92672>=</span> V<span style=color:#f92672>.</span>reshape(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, V<span style=color:#f92672>.</span>size(<span style=color:#ae81ff>2</span>), self<span style=color:#f92672>.</span>d_k)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> mask <span style=color:#f92672>is</span> <span style=color:#f92672>not</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>            <span style=color:#75715e># Expand mask to match the number of heads</span>
</span></span><span style=display:flex><span>            mask <span style=color:#f92672>=</span> mask<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>1</span>)<span style=color:#f92672>.</span>repeat(<span style=color:#ae81ff>1</span>, self<span style=color:#f92672>.</span>num_heads, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>            mask <span style=color:#f92672>=</span> mask<span style=color:#f92672>.</span>reshape(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, mask<span style=color:#f92672>.</span>size(<span style=color:#f92672>-</span><span style=color:#ae81ff>2</span>), mask<span style=color:#f92672>.</span>size(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        context, attn_weights <span style=color:#f92672>=</span> scaled_dot_product_attention(Q_flat, K_flat, V_flat, mask)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 4) Concatenate heads</span>
</span></span><span style=display:flex><span>        context <span style=color:#f92672>=</span> context<span style=color:#f92672>.</span>view(batch_size, self<span style=color:#f92672>.</span>num_heads, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, self<span style=color:#f92672>.</span>d_k)
</span></span><span style=display:flex><span>        context <span style=color:#f92672>=</span> context<span style=color:#f92672>.</span>transpose(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>)<span style=color:#f92672>.</span>contiguous()<span style=color:#f92672>.</span>view(batch_size, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, self<span style=color:#f92672>.</span>num_heads <span style=color:#f92672>*</span> self<span style=color:#f92672>.</span>d_k)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 5) Final linear layer</span>
</span></span><span style=display:flex><span>        output <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>Wo(context)  <span style=color:#75715e># (batch, seq_len, d_model)</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> output, attn_weights
</span></span></code></pre></div><p>The tensor reshaping is the trickiest part. After splitting into heads, I flatten <code>(batch, heads)</code> to reuse <code>scaled_dot_product_attention</code>, then reshape back and concatenate. Use <code>.contiguous()</code> before <code>.view</code> after a transpose.</p><h3 id=positional-encoding>Positional Encoding<a hidden class=anchor aria-hidden=true href=#positional-encoding>#</a></h3><p>Pure attention has no notion of order. We add positional encodings to token embeddings. The original paper used fixed sinusoidal encodings:</p><p>[\text{PE}(pos, 2i) = \sin\bigl(\frac{pos}{10000^{2i/d_{\text{model}}}}\bigr),\quad \text{PE}(pos, 2i+1) = \cos\bigl(\frac{pos}{10000^{2i/d_{\text{model}}}}\bigr)]</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> math
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>PositionalEncoding</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self, d_model, max_len<span style=color:#f92672>=</span><span style=color:#ae81ff>5000</span>):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span><span style=color:#a6e22e>__init__</span>()
</span></span><span style=display:flex><span>        pe <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>zeros(max_len, d_model)
</span></span><span style=display:flex><span>        position <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>0</span>, max_len, dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>float)<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>1</span>)  <span style=color:#75715e># (max_len, 1)</span>
</span></span><span style=display:flex><span>        div_term <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>exp(torch<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>0</span>, d_model, <span style=color:#ae81ff>2</span>)<span style=color:#f92672>.</span>float() <span style=color:#f92672>*</span> (math<span style=color:#f92672>.</span>log(<span style=color:#ae81ff>10000.0</span>) <span style=color:#f92672>/</span> d_model))
</span></span><span style=display:flex><span>        pe[:, <span style=color:#ae81ff>0</span>::<span style=color:#ae81ff>2</span>] <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>sin(position <span style=color:#f92672>*</span> div_term)  <span style=color:#75715e># even dims</span>
</span></span><span style=display:flex><span>        pe[:, <span style=color:#ae81ff>1</span>::<span style=color:#ae81ff>2</span>] <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>cos(position <span style=color:#f92672>*</span> div_term)  <span style=color:#75715e># odd dims</span>
</span></span><span style=display:flex><span>        pe <span style=color:#f92672>=</span> pe<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>0</span>)  <span style=color:#75715e># (1, max_len, d_model)</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Register as buffer so it’s saved with the model but not trainable</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>register_buffer(<span style=color:#e6db74>&#39;pe&#39;</span>, pe)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        <span style=color:#75715e># x: (batch, seq_len, d_model)</span>
</span></span><span style=display:flex><span>        seq_len <span style=color:#f92672>=</span> x<span style=color:#f92672>.</span>size(<span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> x <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>pe[:, :seq_len, :]
</span></span></code></pre></div><p>Shaping <code>pe</code> as <code>(1, max_len, d_model)</code> lets it broadcast-add to a batch of embeddings. Register it as a buffer so it moves with <code>model.to(device)</code> but isn’t updated by the optimizer.</p><h3 id=putting-it-together-encoder-layer>Putting It Together: Encoder Layer<a hidden class=anchor aria-hidden=true href=#putting-it-together-encoder-layer>#</a></h3><p>Each encoder layer does:</p><ol><li>Self-Attention + Add & Norm: <code>x &lt;- LayerNorm(x + MHA(x, x, x))</code></li><li>Feed-Forward + Add & Norm: <code>x &lt;- LayerNorm(x + FFN(x))</code></li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>EncoderLayer</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self, d_model, num_heads, dim_ff, dropout<span style=color:#f92672>=</span><span style=color:#ae81ff>0.1</span>):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span><span style=color:#a6e22e>__init__</span>()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>self_attn <span style=color:#f92672>=</span> MultiHeadAttention(d_model, num_heads)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>norm1 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>LayerNorm(d_model)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>norm2 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>LayerNorm(d_model)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>dropout <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Dropout(dropout)
</span></span><span style=display:flex><span>        <span style=color:#75715e># Feed-forward network (position-wise)</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>fc1 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(d_model, dim_ff)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>fc2 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(dim_ff, d_model)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x, mask<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>        <span style=color:#75715e># 1) Multi-head self-attention</span>
</span></span><span style=display:flex><span>        attn_output, _ <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>self_attn(x, x, x, mask)
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> x <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>dropout(attn_output)  <span style=color:#75715e># Residual</span>
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>norm1(x)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 2) Feed-forward network</span>
</span></span><span style=display:flex><span>        ff_output <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>fc2(self<span style=color:#f92672>.</span>dropout(torch<span style=color:#f92672>.</span>relu(self<span style=color:#f92672>.</span>fc1(x))))
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> x <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>dropout(ff_output)  <span style=color:#75715e># Residual</span>
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>norm2(x)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> x
</span></span></code></pre></div><p>If <code>x</code> is <code>(batch, seq_len, d_model)</code>, the module preserves shapes. Dropout is applied after attention and inside the feed-forward path.</p><h3 id=reflections>Reflections<a hidden class=anchor aria-hidden=true href=#reflections>#</a></h3><ul><li><strong>Understand by implementing</strong>: Writing code forces you to handle details (tensor shapes, masking). Quick dummy tests are invaluable.</li><li><strong>Scaling matters</strong>: Dividing by (\sqrt{d_k}) keeps softmax well-behaved; omitting it caused overly peaky distributions in my tests.</li><li><strong>Reshaping is tricky but logical</strong>: Splitting/combining heads is the most error-prone part; draw shapes while coding.</li><li><strong>Positional encoding is neat</strong>: The sine/cosine patterns give a smooth notion of distance; the model infers relative order.</li><li><strong>Modularity</strong>: Once <code>MultiHeadAttention</code> and <code>EncoderLayer</code> work, stacking them is straightforward and encourages experimentation.</li></ul><p>Overall, the magic is that self-attention lets each position flexibly gather information from anywhere else, and multi-head attention enriches that process. If you’re learning this, don’t just read the paper—try coding the pieces. Small confusions often turn into big insights once resolved.</p></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/transformers/>Transformers</a></li><li><a href=http://localhost:1313/tags/pytorch/>Pytorch</a></li><li><a href=http://localhost:1313/tags/nlp/>Nlp</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>Anoop Maurya</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>