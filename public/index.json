[{"content":"Transformers from Scratch: Implementing Attention Is All You Need Transformers have transformed the field of sequence modeling by replacing recurrent nets with self-attention. I was curious about how exactly this works under the hood, so I dove into the “Attention Is All You Need” paper and tried building the components from scratch in PyTorch. In this post I share my understanding and some hands-on insights from that process. I’ll cover the motivation behind Transformers, break down the core ideas, and show PyTorch snippets for the key modules: scaled dot-product attention, multi-head attention, positional encoding, and an encoder layer. Along the way I’ll mention a few hiccups and “aha!” moments I had – hopefully to make this journey relatable to anyone else who’s wrapped their head around this for the first time.\nMotivation Before Transformers, sequence models like RNNs or LSTMs were the standard for tasks like machine translation or text modeling. These models process data step-by-step, which makes it hard to parallelize and can struggle with long-range dependencies. The Transformer architecture changes that by relying entirely on attention mechanisms, allowing information from all time steps to be used at once. I wanted to understand why this matters and how to implement it myself. Some motivations that stood out to me:\nParallelism and efficiency: Self-attention lets us process all tokens at once instead of sequentially, so training can be much faster on modern hardware. Long-range context: Every token can attend to any other token, so the model can capture relationships across long distances without vanishing gradients. Simplicity of components: Despite being powerful, the Transformer’s building blocks (attention, feed-forward layers, etc.) are relatively simple operations. This made me think it would be feasible to code and inspect them directly. Practical success: Many cutting-edge models (like BERT or GPT) are based on this, so learning it deeply feels like a useful investment. With those motivations in mind, I decided to implement the pieces myself. In the sections that follow, I’ll dive into each major concept. First I’ll list the core concepts at a high level, then explain them in more detail.\nCore Concepts The Transformer encoder is built from a few key ideas. At a high level, these include:\nScaled Dot-Product Attention: Computes attention scores between queries and keys, scales them, and uses softmax to weight values. Multi-Head Attention: Runs multiple attention “heads” in parallel so the model can jointly attend to information from different representation subspaces. Positional Encoding: Injects information about the positions of tokens, since attention alone is agnostic to sequence order. Feed-Forward Network: A simple two-layer MLP applied to each position separately and identically. Add \u0026amp; Norm (Residual Connections + Layer Normalization): Ensures stable training and easy gradient flow by adding skip connections and normalizing. Each of these will get its own discussion below. Together they form an encoder layer, and stacking several of those (typically 6 or 12 in practice) gives you a Transformer encoder.\nScaled Dot-Product Attention At the heart of a Transformer is the attention mechanism. Given a set of queries Q, keys K, and values V, scaled dot-product attention computes a weighted sum of the values, where weights come from the similarity of queries with keys. The “scaled” part means we divide by sqrt(d_k) to keep the values from blowing up when dimensions are large.\nimport torch import math def scaled_dot_product_attention(Q, K, V, mask=None): d_k = Q.size(-1) scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k) if mask is not None: scores = scores.masked_fill(mask == 0, float(\u0026#39;-inf\u0026#39;)) attn_weights = torch.softmax(scores, dim=-1) output = torch.matmul(attn_weights, V) return output, attn_weights ","permalink":"http://localhost:1313/posts/attention_transformer_blog/","summary":"\u003ch1 id=\"transformers-from-scratch-implementing-attention-is-all-you-need\"\u003eTransformers from Scratch: Implementing \u003cem\u003eAttention Is All You Need\u003c/em\u003e\u003c/h1\u003e\n\u003cp\u003eTransformers have transformed the field of sequence modeling by replacing recurrent nets with self-attention. I was curious about how exactly this works under the hood, so I dove into the “Attention Is All You Need” paper and tried building the components from scratch in PyTorch. In this post I share my understanding and some hands-on insights from that process. I’ll cover the motivation behind Transformers, break down the core ideas, and show PyTorch snippets for the key modules: scaled dot-product attention, multi-head attention, positional encoding, and an encoder layer. Along the way I’ll mention a few hiccups and “aha!” moments I had – hopefully to make this journey relatable to anyone else who’s wrapped their head around this for the first time.\u003c/p\u003e","title":"Attention Is All You Need (Transformer Explained)"},{"content":"Transformers from Scratch: Implementing \u0026ldquo;Attention Is All You Need\u0026rdquo; Transformers changed sequence modeling by replacing recurrence with self‑attention. In this post, I share my understanding from reading the paper and building a tiny encoder in PyTorch. We’ll cover:\nWhy Transformers beat RNNs/LSTMs for long sequences Scaled dot‑product attention Multi‑head attention Sinusoidal positional encoding A minimal encoder layer and a tiny end‑to‑end example Why not RNNs/LSTMs? RNNs/LSTMs process tokens sequentially → poor parallelism and longer training time. Long‑range dependencies are hard due to vanishing gradients. Self‑attention lets every position attend to every other position in parallel, shortening the “path length” between tokens. Scaled Dot‑Product Attention Given queries Q, keys K, and values V (all shaped [batch, seq, d_k]), attention is:\nscores = Q @ K^T / sqrt(d_k) optional masking softmax over the key dimension weighted sum of V import torch import math def scaled_dot_product_attention(Q, K, V, mask=None): d_k = Q.size(-1) scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k) if mask is not None: scores = scores.masked_fill(mask == 0, float(\u0026#39;-inf\u0026#39;)) attn = torch.softmax(scores, dim=-1) out = torch.matmul(attn, V) return out, attn Multi‑Head Attention Multiple heads let the model capture different relations (syntax nearby, semantics farther away, etc.). For d_model and num_heads, each head has size d_k = d_model // num_heads.\nimport torch import torch.nn as nn import math class MultiHeadAttention(nn.Module): def __init__(self, d_model: int, num_heads: int): super().__init__() assert d_model % num_heads == 0, \u0026#34;d_model must be divisible by num_heads\u0026#34; self.d_model = d_model self.num_heads = num_heads self.d_k = d_model // num_heads self.Wq = nn.Linear(d_model, d_model) self.Wk = nn.Linear(d_model, d_model) self.Wv = nn.Linear(d_model, d_model) self.Wo = nn.Linear(d_model, d_model) def _split_heads(self, x): # x: [batch, seq, d_model] -\u0026gt; [batch, heads, seq, d_k] b, s, _ = x.size() return x.view(b, s, self.num_heads, self.d_k).transpose(1, 2) def forward(self, Q, K, V, mask=None): Q = self._split_heads(self.Wq(Q)) K = self._split_heads(self.Wk(K)) V = self._split_heads(self.Wv(V)) b, h, s_q, d_k = Q.size() Qf = Q.reshape(b * h, s_q, d_k) Kf = K.reshape(b * h, K.size(2), d_k) Vf = V.reshape(b * h, V.size(2), d_k) if mask is not None: mask = mask.unsqueeze(1).expand(b, h, mask.size(-2), mask.size(-1)) mask = mask.reshape(b * h, mask.size(-2), mask.size(-1)) context, attn = scaled_dot_product_attention(Qf, Kf, Vf, mask) context = context.view(b, h, s_q, d_k).transpose(1, 2).contiguous() context = context.view(b, s_q, self.d_model) return self.Wo(context) Sinusoidal Positional Encoding Attention alone has no sense of order. We add a position‑dependent vector to embeddings.\nclass PositionalEncoding(nn.Module): def __init__(self, d_model: int, max_len: int = 5000, dropout: float = 0.1): super().__init__() self.dropout = nn.Dropout(dropout) pe = torch.zeros(max_len, d_model) position = torch.arange(0, max_len).unsqueeze(1).float() div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) pe[:, 0::2] = torch.sin(position * div_term) pe[:, 1::2] = torch.cos(position * div_term) pe = pe.unsqueeze(0) # [1, max_len, d_model] self.register_buffer(\u0026#39;pe\u0026#39;, pe) def forward(self, x): # x: [batch, seq, d_model] seq_len = x.size(1) x = x + self.pe[:, :seq_len] return self.dropout(x) Minimal Encoder Layer An encoder layer = self‑attention + add\u0026amp;norm, then feed‑forward + add\u0026amp;norm.\nclass EncoderLayer(nn.Module): def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1): super().__init__() self.self_attn = MultiHeadAttention(d_model, num_heads) self.norm1 = nn.LayerNorm(d_model) self.norm2 = nn.LayerNorm(d_model) self.dropout = nn.Dropout(dropout) self.ff = nn.Sequential( nn.Linear(d_model, d_ff), nn.ReLU(), nn.Linear(d_ff, d_model), ) def forward(self, x, mask=None): attn_out = self.self_attn(x, x, x, mask) x = self.norm1(x + self.dropout(attn_out)) ff_out = self.ff(x) x = self.norm2(x + self.dropout(ff_out)) return x A Tiny End‑to‑End Example (toy data) Below is a minimal example that embeds random token ids, adds positions, and passes them through one encoder layer. This is not a full training loop—just a sanity‑check run.\nimport torch import torch.nn as nn torch.manual_seed(0) vocab_size = 100 seq_len = 10 batch_size = 4 d_model = 64 num_heads = 8 d_ff = 256 embed = nn.Embedding(vocab_size, d_model) pe = PositionalEncoding(d_model) enc = EncoderLayer(d_model, num_heads, d_ff) # fake batch of token ids x_ids = torch.randint(0, vocab_size, (batch_size, seq_len)) # embed + positions x = embed(x_ids) x = pe(x) # optional padding mask example (1=keep, 0=mask); here we keep all mask = torch.ones(batch_size, seq_len, seq_len, dtype=torch.bool) out = enc(x, mask) print(out.shape) # torch.Size([4, 10, 64]) What I learned Scaling by sqrt(d_k) keeps softmax in a healthy range. Getting shapes right for heads is the trickiest part—draw the tensors. Positional encoding is simple to implement and easy to visualize. The architecture is modular: once attention and an encoder layer work, stacking is straightforward. If you’d like, I can extend this with a small training loop on a toy copy task or add a decoder with causal masking.\n","permalink":"http://localhost:1313/posts/attention_transformer_blog/","summary":"\u003ch1 id=\"transformers-from-scratch-implementing-attention-is-all-you-need\"\u003eTransformers from Scratch: Implementing \u0026ldquo;Attention Is All You Need\u0026rdquo;\u003c/h1\u003e\n\u003cp\u003eTransformers changed sequence modeling by replacing recurrence with self‑attention. In this post, I share my understanding from reading the paper and building a tiny encoder in PyTorch. We’ll cover:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWhy Transformers beat RNNs/LSTMs for long sequences\u003c/li\u003e\n\u003cli\u003eScaled dot‑product attention\u003c/li\u003e\n\u003cli\u003eMulti‑head attention\u003c/li\u003e\n\u003cli\u003eSinusoidal positional encoding\u003c/li\u003e\n\u003cli\u003eA minimal encoder layer and a tiny end‑to‑end example\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"why-not-rnnslstms\"\u003eWhy not RNNs/LSTMs?\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eRNNs/LSTMs process tokens sequentially → poor parallelism and longer training time.\u003c/li\u003e\n\u003cli\u003eLong‑range dependencies are hard due to vanishing gradients.\u003c/li\u003e\n\u003cli\u003eSelf‑attention lets every position attend to every other position in parallel, shortening the “path length” between tokens.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"scaled-dotproduct-attention\"\u003eScaled Dot‑Product Attention\u003c/h2\u003e\n\u003cp\u003eGiven queries Q, keys K, and values V (all shaped \u003ccode\u003e[batch, seq, d_k]\u003c/code\u003e), attention is:\u003c/p\u003e","title":"Transformers from Scratch: Implementing Attention Is All You Need"},{"content":"Transformers from Scratch: Implementing \u0026ldquo;Attention Is All You Need\u0026rdquo; Transformers changed sequence modeling by replacing recurrence with self‑attention. In this post, I share my understanding from reading the paper and building a tiny encoder in PyTorch. We’ll cover:\nWhy Transformers beat RNNs/LSTMs for long sequences Scaled dot‑product attention Multi‑head attention Sinusoidal positional encoding A minimal encoder layer and a tiny end‑to‑end example Why not RNNs/LSTMs? RNNs/LSTMs process tokens sequentially → poor parallelism and longer training time. Long‑range dependencies are hard due to vanishing gradients. Self‑attention lets every position attend to every other position in parallel, shortening the “path length” between tokens. Scaled Dot‑Product Attention Given queries Q, keys K, and values V (all shaped [batch, seq, d_k]), attention is:\nscores = Q @ K^T / sqrt(d_k) optional masking softmax over the key dimension weighted sum of V import torch import math def scaled_dot_product_attention(Q, K, V, mask=None): d_k = Q.size(-1) scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k) if mask is not None: scores = scores.masked_fill(mask == 0, float(\u0026#39;-inf\u0026#39;)) attn = torch.softmax(scores, dim=-1) out = torch.matmul(attn, V) return out, attn Multi‑Head Attention Multiple heads let the model capture different relations (syntax nearby, semantics farther away, etc.). For d_model and num_heads, each head has size d_k = d_model // num_heads.\nimport torch import torch.nn as nn import math class MultiHeadAttention(nn.Module): def __init__(self, d_model: int, num_heads: int): super().__init__() assert d_model % num_heads == 0, \u0026#34;d_model must be divisible by num_heads\u0026#34; self.d_model = d_model self.num_heads = num_heads self.d_k = d_model // num_heads self.Wq = nn.Linear(d_model, d_model) self.Wk = nn.Linear(d_model, d_model) self.Wv = nn.Linear(d_model, d_model) self.Wo = nn.Linear(d_model, d_model) def _split_heads(self, x): # x: [batch, seq, d_model] -\u0026gt; [batch, heads, seq, d_k] b, s, _ = x.size() return x.view(b, s, self.num_heads, self.d_k).transpose(1, 2) def forward(self, Q, K, V, mask=None): Q = self._split_heads(self.Wq(Q)) K = self._split_heads(self.Wk(K)) V = self._split_heads(self.Wv(V)) b, h, s_q, d_k = Q.size() Qf = Q.reshape(b * h, s_q, d_k) Kf = K.reshape(b * h, K.size(2), d_k) Vf = V.reshape(b * h, V.size(2), d_k) if mask is not None: mask = mask.unsqueeze(1).expand(b, h, mask.size(-2), mask.size(-1)) mask = mask.reshape(b * h, mask.size(-2), mask.size(-1)) context, attn = scaled_dot_product_attention(Qf, Kf, Vf, mask) context = context.view(b, h, s_q, d_k).transpose(1, 2).contiguous() context = context.view(b, s_q, self.d_model) return self.Wo(context) Sinusoidal Positional Encoding Attention alone has no sense of order. We add a position‑dependent vector to embeddings.\nclass PositionalEncoding(nn.Module): def __init__(self, d_model: int, max_len: int = 5000, dropout: float = 0.1): super().__init__() self.dropout = nn.Dropout(dropout) pe = torch.zeros(max_len, d_model) position = torch.arange(0, max_len).unsqueeze(1).float() div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) pe[:, 0::2] = torch.sin(position * div_term) pe[:, 1::2] = torch.cos(position * div_term) pe = pe.unsqueeze(0) # [1, max_len, d_model] self.register_buffer(\u0026#39;pe\u0026#39;, pe) def forward(self, x): # x: [batch, seq, d_model] seq_len = x.size(1) x = x + self.pe[:, :seq_len] return self.dropout(x) Minimal Encoder Layer An encoder layer = self‑attention + add\u0026amp;norm, then feed‑forward + add\u0026amp;norm.\nclass EncoderLayer(nn.Module): def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1): super().__init__() self.self_attn = MultiHeadAttention(d_model, num_heads) self.norm1 = nn.LayerNorm(d_model) self.norm2 = nn.LayerNorm(d_model) self.dropout = nn.Dropout(dropout) self.ff = nn.Sequential( nn.Linear(d_model, d_ff), nn.ReLU(), nn.Linear(d_ff, d_model), ) def forward(self, x, mask=None): attn_out = self.self_attn(x, x, x, mask) x = self.norm1(x + self.dropout(attn_out)) ff_out = self.ff(x) x = self.norm2(x + self.dropout(ff_out)) return x A Tiny End‑to‑End Example (toy data) Below is a minimal example that embeds random token ids, adds positions, and passes them through one encoder layer. This is not a full training loop—just a sanity‑check run.\nimport torch import torch.nn as nn torch.manual_seed(0) vocab_size = 100 seq_len = 10 batch_size = 4 d_model = 64 num_heads = 8 d_ff = 256 embed = nn.Embedding(vocab_size, d_model) pe = PositionalEncoding(d_model) enc = EncoderLayer(d_model, num_heads, d_ff) # fake batch of token ids x_ids = torch.randint(0, vocab_size, (batch_size, seq_len)) # embed + positions x = embed(x_ids) x = pe(x) # optional padding mask example (1=keep, 0=mask); here we keep all mask = torch.ones(batch_size, seq_len, seq_len, dtype=torch.bool) out = enc(x, mask) print(out.shape) # torch.Size([4, 10, 64]) What I learned Scaling by sqrt(d_k) keeps softmax in a healthy range. Getting shapes right for heads is the trickiest part—draw the tensors. Positional encoding is simple to implement and easy to visualize. The architecture is modular: once attention and an encoder layer work, stacking is straightforward. If you’d like, I can extend this with a small training loop on a toy copy task or add a decoder with causal masking.\n","permalink":"http://localhost:1313/posts/attention_transformer_blog/","summary":"\u003ch1 id=\"transformers-from-scratch-implementing-attention-is-all-you-need\"\u003eTransformers from Scratch: Implementing \u0026ldquo;Attention Is All You Need\u0026rdquo;\u003c/h1\u003e\n\u003cp\u003eTransformers changed sequence modeling by replacing recurrence with self‑attention. In this post, I share my understanding from reading the paper and building a tiny encoder in PyTorch. We’ll cover:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWhy Transformers beat RNNs/LSTMs for long sequences\u003c/li\u003e\n\u003cli\u003eScaled dot‑product attention\u003c/li\u003e\n\u003cli\u003eMulti‑head attention\u003c/li\u003e\n\u003cli\u003eSinusoidal positional encoding\u003c/li\u003e\n\u003cli\u003eA minimal encoder layer and a tiny end‑to‑end example\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"why-not-rnnslstms\"\u003eWhy not RNNs/LSTMs?\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eRNNs/LSTMs process tokens sequentially → poor parallelism and longer training time.\u003c/li\u003e\n\u003cli\u003eLong‑range dependencies are hard due to vanishing gradients.\u003c/li\u003e\n\u003cli\u003eSelf‑attention lets every position attend to every other position in parallel, shortening the “path length” between tokens.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"scaled-dotproduct-attention\"\u003eScaled Dot‑Product Attention\u003c/h2\u003e\n\u003cp\u003eGiven queries Q, keys K, and values V (all shaped \u003ccode\u003e[batch, seq, d_k]\u003c/code\u003e), attention is:\u003c/p\u003e","title":"Transformers from Scratch: Implementing Attention Is All You Need"},{"content":"Transformers from Scratch: Implementing \u0026ldquo;Attention Is All You Need\u0026rdquo; Transformers changed sequence modeling by replacing recurrence with self‑attention. In this post, I share my understanding from reading the paper and building a tiny encoder in PyTorch. We’ll cover:\nWhy Transformers beat RNNs/LSTMs for long sequences Scaled dot‑product attention Multi‑head attention Sinusoidal positional encoding A minimal encoder layer and a tiny end‑to‑end example Why not RNNs/LSTMs? RNNs/LSTMs process tokens sequentially → poor parallelism and longer training time. Long‑range dependencies are hard due to vanishing gradients. Self‑attention lets every position attend to every other position in parallel, shortening the “path length” between tokens. Scaled Dot‑Product Attention Given queries Q, keys K, and values V (all shaped [batch, seq, d_k]), attention is:\nscores = Q @ K^T / sqrt(d_k) optional masking softmax over the key dimension weighted sum of V import torch import math def scaled_dot_product_attention(Q, K, V, mask=None): d_k = Q.size(-1) scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k) if mask is not None: scores = scores.masked_fill(mask == 0, float(\u0026#39;-inf\u0026#39;)) attn = torch.softmax(scores, dim=-1) out = torch.matmul(attn, V) return out, attn Multi‑Head Attention Multiple heads let the model capture different relations (syntax nearby, semantics farther away, etc.). For d_model and num_heads, each head has size d_k = d_model // num_heads.\nimport torch import torch.nn as nn import math class MultiHeadAttention(nn.Module): def __init__(self, d_model: int, num_heads: int): super().__init__() assert d_model % num_heads == 0, \u0026#34;d_model must be divisible by num_heads\u0026#34; self.d_model = d_model self.num_heads = num_heads self.d_k = d_model // num_heads self.Wq = nn.Linear(d_model, d_model) self.Wk = nn.Linear(d_model, d_model) self.Wv = nn.Linear(d_model, d_model) self.Wo = nn.Linear(d_model, d_model) def _split_heads(self, x): # x: [batch, seq, d_model] -\u0026gt; [batch, heads, seq, d_k] b, s, _ = x.size() return x.view(b, s, self.num_heads, self.d_k).transpose(1, 2) def forward(self, Q, K, V, mask=None): Q = self._split_heads(self.Wq(Q)) K = self._split_heads(self.Wk(K)) V = self._split_heads(self.Wv(V)) b, h, s_q, d_k = Q.size() Qf = Q.reshape(b * h, s_q, d_k) Kf = K.reshape(b * h, K.size(2), d_k) Vf = V.reshape(b * h, V.size(2), d_k) if mask is not None: mask = mask.unsqueeze(1).expand(b, h, mask.size(-2), mask.size(-1)) mask = mask.reshape(b * h, mask.size(-2), mask.size(-1)) context, attn = scaled_dot_product_attention(Qf, Kf, Vf, mask) context = context.view(b, h, s_q, d_k).transpose(1, 2).contiguous() context = context.view(b, s_q, self.d_model) return self.Wo(context) Sinusoidal Positional Encoding Attention alone has no sense of order. We add a position‑dependent vector to embeddings.\nclass PositionalEncoding(nn.Module): def __init__(self, d_model: int, max_len: int = 5000, dropout: float = 0.1): super().__init__() self.dropout = nn.Dropout(dropout) pe = torch.zeros(max_len, d_model) position = torch.arange(0, max_len).unsqueeze(1).float() div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) pe[:, 0::2] = torch.sin(position * div_term) pe[:, 1::2] = torch.cos(position * div_term) pe = pe.unsqueeze(0) # [1, max_len, d_model] self.register_buffer(\u0026#39;pe\u0026#39;, pe) def forward(self, x): # x: [batch, seq, d_model] seq_len = x.size(1) x = x + self.pe[:, :seq_len] return self.dropout(x) Minimal Encoder Layer An encoder layer = self‑attention + add\u0026amp;norm, then feed‑forward + add\u0026amp;norm.\nclass EncoderLayer(nn.Module): def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1): super().__init__() self.self_attn = MultiHeadAttention(d_model, num_heads) self.norm1 = nn.LayerNorm(d_model) self.norm2 = nn.LayerNorm(d_model) self.dropout = nn.Dropout(dropout) self.ff = nn.Sequential( nn.Linear(d_model, d_ff), nn.ReLU(), nn.Linear(d_ff, d_model), ) def forward(self, x, mask=None): attn_out = self.self_attn(x, x, x, mask) x = self.norm1(x + self.dropout(attn_out)) ff_out = self.ff(x) x = self.norm2(x + self.dropout(ff_out)) return x A Tiny End‑to‑End Example (toy data) Below is a minimal example that embeds random token ids, adds positions, and passes them through one encoder layer. This is not a full training loop—just a sanity‑check run.\nimport torch import torch.nn as nn torch.manual_seed(0) vocab_size = 100 seq_len = 10 batch_size = 4 d_model = 64 num_heads = 8 d_ff = 256 embed = nn.Embedding(vocab_size, d_model) pe = PositionalEncoding(d_model) enc = EncoderLayer(d_model, num_heads, d_ff) # fake batch of token ids x_ids = torch.randint(0, vocab_size, (batch_size, seq_len)) # embed + positions x = embed(x_ids) x = pe(x) # optional padding mask example (1=keep, 0=mask); here we keep all mask = torch.ones(batch_size, seq_len, seq_len, dtype=torch.bool) out = enc(x, mask) print(out.shape) # torch.Size([4, 10, 64]) What I learned Scaling by sqrt(d_k) keeps softmax in a healthy range. Getting shapes right for heads is the trickiest part—draw the tensors. Positional encoding is simple to implement and easy to visualize. The architecture is modular: once attention and an encoder layer work, stacking is straightforward. If you’d like, I can extend this with a small training loop on a toy copy task or add a decoder with causal masking.\n","permalink":"http://localhost:1313/posts/attention_transformer_blog/","summary":"\u003ch1 id=\"transformers-from-scratch-implementing-attention-is-all-you-need\"\u003eTransformers from Scratch: Implementing \u0026ldquo;Attention Is All You Need\u0026rdquo;\u003c/h1\u003e\n\u003cp\u003eTransformers changed sequence modeling by replacing recurrence with self‑attention. In this post, I share my understanding from reading the paper and building a tiny encoder in PyTorch. We’ll cover:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWhy Transformers beat RNNs/LSTMs for long sequences\u003c/li\u003e\n\u003cli\u003eScaled dot‑product attention\u003c/li\u003e\n\u003cli\u003eMulti‑head attention\u003c/li\u003e\n\u003cli\u003eSinusoidal positional encoding\u003c/li\u003e\n\u003cli\u003eA minimal encoder layer and a tiny end‑to‑end example\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"why-not-rnnslstms\"\u003eWhy not RNNs/LSTMs?\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eRNNs/LSTMs process tokens sequentially → poor parallelism and longer training time.\u003c/li\u003e\n\u003cli\u003eLong‑range dependencies are hard due to vanishing gradients.\u003c/li\u003e\n\u003cli\u003eSelf‑attention lets every position attend to every other position in parallel, shortening the “path length” between tokens.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"scaled-dotproduct-attention\"\u003eScaled Dot‑Product Attention\u003c/h2\u003e\n\u003cp\u003eGiven queries Q, keys K, and values V (all shaped \u003ccode\u003e[batch, seq, d_k]\u003c/code\u003e), attention is:\u003c/p\u003e","title":"Transformers from Scratch: Implementing Attention Is All You Need"},{"content":"Transformers have transformed the field of sequence modeling by replacing recurrent nets with self-attention. I was curious about how this works under the hood, so I dove into the “Attention Is All You Need” paper and built the components from scratch in PyTorch. In this post I share my understanding and hands-on insights. I’ll cover the motivation behind Transformers, break down the core ideas, and show PyTorch snippets for the key modules: scaled dot-product attention, multi-head attention, positional encoding, and an encoder layer.\nAlong the way I’ll mention a few hiccups and “aha!” moments I had – hopefully making this journey relatable to anyone wrapping their head around this for the first time.\nMotivation Before Transformers, sequence models like RNNs or LSTMs were standard for machine translation or language modeling. These models process data step-by-step, which is hard to parallelize and can struggle with long-range dependencies. The Transformer relies entirely on attention, allowing information from all time steps to be used at once.\nParallelism and efficiency: Self-attention lets us process all tokens at once, so training is much faster on modern hardware. Long-range context: Every token can attend to any other token, capturing long-distance relationships without vanishing gradients. Simplicity of components: The building blocks (attention, feed-forward layers, etc.) are relatively simple operations—feasible to code and inspect directly. Practical success: Many cutting-edge models (BERT, GPT) are based on Transformers, so learning it deeply pays off. Core Concepts The Transformer encoder is built from a few key ideas:\nScaled Dot-Product Attention: Computes attention scores between queries and keys, scales them, and uses softmax to weight values. Multi-Head Attention: Runs several attention “heads” in parallel so the model can jointly attend to different representation subspaces. Positional Encoding: Injects information about token positions, since attention alone is order-agnostic. Feed-Forward Network: A two-layer MLP applied to each position separately and identically. Add \u0026amp; Norm (Residual + LayerNorm): Residual connections and layer normalization for stable training and easy gradient flow. Stacking several encoder layers (typically 6–12) gives you a Transformer encoder.\nScaled Dot-Product Attention Given queries Q, keys K, and values V, scaled dot-product attention computes a weighted sum of the values, where weights come from the similarity of queries with keys. The scores are divided by (\\sqrt{d_k}) to keep values in a reasonable range.\nHigh level steps for a single query (\\mathbf{q}) and keys (\\mathbf{k}_j):\nCompute raw scores: (\\text{score}_j = \\mathbf{q} \\cdot \\mathbf{k}_j^\\top) Scale: (\\text{score}_j / \\sqrt{d_k}) Optionally mask some scores to (-\\infty) Softmax to probabilities: (\\alpha_j) Weighted sum of values: (\\sum_j \\alpha_j \\mathbf{v}_j) PyTorch implementation:\nimport torch import math def scaled_dot_product_attention(Q, K, V, mask=None): \u0026#34;\u0026#34;\u0026#34; Q, K, V: shape (batch, seq_len, d_k) mask: optional mask tensor of shape (batch, seq_len_q, seq_len_k) Returns: (output, attention_weights) \u0026#34;\u0026#34;\u0026#34; d_k = Q.size(-1) # Compute scaled dot products scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k) # Apply the mask (if provided) by setting scores to large negative where mask==0 if mask is not None: scores = scores.masked_fill(mask == 0, float(\u0026#39;-inf\u0026#39;)) # Softmax to get attention weights attn_weights = torch.softmax(scores, dim=-1) # Multiply by values output = torch.matmul(attn_weights, V) return output, attn_weights Here, scores has shape (batch, seq_q, seq_k) after the matmul. Multiplying by V then gives an output of shape (batch, seq_q, d_k). For masks (e.g., padding or causal), use masked_fill(-inf) before softmax so those positions get effectively zero weight.\nMulti-Head Attention One attention head gives one context representation. Multi-head attention runs several heads in parallel with separate linear projections, letting each head focus on different aspects (e.g., local vs. long-range patterns).\nIf d_model is the model dimension and we have h heads, each head uses d_k = d_model / h. There are learned projections Wq, Wk, Wv and an output projection Wo.\nimport torch import torch.nn as nn class MultiHeadAttention(nn.Module): def __init__(self, d_model, num_heads): super().__init__() assert d_model % num_heads == 0, \u0026#34;d_model must be divisible by num_heads\u0026#34; self.num_heads = num_heads self.d_k = d_model // num_heads # Linear layers to project inputs to queries, keys, values self.Wq = nn.Linear(d_model, d_model) self.Wk = nn.Linear(d_model, d_model) self.Wv = nn.Linear(d_model, d_model) # Output linear layer self.Wo = nn.Linear(d_model, d_model) def forward(self, Q, K, V, mask=None): batch_size = Q.size(0) # 1) Linear projections Q = self.Wq(Q) # (batch, seq_len, d_model) K = self.Wk(K) V = self.Wv(V) # 2) Split into heads by reshaping Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) # Now Q, K, V are (batch, heads, seq_len, d_k) # 3) Apply scaled dot-product attention on each head # Combine batch and heads dims for efficiency Q_flat = Q.reshape(-1, Q.size(2), self.d_k) # (batch*heads, seq_q, d_k) K_flat = K.reshape(-1, K.size(2), self.d_k) V_flat = V.reshape(-1, V.size(2), self.d_k) if mask is not None: # Expand mask to match the number of heads mask = mask.unsqueeze(1).repeat(1, self.num_heads, 1, 1) mask = mask.reshape(-1, mask.size(-2), mask.size(-1)) context, attn_weights = scaled_dot_product_attention(Q_flat, K_flat, V_flat, mask) # 4) Concatenate heads context = context.view(batch_size, self.num_heads, -1, self.d_k) context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k) # 5) Final linear layer output = self.Wo(context) # (batch, seq_len, d_model) return output, attn_weights The tensor reshaping is the trickiest part. After splitting into heads, I flatten (batch, heads) to reuse scaled_dot_product_attention, then reshape back and concatenate. Use .contiguous() before .view after a transpose.\nPositional Encoding Pure attention has no notion of order. We add positional encodings to token embeddings. The original paper used fixed sinusoidal encodings:\n[\\text{PE}(pos, 2i) = \\sin\\bigl(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\bigr),\\quad \\text{PE}(pos, 2i+1) = \\cos\\bigl(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\bigr)]\nimport torch import torch.nn as nn import math class PositionalEncoding(nn.Module): def __init__(self, d_model, max_len=5000): super().__init__() pe = torch.zeros(max_len, d_model) position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # (max_len, 1) div_term = torch.exp(torch.arange(0, d_model, 2).float() * (math.log(10000.0) / d_model)) pe[:, 0::2] = torch.sin(position * div_term) # even dims pe[:, 1::2] = torch.cos(position * div_term) # odd dims pe = pe.unsqueeze(0) # (1, max_len, d_model) # Register as buffer so it’s saved with the model but not trainable self.register_buffer(\u0026#39;pe\u0026#39;, pe) def forward(self, x): # x: (batch, seq_len, d_model) seq_len = x.size(1) return x + self.pe[:, :seq_len, :] Shaping pe as (1, max_len, d_model) lets it broadcast-add to a batch of embeddings. Register it as a buffer so it moves with model.to(device) but isn’t updated by the optimizer.\nPutting It Together: Encoder Layer Each encoder layer does:\nSelf-Attention + Add \u0026amp; Norm: x \u0026lt;- LayerNorm(x + MHA(x, x, x)) Feed-Forward + Add \u0026amp; Norm: x \u0026lt;- LayerNorm(x + FFN(x)) import torch.nn as nn class EncoderLayer(nn.Module): def __init__(self, d_model, num_heads, dim_ff, dropout=0.1): super().__init__() self.self_attn = MultiHeadAttention(d_model, num_heads) self.norm1 = nn.LayerNorm(d_model) self.norm2 = nn.LayerNorm(d_model) self.dropout = nn.Dropout(dropout) # Feed-forward network (position-wise) self.fc1 = nn.Linear(d_model, dim_ff) self.fc2 = nn.Linear(dim_ff, d_model) def forward(self, x, mask=None): # 1) Multi-head self-attention attn_output, _ = self.self_attn(x, x, x, mask) x = x + self.dropout(attn_output) # Residual x = self.norm1(x) # 2) Feed-forward network ff_output = self.fc2(self.dropout(torch.relu(self.fc1(x)))) x = x + self.dropout(ff_output) # Residual x = self.norm2(x) return x If x is (batch, seq_len, d_model), the module preserves shapes. Dropout is applied after attention and inside the feed-forward path.\nReflections Understand by implementing: Writing code forces you to handle details (tensor shapes, masking). Quick dummy tests are invaluable. Scaling matters: Dividing by (\\sqrt{d_k}) keeps softmax well-behaved; omitting it caused overly peaky distributions in my tests. Reshaping is tricky but logical: Splitting/combining heads is the most error-prone part; draw shapes while coding. Positional encoding is neat: The sine/cosine patterns give a smooth notion of distance; the model infers relative order. Modularity: Once MultiHeadAttention and EncoderLayer work, stacking them is straightforward and encourages experimentation. Overall, the magic is that self-attention lets each position flexibly gather information from anywhere else, and multi-head attention enriches that process. If you’re learning this, don’t just read the paper—try coding the pieces. Small confusions often turn into big insights once resolved.\n","permalink":"http://localhost:1313/posts/attention_transformer_blog/","summary":"\u003cp\u003eTransformers have transformed the field of sequence modeling by replacing recurrent nets with self-attention. I was curious about how this works under the hood, so I dove into the “Attention Is All You Need” paper and built the components from scratch in PyTorch. In this post I share my understanding and hands-on insights. I’ll cover the motivation behind Transformers, break down the core ideas, and show PyTorch snippets for the key modules: scaled dot-product attention, multi-head attention, positional encoding, and an encoder layer.\u003c/p\u003e","title":"Transformers from Scratch: Implementing Attention Is All You Need"},{"content":"Transformers have transformed the field of sequence modeling by replacing recurrent nets with self-attention. I was curious about how this works under the hood, so I dove into the “Attention Is All You Need” paper and built the components from scratch in PyTorch. In this post I share my understanding and hands-on insights. I’ll cover the motivation behind Transformers, break down the core ideas, and show PyTorch snippets for the key modules: scaled dot-product attention, multi-head attention, positional encoding, and an encoder layer.\nAlong the way I’ll mention a few hiccups and “aha!” moments I had – hopefully making this journey relatable to anyone wrapping their head around this for the first time.\nMotivation Before Transformers, sequence models like RNNs or LSTMs were standard for machine translation or language modeling. These models process data step-by-step, which is hard to parallelize and can struggle with long-range dependencies. The Transformer relies entirely on attention, allowing information from all time steps to be used at once.\nParallelism and efficiency: Self-attention lets us process all tokens at once, so training is much faster on modern hardware. Long-range context: Every token can attend to any other token, capturing long-distance relationships without vanishing gradients. Simplicity of components: The building blocks (attention, feed-forward layers, etc.) are relatively simple operations—feasible to code and inspect directly. Practical success: Many cutting-edge models (BERT, GPT) are based on Transformers, so learning it deeply pays off. Core Concepts The Transformer encoder is built from a few key ideas:\nScaled Dot-Product Attention: Computes attention scores between queries and keys, scales them, and uses softmax to weight values. Multi-Head Attention: Runs several attention “heads” in parallel so the model can jointly attend to different representation subspaces. Positional Encoding: Injects information about token positions, since attention alone is order-agnostic. Feed-Forward Network: A two-layer MLP applied to each position separately and identically. Add \u0026amp; Norm (Residual + LayerNorm): Residual connections and layer normalization for stable training and easy gradient flow. Stacking several encoder layers (typically 6–12) gives you a Transformer encoder.\nScaled Dot-Product Attention Given queries Q, keys K, and values V, scaled dot-product attention computes a weighted sum of the values, where weights come from the similarity of queries with keys. The scores are divided by (\\sqrt{d_k}) to keep values in a reasonable range.\nHigh level steps for a single query (\\mathbf{q}) and keys (\\mathbf{k}_j):\nCompute raw scores: (\\text{score}_j = \\mathbf{q} \\cdot \\mathbf{k}_j^\\top) Scale: (\\text{score}_j / \\sqrt{d_k}) Optionally mask some scores to (-\\infty) Softmax to probabilities: (\\alpha_j) Weighted sum of values: (\\sum_j \\alpha_j \\mathbf{v}_j) PyTorch implementation:\nimport torch import math def scaled_dot_product_attention(Q, K, V, mask=None): \u0026#34;\u0026#34;\u0026#34; Q, K, V: shape (batch, seq_len, d_k) mask: optional mask tensor of shape (batch, seq_len_q, seq_len_k) Returns: (output, attention_weights) \u0026#34;\u0026#34;\u0026#34; d_k = Q.size(-1) # Compute scaled dot products scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k) # Apply the mask (if provided) by setting scores to large negative where mask==0 if mask is not None: scores = scores.masked_fill(mask == 0, float(\u0026#39;-inf\u0026#39;)) # Softmax to get attention weights attn_weights = torch.softmax(scores, dim=-1) # Multiply by values output = torch.matmul(attn_weights, V) return output, attn_weights Here, scores has shape (batch, seq_q, seq_k) after the matmul. Multiplying by V then gives an output of shape (batch, seq_q, d_k). For masks (e.g., padding or causal), use masked_fill(-inf) before softmax so those positions get effectively zero weight.\nMulti-Head Attention One attention head gives one context representation. Multi-head attention runs several heads in parallel with separate linear projections, letting each head focus on different aspects (e.g., local vs. long-range patterns).\nIf d_model is the model dimension and we have h heads, each head uses d_k = d_model / h. There are learned projections Wq, Wk, Wv and an output projection Wo.\nimport torch import torch.nn as nn class MultiHeadAttention(nn.Module): def __init__(self, d_model, num_heads): super().__init__() assert d_model % num_heads == 0, \u0026#34;d_model must be divisible by num_heads\u0026#34; self.num_heads = num_heads self.d_k = d_model // num_heads # Linear layers to project inputs to queries, keys, values self.Wq = nn.Linear(d_model, d_model) self.Wk = nn.Linear(d_model, d_model) self.Wv = nn.Linear(d_model, d_model) # Output linear layer self.Wo = nn.Linear(d_model, d_model) def forward(self, Q, K, V, mask=None): batch_size = Q.size(0) # 1) Linear projections Q = self.Wq(Q) # (batch, seq_len, d_model) K = self.Wk(K) V = self.Wv(V) # 2) Split into heads by reshaping Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) # Now Q, K, V are (batch, heads, seq_len, d_k) # 3) Apply scaled dot-product attention on each head # Combine batch and heads dims for efficiency Q_flat = Q.reshape(-1, Q.size(2), self.d_k) # (batch*heads, seq_q, d_k) K_flat = K.reshape(-1, K.size(2), self.d_k) V_flat = V.reshape(-1, V.size(2), self.d_k) if mask is not None: # Expand mask to match the number of heads mask = mask.unsqueeze(1).repeat(1, self.num_heads, 1, 1) mask = mask.reshape(-1, mask.size(-2), mask.size(-1)) context, attn_weights = scaled_dot_product_attention(Q_flat, K_flat, V_flat, mask) # 4) Concatenate heads context = context.view(batch_size, self.num_heads, -1, self.d_k) context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k) # 5) Final linear layer output = self.Wo(context) # (batch, seq_len, d_model) return output, attn_weights The tensor reshaping is the trickiest part. After splitting into heads, I flatten (batch, heads) to reuse scaled_dot_product_attention, then reshape back and concatenate. Use .contiguous() before .view after a transpose.\nPositional Encoding Pure attention has no notion of order. We add positional encodings to token embeddings. The original paper used fixed sinusoidal encodings:\n[\\text{PE}(pos, 2i) = \\sin\\bigl(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\bigr),\\quad \\text{PE}(pos, 2i+1) = \\cos\\bigl(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\bigr)]\nimport torch import torch.nn as nn import math class PositionalEncoding(nn.Module): def __init__(self, d_model, max_len=5000): super().__init__() pe = torch.zeros(max_len, d_model) position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # (max_len, 1) div_term = torch.exp(torch.arange(0, d_model, 2).float() * (math.log(10000.0) / d_model)) pe[:, 0::2] = torch.sin(position * div_term) # even dims pe[:, 1::2] = torch.cos(position * div_term) # odd dims pe = pe.unsqueeze(0) # (1, max_len, d_model) # Register as buffer so it’s saved with the model but not trainable self.register_buffer(\u0026#39;pe\u0026#39;, pe) def forward(self, x): # x: (batch, seq_len, d_model) seq_len = x.size(1) return x + self.pe[:, :seq_len, :] Shaping pe as (1, max_len, d_model) lets it broadcast-add to a batch of embeddings. Register it as a buffer so it moves with model.to(device) but isn’t updated by the optimizer.\nPutting It Together: Encoder Layer Each encoder layer does:\nSelf-Attention + Add \u0026amp; Norm: x \u0026lt;- LayerNorm(x + MHA(x, x, x)) Feed-Forward + Add \u0026amp; Norm: x \u0026lt;- LayerNorm(x + FFN(x)) import torch.nn as nn class EncoderLayer(nn.Module): def __init__(self, d_model, num_heads, dim_ff, dropout=0.1): super().__init__() self.self_attn = MultiHeadAttention(d_model, num_heads) self.norm1 = nn.LayerNorm(d_model) self.norm2 = nn.LayerNorm(d_model) self.dropout = nn.Dropout(dropout) # Feed-forward network (position-wise) self.fc1 = nn.Linear(d_model, dim_ff) self.fc2 = nn.Linear(dim_ff, d_model) def forward(self, x, mask=None): # 1) Multi-head self-attention attn_output, _ = self.self_attn(x, x, x, mask) x = x + self.dropout(attn_output) # Residual x = self.norm1(x) # 2) Feed-forward network ff_output = self.fc2(self.dropout(torch.relu(self.fc1(x)))) x = x + self.dropout(ff_output) # Residual x = self.norm2(x) return x If x is (batch, seq_len, d_model), the module preserves shapes. Dropout is applied after attention and inside the feed-forward path.\nReflections Understand by implementing: Writing code forces you to handle details (tensor shapes, masking). Quick dummy tests are invaluable. Scaling matters: Dividing by (\\sqrt{d_k}) keeps softmax well-behaved; omitting it caused overly peaky distributions in my tests. Reshaping is tricky but logical: Splitting/combining heads is the most error-prone part; draw shapes while coding. Positional encoding is neat: The sine/cosine patterns give a smooth notion of distance; the model infers relative order. Modularity: Once MultiHeadAttention and EncoderLayer work, stacking them is straightforward and encourages experimentation. Overall, the magic is that self-attention lets each position flexibly gather information from anywhere else, and multi-head attention enriches that process. If you’re learning this, don’t just read the paper—try coding the pieces. Small confusions often turn into big insights once resolved.\n","permalink":"http://localhost:1313/posts/attention_transformer_blog/","summary":"\u003cp\u003eTransformers have transformed the field of sequence modeling by replacing recurrent nets with self-attention. I was curious about how this works under the hood, so I dove into the “Attention Is All You Need” paper and built the components from scratch in PyTorch. In this post I share my understanding and hands-on insights. I’ll cover the motivation behind Transformers, break down the core ideas, and show PyTorch snippets for the key modules: scaled dot-product attention, multi-head attention, positional encoding, and an encoder layer.\u003c/p\u003e","title":"Transformers from Scratch: Implementing Attention Is All You Need"},{"content":"Transformers have transformed the field of sequence modeling by replacing recurrent nets with self-attention. I was curious about how this works under the hood, so I dove into the “Attention Is All You Need” paper and built the components from scratch in PyTorch. In this post I share my understanding and hands-on insights. I’ll cover the motivation behind Transformers, break down the core ideas, and show PyTorch snippets for the key modules: scaled dot-product attention, multi-head attention, positional encoding, and an encoder layer.\nAlong the way I’ll mention a few hiccups and “aha!” moments I had – hopefully making this journey relatable to anyone wrapping their head around this for the first time.\nMotivation Before Transformers, sequence models like RNNs or LSTMs were standard for machine translation or language modeling. These models process data step-by-step, which is hard to parallelize and can struggle with long-range dependencies. The Transformer relies entirely on attention, allowing information from all time steps to be used at once.\nParallelism and efficiency: Self-attention lets us process all tokens at once, so training is much faster on modern hardware. Long-range context: Every token can attend to any other token, capturing long-distance relationships without vanishing gradients. Simplicity of components: The building blocks (attention, feed-forward layers, etc.) are relatively simple operations—feasible to code and inspect directly. Practical success: Many cutting-edge models (BERT, GPT) are based on Transformers, so learning it deeply pays off. Core Concepts The Transformer encoder is built from a few key ideas:\nScaled Dot-Product Attention: Computes attention scores between queries and keys, scales them, and uses softmax to weight values. Multi-Head Attention: Runs several attention “heads” in parallel so the model can jointly attend to different representation subspaces. Positional Encoding: Injects information about token positions, since attention alone is order-agnostic. Feed-Forward Network: A two-layer MLP applied to each position separately and identically. Add \u0026amp; Norm (Residual + LayerNorm): Residual connections and layer normalization for stable training and easy gradient flow. Stacking several encoder layers (typically 6–12) gives you a Transformer encoder.\nScaled Dot-Product Attention Given queries Q, keys K, and values V, scaled dot-product attention computes a weighted sum of the values, where weights come from the similarity of queries with keys. The scores are divided by (\\sqrt{d_k}) to keep values in a reasonable range.\nHigh level steps for a single query (\\mathbf{q}) and keys (\\mathbf{k}_j):\nCompute raw scores: (\\text{score}_j = \\mathbf{q} \\cdot \\mathbf{k}_j^\\top) Scale: (\\text{score}_j / \\sqrt{d_k}) Optionally mask some scores to (-\\infty) Softmax to probabilities: (\\alpha_j) Weighted sum of values: (\\sum_j \\alpha_j \\mathbf{v}_j) PyTorch implementation:\nimport torch import math def scaled_dot_product_attention(Q, K, V, mask=None): \u0026#34;\u0026#34;\u0026#34; Q, K, V: shape (batch, seq_len, d_k) mask: optional mask tensor of shape (batch, seq_len_q, seq_len_k) Returns: (output, attention_weights) \u0026#34;\u0026#34;\u0026#34; d_k = Q.size(-1) # Compute scaled dot products scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k) # Apply the mask (if provided) by setting scores to large negative where mask==0 if mask is not None: scores = scores.masked_fill(mask == 0, float(\u0026#39;-inf\u0026#39;)) # Softmax to get attention weights attn_weights = torch.softmax(scores, dim=-1) # Multiply by values output = torch.matmul(attn_weights, V) return output, attn_weights Here, scores has shape (batch, seq_q, seq_k) after the matmul. Multiplying by V then gives an output of shape (batch, seq_q, d_k). For masks (e.g., padding or causal), use masked_fill(-inf) before softmax so those positions get effectively zero weight.\nMulti-Head Attention One attention head gives one context representation. Multi-head attention runs several heads in parallel with separate linear projections, letting each head focus on different aspects (e.g., local vs. long-range patterns).\nIf d_model is the model dimension and we have h heads, each head uses d_k = d_model / h. There are learned projections Wq, Wk, Wv and an output projection Wo.\nimport torch import torch.nn as nn class MultiHeadAttention(nn.Module): def __init__(self, d_model, num_heads): super().__init__() assert d_model % num_heads == 0, \u0026#34;d_model must be divisible by num_heads\u0026#34; self.num_heads = num_heads self.d_k = d_model // num_heads # Linear layers to project inputs to queries, keys, values self.Wq = nn.Linear(d_model, d_model) self.Wk = nn.Linear(d_model, d_model) self.Wv = nn.Linear(d_model, d_model) # Output linear layer self.Wo = nn.Linear(d_model, d_model) def forward(self, Q, K, V, mask=None): batch_size = Q.size(0) # 1) Linear projections Q = self.Wq(Q) # (batch, seq_len, d_model) K = self.Wk(K) V = self.Wv(V) # 2) Split into heads by reshaping Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) # Now Q, K, V are (batch, heads, seq_len, d_k) # 3) Apply scaled dot-product attention on each head # Combine batch and heads dims for efficiency Q_flat = Q.reshape(-1, Q.size(2), self.d_k) # (batch*heads, seq_q, d_k) K_flat = K.reshape(-1, K.size(2), self.d_k) V_flat = V.reshape(-1, V.size(2), self.d_k) if mask is not None: # Expand mask to match the number of heads mask = mask.unsqueeze(1).repeat(1, self.num_heads, 1, 1) mask = mask.reshape(-1, mask.size(-2), mask.size(-1)) context, attn_weights = scaled_dot_product_attention(Q_flat, K_flat, V_flat, mask) # 4) Concatenate heads context = context.view(batch_size, self.num_heads, -1, self.d_k) context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k) # 5) Final linear layer output = self.Wo(context) # (batch, seq_len, d_model) return output, attn_weights The tensor reshaping is the trickiest part. After splitting into heads, I flatten (batch, heads) to reuse scaled_dot_product_attention, then reshape back and concatenate. Use .contiguous() before .view after a transpose.\nPositional Encoding Pure attention has no notion of order. We add positional encodings to token embeddings. The original paper used fixed sinusoidal encodings:\n[\\text{PE}(pos, 2i) = \\sin\\bigl(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\bigr),\\quad \\text{PE}(pos, 2i+1) = \\cos\\bigl(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\bigr)]\nimport torch import torch.nn as nn import math class PositionalEncoding(nn.Module): def __init__(self, d_model, max_len=5000): super().__init__() pe = torch.zeros(max_len, d_model) position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # (max_len, 1) div_term = torch.exp(torch.arange(0, d_model, 2).float() * (math.log(10000.0) / d_model)) pe[:, 0::2] = torch.sin(position * div_term) # even dims pe[:, 1::2] = torch.cos(position * div_term) # odd dims pe = pe.unsqueeze(0) # (1, max_len, d_model) # Register as buffer so it’s saved with the model but not trainable self.register_buffer(\u0026#39;pe\u0026#39;, pe) def forward(self, x): # x: (batch, seq_len, d_model) seq_len = x.size(1) return x + self.pe[:, :seq_len, :] Shaping pe as (1, max_len, d_model) lets it broadcast-add to a batch of embeddings. Register it as a buffer so it moves with model.to(device) but isn’t updated by the optimizer.\nPutting It Together: Encoder Layer Each encoder layer does:\nSelf-Attention + Add \u0026amp; Norm: x \u0026lt;- LayerNorm(x + MHA(x, x, x)) Feed-Forward + Add \u0026amp; Norm: x \u0026lt;- LayerNorm(x + FFN(x)) import torch.nn as nn class EncoderLayer(nn.Module): def __init__(self, d_model, num_heads, dim_ff, dropout=0.1): super().__init__() self.self_attn = MultiHeadAttention(d_model, num_heads) self.norm1 = nn.LayerNorm(d_model) self.norm2 = nn.LayerNorm(d_model) self.dropout = nn.Dropout(dropout) # Feed-forward network (position-wise) self.fc1 = nn.Linear(d_model, dim_ff) self.fc2 = nn.Linear(dim_ff, d_model) def forward(self, x, mask=None): # 1) Multi-head self-attention attn_output, _ = self.self_attn(x, x, x, mask) x = x + self.dropout(attn_output) # Residual x = self.norm1(x) # 2) Feed-forward network ff_output = self.fc2(self.dropout(torch.relu(self.fc1(x)))) x = x + self.dropout(ff_output) # Residual x = self.norm2(x) return x If x is (batch, seq_len, d_model), the module preserves shapes. Dropout is applied after attention and inside the feed-forward path.\nReflections Understand by implementing: Writing code forces you to handle details (tensor shapes, masking). Quick dummy tests are invaluable. Scaling matters: Dividing by (\\sqrt{d_k}) keeps softmax well-behaved; omitting it caused overly peaky distributions in my tests. Reshaping is tricky but logical: Splitting/combining heads is the most error-prone part; draw shapes while coding. Positional encoding is neat: The sine/cosine patterns give a smooth notion of distance; the model infers relative order. Modularity: Once MultiHeadAttention and EncoderLayer work, stacking them is straightforward and encourages experimentation. Overall, the magic is that self-attention lets each position flexibly gather information from anywhere else, and multi-head attention enriches that process. If you’re learning this, don’t just read the paper—try coding the pieces. Small confusions often turn into big insights once resolved.\n","permalink":"http://localhost:1313/posts/attention_transformer_blog/","summary":"\u003cp\u003eTransformers have transformed the field of sequence modeling by replacing recurrent nets with self-attention. I was curious about how this works under the hood, so I dove into the “Attention Is All You Need” paper and built the components from scratch in PyTorch. In this post I share my understanding and hands-on insights. I’ll cover the motivation behind Transformers, break down the core ideas, and show PyTorch snippets for the key modules: scaled dot-product attention, multi-head attention, positional encoding, and an encoder layer.\u003c/p\u003e","title":"Transformers from Scratch: Implementing Attention Is All You Need"},{"content":"Transformers have transformed the field of sequence modeling by replacing recurrent nets with self-attention. I was curious about how this works under the hood, so I dove into the “Attention Is All You Need” paper and built the components from scratch in PyTorch. In this post I share my understanding and hands-on insights. I’ll cover the motivation behind Transformers, break down the core ideas, and show PyTorch snippets for the key modules: scaled dot-product attention, multi-head attention, positional encoding, and an encoder layer.\nAlong the way I’ll mention a few hiccups and “aha!” moments I had – hopefully making this journey relatable to anyone wrapping their head around this for the first time.\nMotivation Before Transformers, sequence models like RNNs or LSTMs were standard for machine translation or language modeling. These models process data step-by-step, which is hard to parallelize and can struggle with long-range dependencies. The Transformer relies entirely on attention, allowing information from all time steps to be used at once.\nParallelism and efficiency: Self-attention lets us process all tokens at once, so training is much faster on modern hardware. Long-range context: Every token can attend to any other token, capturing long-distance relationships without vanishing gradients. Simplicity of components: The building blocks (attention, feed-forward layers, etc.) are relatively simple operations—feasible to code and inspect directly. Practical success: Many cutting-edge models (BERT, GPT) are based on Transformers, so learning it deeply pays off. Core Concepts The Transformer encoder is built from a few key ideas:\nScaled Dot-Product Attention: Computes attention scores between queries and keys, scales them, and uses softmax to weight values. Multi-Head Attention: Runs several attention “heads” in parallel so the model can jointly attend to different representation subspaces. Positional Encoding: Injects information about token positions, since attention alone is order-agnostic. Feed-Forward Network: A two-layer MLP applied to each position separately and identically. Add \u0026amp; Norm (Residual + LayerNorm): Residual connections and layer normalization for stable training and easy gradient flow. Stacking several encoder layers (typically 6–12) gives you a Transformer encoder.\nScaled Dot-Product Attention Given queries Q, keys K, and values V, scaled dot-product attention computes a weighted sum of the values, where weights come from the similarity of queries with keys. The scores are divided by (\\sqrt{d_k}) to keep values in a reasonable range.\nHigh level steps for a single query (\\mathbf{q}) and keys (\\mathbf{k}_j):\nCompute raw scores: (\\text{score}_j = \\mathbf{q} \\cdot \\mathbf{k}_j^\\top) Scale: (\\text{score}_j / \\sqrt{d_k}) Optionally mask some scores to (-\\infty) Softmax to probabilities: (\\alpha_j) Weighted sum of values: (\\sum_j \\alpha_j \\mathbf{v}_j) PyTorch implementation:\nimport torch import math def scaled_dot_product_attention(Q, K, V, mask=None): \u0026#34;\u0026#34;\u0026#34; Q, K, V: shape (batch, seq_len, d_k) mask: optional mask tensor of shape (batch, seq_len_q, seq_len_k) Returns: (output, attention_weights) \u0026#34;\u0026#34;\u0026#34; d_k = Q.size(-1) # Compute scaled dot products scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k) # Apply the mask (if provided) by setting scores to large negative where mask==0 if mask is not None: scores = scores.masked_fill(mask == 0, float(\u0026#39;-inf\u0026#39;)) # Softmax to get attention weights attn_weights = torch.softmax(scores, dim=-1) # Multiply by values output = torch.matmul(attn_weights, V) return output, attn_weights Here, scores has shape (batch, seq_q, seq_k) after the matmul. Multiplying by V then gives an output of shape (batch, seq_q, d_k). For masks (e.g., padding or causal), use masked_fill(-inf) before softmax so those positions get effectively zero weight.\nMulti-Head Attention One attention head gives one context representation. Multi-head attention runs several heads in parallel with separate linear projections, letting each head focus on different aspects (e.g., local vs. long-range patterns).\nIf d_model is the model dimension and we have h heads, each head uses d_k = d_model / h. There are learned projections Wq, Wk, Wv and an output projection Wo.\nimport torch import torch.nn as nn class MultiHeadAttention(nn.Module): def __init__(self, d_model, num_heads): super().__init__() assert d_model % num_heads == 0, \u0026#34;d_model must be divisible by num_heads\u0026#34; self.num_heads = num_heads self.d_k = d_model // num_heads # Linear layers to project inputs to queries, keys, values self.Wq = nn.Linear(d_model, d_model) self.Wk = nn.Linear(d_model, d_model) self.Wv = nn.Linear(d_model, d_model) # Output linear layer self.Wo = nn.Linear(d_model, d_model) def forward(self, Q, K, V, mask=None): batch_size = Q.size(0) # 1) Linear projections Q = self.Wq(Q) # (batch, seq_len, d_model) K = self.Wk(K) V = self.Wv(V) # 2) Split into heads by reshaping Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) # Now Q, K, V are (batch, heads, seq_len, d_k) # 3) Apply scaled dot-product attention on each head # Combine batch and heads dims for efficiency Q_flat = Q.reshape(-1, Q.size(2), self.d_k) # (batch*heads, seq_q, d_k) K_flat = K.reshape(-1, K.size(2), self.d_k) V_flat = V.reshape(-1, V.size(2), self.d_k) if mask is not None: # Expand mask to match the number of heads mask = mask.unsqueeze(1).repeat(1, self.num_heads, 1, 1) mask = mask.reshape(-1, mask.size(-2), mask.size(-1)) context, attn_weights = scaled_dot_product_attention(Q_flat, K_flat, V_flat, mask) # 4) Concatenate heads context = context.view(batch_size, self.num_heads, -1, self.d_k) context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k) # 5) Final linear layer output = self.Wo(context) # (batch, seq_len, d_model) return output, attn_weights The tensor reshaping is the trickiest part. After splitting into heads, I flatten (batch, heads) to reuse scaled_dot_product_attention, then reshape back and concatenate. Use .contiguous() before .view after a transpose.\nPositional Encoding Pure attention has no notion of order. We add positional encodings to token embeddings. The original paper used fixed sinusoidal encodings:\n[\\text{PE}(pos, 2i) = \\sin\\bigl(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\bigr),\\quad \\text{PE}(pos, 2i+1) = \\cos\\bigl(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\bigr)]\nimport torch import torch.nn as nn import math class PositionalEncoding(nn.Module): def __init__(self, d_model, max_len=5000): super().__init__() pe = torch.zeros(max_len, d_model) position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # (max_len, 1) div_term = torch.exp(torch.arange(0, d_model, 2).float() * (math.log(10000.0) / d_model)) pe[:, 0::2] = torch.sin(position * div_term) # even dims pe[:, 1::2] = torch.cos(position * div_term) # odd dims pe = pe.unsqueeze(0) # (1, max_len, d_model) # Register as buffer so it’s saved with the model but not trainable self.register_buffer(\u0026#39;pe\u0026#39;, pe) def forward(self, x): # x: (batch, seq_len, d_model) seq_len = x.size(1) return x + self.pe[:, :seq_len, :] Shaping pe as (1, max_len, d_model) lets it broadcast-add to a batch of embeddings. Register it as a buffer so it moves with model.to(device) but isn’t updated by the optimizer.\nPutting It Together: Encoder Layer Each encoder layer does:\nSelf-Attention + Add \u0026amp; Norm: x \u0026lt;- LayerNorm(x + MHA(x, x, x)) Feed-Forward + Add \u0026amp; Norm: x \u0026lt;- LayerNorm(x + FFN(x)) import torch.nn as nn class EncoderLayer(nn.Module): def __init__(self, d_model, num_heads, dim_ff, dropout=0.1): super().__init__() self.self_attn = MultiHeadAttention(d_model, num_heads) self.norm1 = nn.LayerNorm(d_model) self.norm2 = nn.LayerNorm(d_model) self.dropout = nn.Dropout(dropout) # Feed-forward network (position-wise) self.fc1 = nn.Linear(d_model, dim_ff) self.fc2 = nn.Linear(dim_ff, d_model) def forward(self, x, mask=None): # 1) Multi-head self-attention attn_output, _ = self.self_attn(x, x, x, mask) x = x + self.dropout(attn_output) # Residual x = self.norm1(x) # 2) Feed-forward network ff_output = self.fc2(self.dropout(torch.relu(self.fc1(x)))) x = x + self.dropout(ff_output) # Residual x = self.norm2(x) return x If x is (batch, seq_len, d_model), the module preserves shapes. Dropout is applied after attention and inside the feed-forward path.\nReflections Understand by implementing: Writing code forces you to handle details (tensor shapes, masking). Quick dummy tests are invaluable. Scaling matters: Dividing by (\\sqrt{d_k}) keeps softmax well-behaved; omitting it caused overly peaky distributions in my tests. Reshaping is tricky but logical: Splitting/combining heads is the most error-prone part; draw shapes while coding. Positional encoding is neat: The sine/cosine patterns give a smooth notion of distance; the model infers relative order. Modularity: Once MultiHeadAttention and EncoderLayer work, stacking them is straightforward and encourages experimentation. Overall, the magic is that self-attention lets each position flexibly gather information from anywhere else, and multi-head attention enriches that process. If you’re learning this, don’t just read the paper—try coding the pieces. Small confusions often turn into big insights once resolved.\n","permalink":"http://localhost:1313/posts/attention_transformer_blog/","summary":"\u003cp\u003eTransformers have transformed the field of sequence modeling by replacing recurrent nets with self-attention. I was curious about how this works under the hood, so I dove into the “Attention Is All You Need” paper and built the components from scratch in PyTorch. In this post I share my understanding and hands-on insights. I’ll cover the motivation behind Transformers, break down the core ideas, and show PyTorch snippets for the key modules: scaled dot-product attention, multi-head attention, positional encoding, and an encoder layer.\u003c/p\u003e","title":"Transformers from Scratch: Implementing Attention Is All You Need"},{"content":"Transformers have transformed the field of sequence modeling by replacing recurrent nets with self-attention. I was curious about how this works under the hood, so I dove into the “Attention Is All You Need” paper and built the components from scratch in PyTorch. In this post I share my understanding and hands-on insights. I’ll cover the motivation behind Transformers, break down the core ideas, and show PyTorch snippets for the key modules: scaled dot-product attention, multi-head attention, positional encoding, and an encoder layer.\nAlong the way I’ll mention a few hiccups and “aha!” moments I had – hopefully making this journey relatable to anyone wrapping their head around this for the first time.\nMotivation Before Transformers, sequence models like RNNs or LSTMs were standard for machine translation or language modeling. These models process data step-by-step, which is hard to parallelize and can struggle with long-range dependencies. The Transformer relies entirely on attention, allowing information from all time steps to be used at once.\nParallelism and efficiency: Self-attention lets us process all tokens at once, so training is much faster on modern hardware. Long-range context: Every token can attend to any other token, capturing long-distance relationships without vanishing gradients. Simplicity of components: The building blocks (attention, feed-forward layers, etc.) are relatively simple operations—feasible to code and inspect directly. Practical success: Many cutting-edge models (BERT, GPT) are based on Transformers, so learning it deeply pays off. Core Concepts The Transformer encoder is built from a few key ideas:\nScaled Dot-Product Attention: Computes attention scores between queries and keys, scales them, and uses softmax to weight values. Multi-Head Attention: Runs several attention “heads” in parallel so the model can jointly attend to different representation subspaces. Positional Encoding: Injects information about token positions, since attention alone is order-agnostic. Feed-Forward Network: A two-layer MLP applied to each position separately and identically. Add \u0026amp; Norm (Residual + LayerNorm): Residual connections and layer normalization for stable training and easy gradient flow. Stacking several encoder layers (typically 6–12) gives you a Transformer encoder.\nScaled Dot-Product Attention Given queries Q, keys K, and values V, scaled dot-product attention computes a weighted sum of the values, where weights come from the similarity of queries with keys. The scores are divided by (\\sqrt{d_k}) to keep values in a reasonable range.\nHigh level steps for a single query (\\mathbf{q}) and keys (\\mathbf{k}_j):\nCompute raw scores: (\\text{score}_j = \\mathbf{q} \\cdot \\mathbf{k}_j^\\top) Scale: (\\text{score}_j / \\sqrt{d_k}) Optionally mask some scores to (-\\infty) Softmax to probabilities: (\\alpha_j) Weighted sum of values: (\\sum_j \\alpha_j \\mathbf{v}_j) PyTorch implementation:\nimport torch import math def scaled_dot_product_attention(Q, K, V, mask=None): \u0026#34;\u0026#34;\u0026#34; Q, K, V: shape (batch, seq_len, d_k) mask: optional mask tensor of shape (batch, seq_len_q, seq_len_k) Returns: (output, attention_weights) \u0026#34;\u0026#34;\u0026#34; d_k = Q.size(-1) # Compute scaled dot products scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k) # Apply the mask (if provided) by setting scores to large negative where mask==0 if mask is not None: scores = scores.masked_fill(mask == 0, float(\u0026#39;-inf\u0026#39;)) # Softmax to get attention weights attn_weights = torch.softmax(scores, dim=-1) # Multiply by values output = torch.matmul(attn_weights, V) return output, attn_weights Here, scores has shape (batch, seq_q, seq_k) after the matmul. Multiplying by V then gives an output of shape (batch, seq_q, d_k). For masks (e.g., padding or causal), use masked_fill(-inf) before softmax so those positions get effectively zero weight.\nMulti-Head Attention One attention head gives one context representation. Multi-head attention runs several heads in parallel with separate linear projections, letting each head focus on different aspects (e.g., local vs. long-range patterns).\nIf d_model is the model dimension and we have h heads, each head uses d_k = d_model / h. There are learned projections Wq, Wk, Wv and an output projection Wo.\nimport torch import torch.nn as nn class MultiHeadAttention(nn.Module): def __init__(self, d_model, num_heads): super().__init__() assert d_model % num_heads == 0, \u0026#34;d_model must be divisible by num_heads\u0026#34; self.num_heads = num_heads self.d_k = d_model // num_heads # Linear layers to project inputs to queries, keys, values self.Wq = nn.Linear(d_model, d_model) self.Wk = nn.Linear(d_model, d_model) self.Wv = nn.Linear(d_model, d_model) # Output linear layer self.Wo = nn.Linear(d_model, d_model) def forward(self, Q, K, V, mask=None): batch_size = Q.size(0) # 1) Linear projections Q = self.Wq(Q) # (batch, seq_len, d_model) K = self.Wk(K) V = self.Wv(V) # 2) Split into heads by reshaping Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) # Now Q, K, V are (batch, heads, seq_len, d_k) # 3) Apply scaled dot-product attention on each head # Combine batch and heads dims for efficiency Q_flat = Q.reshape(-1, Q.size(2), self.d_k) # (batch*heads, seq_q, d_k) K_flat = K.reshape(-1, K.size(2), self.d_k) V_flat = V.reshape(-1, V.size(2), self.d_k) if mask is not None: # Expand mask to match the number of heads mask = mask.unsqueeze(1).repeat(1, self.num_heads, 1, 1) mask = mask.reshape(-1, mask.size(-2), mask.size(-1)) context, attn_weights = scaled_dot_product_attention(Q_flat, K_flat, V_flat, mask) # 4) Concatenate heads context = context.view(batch_size, self.num_heads, -1, self.d_k) context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k) # 5) Final linear layer output = self.Wo(context) # (batch, seq_len, d_model) return output, attn_weights The tensor reshaping is the trickiest part. After splitting into heads, I flatten (batch, heads) to reuse scaled_dot_product_attention, then reshape back and concatenate. Use .contiguous() before .view after a transpose.\nPositional Encoding Pure attention has no notion of order. We add positional encodings to token embeddings. The original paper used fixed sinusoidal encodings:\n[\\text{PE}(pos, 2i) = \\sin\\bigl(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\bigr),\\quad \\text{PE}(pos, 2i+1) = \\cos\\bigl(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\bigr)]\nimport torch import torch.nn as nn import math class PositionalEncoding(nn.Module): def __init__(self, d_model, max_len=5000): super().__init__() pe = torch.zeros(max_len, d_model) position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # (max_len, 1) div_term = torch.exp(torch.arange(0, d_model, 2).float() * (math.log(10000.0) / d_model)) pe[:, 0::2] = torch.sin(position * div_term) # even dims pe[:, 1::2] = torch.cos(position * div_term) # odd dims pe = pe.unsqueeze(0) # (1, max_len, d_model) # Register as buffer so it’s saved with the model but not trainable self.register_buffer(\u0026#39;pe\u0026#39;, pe) def forward(self, x): # x: (batch, seq_len, d_model) seq_len = x.size(1) return x + self.pe[:, :seq_len, :] Shaping pe as (1, max_len, d_model) lets it broadcast-add to a batch of embeddings. Register it as a buffer so it moves with model.to(device) but isn’t updated by the optimizer.\nPutting It Together: Encoder Layer Each encoder layer does:\nSelf-Attention + Add \u0026amp; Norm: x \u0026lt;- LayerNorm(x + MHA(x, x, x)) Feed-Forward + Add \u0026amp; Norm: x \u0026lt;- LayerNorm(x + FFN(x)) import torch.nn as nn class EncoderLayer(nn.Module): def __init__(self, d_model, num_heads, dim_ff, dropout=0.1): super().__init__() self.self_attn = MultiHeadAttention(d_model, num_heads) self.norm1 = nn.LayerNorm(d_model) self.norm2 = nn.LayerNorm(d_model) self.dropout = nn.Dropout(dropout) # Feed-forward network (position-wise) self.fc1 = nn.Linear(d_model, dim_ff) self.fc2 = nn.Linear(dim_ff, d_model) def forward(self, x, mask=None): # 1) Multi-head self-attention attn_output, _ = self.self_attn(x, x, x, mask) x = x + self.dropout(attn_output) # Residual x = self.norm1(x) # 2) Feed-forward network ff_output = self.fc2(self.dropout(torch.relu(self.fc1(x)))) x = x + self.dropout(ff_output) # Residual x = self.norm2(x) return x If x is (batch, seq_len, d_model), the module preserves shapes. Dropout is applied after attention and inside the feed-forward path.\nReflections Understand by implementing: Writing code forces you to handle details (tensor shapes, masking). Quick dummy tests are invaluable. Scaling matters: Dividing by (\\sqrt{d_k}) keeps softmax well-behaved; omitting it caused overly peaky distributions in my tests. Reshaping is tricky but logical: Splitting/combining heads is the most error-prone part; draw shapes while coding. Positional encoding is neat: The sine/cosine patterns give a smooth notion of distance; the model infers relative order. Modularity: Once MultiHeadAttention and EncoderLayer work, stacking them is straightforward and encourages experimentation. Overall, the magic is that self-attention lets each position flexibly gather information from anywhere else, and multi-head attention enriches that process. If you’re learning this, don’t just read the paper—try coding the pieces. Small confusions often turn into big insights once resolved.\n","permalink":"http://localhost:1313/posts/attention_transformer_blog/","summary":"\u003cp\u003eTransformers have transformed the field of sequence modeling by replacing recurrent nets with self-attention. I was curious about how this works under the hood, so I dove into the “Attention Is All You Need” paper and built the components from scratch in PyTorch. In this post I share my understanding and hands-on insights. I’ll cover the motivation behind Transformers, break down the core ideas, and show PyTorch snippets for the key modules: scaled dot-product attention, multi-head attention, positional encoding, and an encoder layer.\u003c/p\u003e","title":"Transformers from Scratch: Implementing Attention Is All You Need"}]