[{"content":"Introduction As a data scientist and AI engineer, I\u0026rsquo;ve spent countless hours working with various agentic frameworks like LangGraph, AutoGen, and CrewAI. While these tools are incredibly powerfull and make our lives easier, I often found myself wondering: what actually happens under the hood? What makes an AI agent tick? How do all these pieces come together to create something that can think, plan, and act autonomously?\nThat curiosity led me down a fascinating rabbit hole, and eventually to building an AI agent completly from scratch - no frameworks, no abstractions, just pure Python and a deep dive into the fundamental building blocks that make agents work.\nIn today\u0026rsquo;s rapidly evolving AI landscape, we\u0026rsquo;re witnessing an explosion of agentic systems. These aren\u0026rsquo;t just chatbots that respond to queries - they\u0026rsquo;re autonomous entities capable of complex reasoning, planning, and executing multi-step tasks. But here\u0026rsquo;s the thing: most of us are building on top of these sophisticated frameworks without truely understanding what\u0026rsquo;s happening beneath the surface.\nWhy does this matter? Because when you understand the fundamentals, you become a better AI engineer. You can debug issues more effectively, optimize performance better, and most importantly, you can innovate beyond the constraints of existing frameworks.\nSo let\u0026rsquo;s pull back the curtain and build an agent from the ground up. By the end of this post, you\u0026rsquo;ll have a clear understanding of what an AI agent really is, why we need them, and how to build one yourself.\nWhat is an AI Agent? Before we dive into the code, let\u0026rsquo;s establish a clear understanding of what we mean by an \u0026ldquo;AI agent.\u0026rdquo;\nAn AI agent is fundamentally different from a simple language model. While an LLM can generate text based on prompts, an agent can:\nThink about problems systematically Plan multi-step solutions Use tools to interact with the world Remember previous interactions and learnings Iterate until a goal is achieved Think of it this way:\nLLM: \u0026ldquo;Tell me about the weather\u0026rdquo; Agent: \u0026ldquo;Check the weather, plan my day accordingly, set reminders, and book restaurant reservations if it\u0026rsquo;s going to rain\u0026rdquo; The agent doesn\u0026rsquo;t just provide information - it takes action.\n##W The Agent Equation: Two Ways to Think About It\nThroughout my research and experimentation, I\u0026rsquo;ve found two complementary ways to conceptualize what an agent is:\nFormula 1: The Loop Perspective agent = llm + memory + tools + while loop This formula emphasizes the iterative nature of agents. The while loop is crucial - it\u0026rsquo;s what allows the agent to keep working until the task is complete, rather than giving a single response.\nFormula 2: The Capability Perspective agent = llm(read+write) + planning + tools + (condition+loop) This formula emphasizes the capabilities of agents. The LLM can both read (understand) and write (generate), planning provides structure, tools provide actions, and conditions with loops provide control flow.\nBoth formulas are correct - they just highlight different aspects of the same underlying system.\nWhy Build Agents from Scratch? You might be thinking: \u0026ldquo;Why re-invent the wheel? LangGraph and AutoGen work perfectly fine!\u0026rdquo;\nAnd you\u0026rsquo;re absolutley right - for production systems, you should definitely use these battle-tested frameworks. But here\u0026rsquo;s why building from scratch is invaluable:\n1. Deep Understanding When you build something yourself, you understand every component. This knowledge is incredibly valuable when debugging production issues or optimizing performance.\n2. Framework Independence Understanding the fundamentals means you\u0026rsquo;re not locked into any particular framework. You can evaluate new tools more effectively and migrate between them when needed.\n3. Innovation Opportunities The best innovations often come from understanding the basics deeply enough to see new possibilities. Many of today\u0026rsquo;s popular frameworks started as someone\u0026rsquo;s experiment in building agents from scratch.\n4. Debugging Superpowers When your production agent misbehaves, knowing what\u0026rsquo;s happening under the hood makes troubleshooting infinitely easier.\nThe Architecture: Building Blocks of Our Agent Let me walk you through the core components we\u0026rsquo;ll be building:\nComponent 1: The LLM Foundation Our agent starts with a simple LLM interface. Here\u0026rsquo;s how we set up the basic communication:\nfrom ai import openai def basic_completion(): completion = openai.chat.completions.create({ \u0026#34;messages\u0026#34;: [ {\u0026#34;role\u0026#34;: \u0026#34;developer\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;You are a helpful assistant.\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What is your name?\u0026#34;}, ], \u0026#34;model\u0026#34;: \u0026#34;llama3.2:latest\u0026#34;, }) return completion.get(\u0026#34;choices\u0026#34;, [None])[0] This is our foundation - a simple way to communicate with an LLM. But it\u0026rsquo;s just a one-shot interaction. To make it agentic, we need more.\nComponent 2: Adding Conditions and Judgment The next step is teaching our agent to evaluate its own responses:\ndef answer_with_judgment(question): # First: get an answer completion = openai.chat.completions.create({ \u0026#34;messages\u0026#34;: [{\u0026#34;role\u0026#34;: \u0026#34;developer\u0026#34;, \u0026#34;content\u0026#34;: question}], \u0026#34;model\u0026#34;: \u0026#34;llama3.2:latest\u0026#34;, }) answer = completion.get(\u0026#34;choices\u0026#34;, [{}])[0].get(\u0026#34;message\u0026#34;, {}).get(\u0026#34;content\u0026#34;) # Second: judge the completeness check = openai.chat.completions.create({ \u0026#34;model\u0026#34;: \u0026#34;llama3.2:latest\u0026#34;, \u0026#34;messages\u0026#34;: [{ \u0026#34;role\u0026#34;: \u0026#34;developer\u0026#34;, \u0026#34;content\u0026#34;: ( \u0026#34;You are a judge. Is this answer complete?\\n\\n\u0026#34; f\u0026#34;Question: {question}\\n\\nAnswer: {answer}\u0026#34; ), }], }) judge_response = check.get(\u0026#34;choices\u0026#34;, [{}])[0].get(\u0026#34;message\u0026#34;, {}).get(\u0026#34;content\u0026#34;, \u0026#34;\u0026#34;) is_complete = \u0026#34;yes\u0026#34; in judge_response.lower() or \u0026#34;complete\u0026#34; in judge_response.lower() return answer, is_complete Now our agent can evaluate its own work! This self-reflection capability is crucial for building reliable agents.\nComponent 3: Tool Integration This is where things get really interesting. Tools allow our agent to interact with the outside world:\ndef complete_with_tools(args): \u0026#34;\u0026#34;\u0026#34;Run completion and automatically handle tool calls.\u0026#34;\u0026#34;\u0026#34; completion = openai.chat.completions.create(args) choice0 = completion.get(\u0026#34;choices\u0026#34;, [{}])[0] message = choice0.get(\u0026#34;message\u0026#34;, {}) tool_calls = message.get(\u0026#34;tool_calls\u0026#34;) if tool_calls: # Execute each tool call args[\u0026#34;messages\u0026#34;].append(message) for tool_call in tool_calls: fn_name = tool_call.get(\u0026#34;function\u0026#34;, {}).get(\u0026#34;name\u0026#34;) fn_args = json.loads(tool_call.get(\u0026#34;function\u0026#34;, {}).get(\u0026#34;arguments\u0026#34;, \u0026#34;{}\u0026#34;)) # Execute the tool result = tool_functions[fn_name](fn_args) # Add result back to conversation args[\u0026#34;messages\u0026#34;].append({ \u0026#34;role\u0026#34;: \u0026#34;tool\u0026#34;, \u0026#34;tool_call_id\u0026#34;: tool_call.get(\u0026#34;id\u0026#34;, \u0026#34;\u0026#34;), \u0026#34;content\u0026#34;: result, }) # Continue the conversation return complete_with_tools(args) return completion This recusive approach allows our agent to use tools as needed and continue the conversation based on the results.\nComponent 4: Planning and Memory The final piece is giving our agent the ability to plan and remember:\n# Todo list functions for planning _todos = [] _done = [] def add_todos(params): \u0026#34;\u0026#34;\u0026#34;Add new todos to the planning list.\u0026#34;\u0026#34;\u0026#34; new_todos = params.get(\u0026#34;newTodos\u0026#34;, []) _todos.extend(new_todos) return f\u0026#34;Added {len(new_todos)} todos. Total: {len(_todos)}\u0026#34; def mark_todo_done(params): \u0026#34;\u0026#34;\u0026#34;Mark a todo as completed.\u0026#34;\u0026#34;\u0026#34; todo = params.get(\u0026#34;todo\u0026#34;) if todo in _todos: _todos.remove(todo) _done.append(todo) return f\u0026#34;Completed: {todo}\u0026#34; return f\u0026#34;Todo not found: {todo}\u0026#34; With these planning tools, our agent can break down complex tasks, track progress, and ensure nothing gets forgotten.\nPutting It All Together: The Complete Agent Here\u0026rsquo;s how all these components work together in our main agent loop:\ndef run_agent(goal): prompt = f\u0026#34;\u0026#34;\u0026#34; You are a helpful assistant. Before starting any work, make a plan using your todo list. Use available tools to accomplish tasks. Mark todos as done when complete. Summarize your actions when finished. Today is {datetime.now()} \u0026#34;\u0026#34;\u0026#34; completion = complete_with_tools({ \u0026#34;messages\u0026#34;: [ {\u0026#34;role\u0026#34;: \u0026#34;developer\u0026#34;, \u0026#34;content\u0026#34;: prompt}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: goal}, ], \u0026#34;model\u0026#34;: \u0026#34;llama3.2:latest\u0026#34;, \u0026#34;tool_choice\u0026#34;: \u0026#34;auto\u0026#34;, \u0026#34;tools\u0026#34;: available_tools, }) return completion.get(\u0026#34;choices\u0026#34;, [{}])[0].get(\u0026#34;message\u0026#34;, {}).get(\u0026#34;content\u0026#34;) This simple function creates an agent that can:\nUnderstand the goal Create a plan (using todo tools) Execute actions (using available tools) Track progress (marking todos complete) Provide a summary The Magic in Action When you run this agent with a goal like \u0026ldquo;I want to learn about building agents without a framework,\u0026rdquo; here\u0026rsquo;s what happens:\nPlanning Phase: The agent creates a todo list\n\u0026ldquo;Research agent fundamentals\u0026rdquo; \u0026ldquo;Find relevant resources\u0026rdquo; \u0026ldquo;Summarize key concepts\u0026rdquo; Execution Phase: The agent uses tools to:\nSearch for information Browse websites Take notes Tracking Phase: As each task completes, it marks todos as done\nSummary Phase: Finally, it provides a comprehensive report\nThe beautiful part? You can watch this entire process unfold in real-time through the console output.\nWhat We\u0026rsquo;ve Learned Building this agent from scratch has taught me several key insights:\n1. Simplicity is Powerful You don\u0026rsquo;t need complex frameworks to build effective agents. The core concepts are surprisingly straightforward.\n2. The Loop is Everything The while loop that continues until completion is what transforms an LLM from a chatbot into an agent.\n3. Self-Evaluation is Critical Agents that can judge their own work are far more reliable than those that can\u0026rsquo;t.\n4. Tools Extend Capabilities The real power comes not from the LLM itself, but from the tools it can use to interact with the world.\n5. Planning Makes Agents Reliable Agents that plan before acting are more systematic and less likely to miss important steps.\nWhy This Matters for Your Career Understanding these fundamentals makes you a better AI engineer in several ways:\nFramework Agnostic: You can work with any agentic framework becuase you understand the underlying principles Better Debugging: When production agents fail, you know where to look Innovation Ready: You can spot oportunities for new approaches and optimizations Technical Leadership: You can guide architectural decisions with deep understanding The Future of Agentic Systems We\u0026rsquo;re still in the early days of AI agents. While frameworks like LangGraph and AutoGen are fantastic, the field is evolving rapidly. New patterns, architectures, and approaches are emerging constantly.\nBy understanding the fundamentals - the core loop, tool integration, planning, and self-evaluation - you\u0026rsquo;re positioned to adapt to whatever comes next. You\u0026rsquo;re not just using AI; you\u0026rsquo;re truly understanding it.\nReady to Build Your Own? If this post has sparked your curiosity (and I hope it has!), I encourage you to try building your own agent from scratch. You\u0026rsquo;ll be amazed at how much you learn in the process.\nThe complete code for everything discussed in this post is available on GitHub: https://github.com/imanoop7/agent_from_scratch\nStart simple - build the basic LLM interface, add some conditions, integrate a tool or two, and before you know it, you\u0026rsquo;ll have a working agent. Then you can expand from there.\nRemember: every expert was once a beginner. The best way to truly understand AI agents is to build one yourself. So grab your code editor, fire up that local LLM, and start building!\nWhat will you teach your agent to do?\nHave you built your own agents from scratch? I\u0026rsquo;d love to hear about your experiences and what you learned along the way. Drop me a note and let\u0026rsquo;s discuss the fascinating world of agentic AI!\nCitation\rPlease cite this work as:\nAnoop Maurya. \"Building AI Agents from Scratch: Understanding the Core Components Behind the Magic\". Anoop Maurya (Sep 2025). http://localhost:1313/posts/agent_from_scratch/\rcopy\rOr use the BibTeX citation:\n@article{maurya2025sep,\rtitle = {Building AI Agents from Scratch: Understanding the Core Components Behind the Magic},\rauthor = {Anoop Maurya},\rjournal = {Anoop Maurya},\ryear = {2025},\rmonth = {September},\rurl = \"http://localhost:1313/posts/agent_from_scratch/\"\r}\rcopy\r","permalink":"http://localhost:1313/posts/agent_from_scratch/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eAs a data scientist and AI engineer, I\u0026rsquo;ve spent countless hours working with various agentic frameworks like LangGraph, AutoGen, and CrewAI. While these tools are incredibly powerfull and make our lives easier, I often found myself wondering: \u003cem\u003ewhat actually happens under the hood?\u003c/em\u003e What makes an AI agent tick? How do all these pieces come together to create something that can think, plan, and act autonomously?\u003c/p\u003e\n\u003cp\u003eThat curiosity led me down a fascinating rabbit hole, and eventually to building an AI agent completly from scratch - no frameworks, no abstractions, just pure Python and a deep dive into the fundamental building blocks that make agents work.\u003c/p\u003e","title":"Building AI Agents from Scratch: Understanding the Core Components Behind the Magic"},{"content":"Transformers have transformed the field of sequence modeling by replacing recurrent nets with self-attention. I was curious about how this works under the hood, so I dove into the “Attention Is All You Need” paper and built the components from scratch in PyTorch. In this post I share my understanding and hands-on insights. I’ll cover the motivation behind Transformers, break down the core ideas, and show PyTorch snippets for the key modules: scaled dot-product attention, multi-head attention, positional encoding, and an encoder layer.\nAlong the way I’ll mention a few hiccups and “aha!” moments I had – hopefully making this journey relatable to anyone wrapping their head around this for the first time.\nMotivation Before Transformers, sequence models like RNNs or LSTMs were standard for machine translation or language modeling. These models process data step-by-step, which is hard to parallelize and can struggle with long-range dependencies. The Transformer relies entirely on attention, allowing information from all time steps to be used at once.\nParallelism and efficiency: Self-attention lets us process all tokens at once, so training is much faster on modern hardware. Long-range context: Every token can attend to any other token, capturing long-distance relationships without vanishing gradients. Simplicity of components: The building blocks (attention, feed-forward layers, etc.) are relatively simple operations—feasible to code and inspect directly. Practical success: Many cutting-edge models (BERT, GPT) are based on Transformers, so learning it deeply pays off. Core Concepts The Transformer encoder is built from a few key ideas:\nScaled Dot-Product Attention: Computes attention scores between queries and keys, scales them, and uses softmax to weight values. Multi-Head Attention: Runs several attention “heads” in parallel so the model can jointly attend to different representation subspaces. Positional Encoding: Injects information about token positions, since attention alone is order-agnostic. Feed-Forward Network: A two-layer MLP applied to each position separately and identically. Add \u0026amp; Norm (Residual + LayerNorm): Residual connections and layer normalization for stable training and easy gradient flow. Stacking several encoder layers (typically 6–12) gives you a Transformer encoder.\nScaled Dot-Product Attention Given queries Q, keys K, and values V, scaled dot-product attention computes a weighted sum of the values, where weights come from the similarity of queries with keys. The scores are divided by (\\sqrt{d_k}) to keep values in a reasonable range.\nHigh level steps for a single query (\\mathbf{q}) and keys (\\mathbf{k}_j):\nCompute raw scores: (\\text{score}_j = \\mathbf{q} \\cdot \\mathbf{k}_j^\\top) Scale: (\\text{score}_j / \\sqrt{d_k}) Optionally mask some scores to (-\\infty) Softmax to probabilities: (\\alpha_j) Weighted sum of values: (\\sum_j \\alpha_j \\mathbf{v}_j) PyTorch implementation:\nimport torch import math def scaled_dot_product_attention(Q, K, V, mask=None): \u0026#34;\u0026#34;\u0026#34; Q, K, V: shape (batch, seq_len, d_k) mask: optional mask tensor of shape (batch, seq_len_q, seq_len_k) Returns: (output, attention_weights) \u0026#34;\u0026#34;\u0026#34; d_k = Q.size(-1) # Compute scaled dot products scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k) # Apply the mask (if provided) by setting scores to large negative where mask==0 if mask is not None: scores = scores.masked_fill(mask == 0, float(\u0026#39;-inf\u0026#39;)) # Softmax to get attention weights attn_weights = torch.softmax(scores, dim=-1) # Multiply by values output = torch.matmul(attn_weights, V) return output, attn_weights Here, scores has shape (batch, seq_q, seq_k) after the matmul. Multiplying by V then gives an output of shape (batch, seq_q, d_k). For masks (e.g., padding or causal), use masked_fill(-inf) before softmax so those positions get effectively zero weight.\nMulti-Head Attention One attention head gives one context representation. Multi-head attention runs several heads in parallel with separate linear projections, letting each head focus on different aspects (e.g., local vs. long-range patterns).\nIf d_model is the model dimension and we have h heads, each head uses d_k = d_model / h. There are learned projections Wq, Wk, Wv and an output projection Wo.\nimport torch import torch.nn as nn class MultiHeadAttention(nn.Module): def __init__(self, d_model, num_heads): super().__init__() assert d_model % num_heads == 0, \u0026#34;d_model must be divisible by num_heads\u0026#34; self.num_heads = num_heads self.d_k = d_model // num_heads # Linear layers to project inputs to queries, keys, values self.Wq = nn.Linear(d_model, d_model) self.Wk = nn.Linear(d_model, d_model) self.Wv = nn.Linear(d_model, d_model) # Output linear layer self.Wo = nn.Linear(d_model, d_model) def forward(self, Q, K, V, mask=None): batch_size = Q.size(0) # 1) Linear projections Q = self.Wq(Q) # (batch, seq_len, d_model) K = self.Wk(K) V = self.Wv(V) # 2) Split into heads by reshaping Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) # Now Q, K, V are (batch, heads, seq_len, d_k) # 3) Apply scaled dot-product attention on each head # Combine batch and heads dims for efficiency Q_flat = Q.reshape(-1, Q.size(2), self.d_k) # (batch*heads, seq_q, d_k) K_flat = K.reshape(-1, K.size(2), self.d_k) V_flat = V.reshape(-1, V.size(2), self.d_k) if mask is not None: # Expand mask to match the number of heads mask = mask.unsqueeze(1).repeat(1, self.num_heads, 1, 1) mask = mask.reshape(-1, mask.size(-2), mask.size(-1)) context, attn_weights = scaled_dot_product_attention(Q_flat, K_flat, V_flat, mask) # 4) Concatenate heads context = context.view(batch_size, self.num_heads, -1, self.d_k) context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k) # 5) Final linear layer output = self.Wo(context) # (batch, seq_len, d_model) return output, attn_weights The tensor reshaping is the trickiest part. After splitting into heads, I flatten (batch, heads) to reuse scaled_dot_product_attention, then reshape back and concatenate. Use .contiguous() before .view after a transpose.\nPositional Encoding Pure attention has no notion of order. We add positional encodings to token embeddings. The original paper used fixed sinusoidal encodings:\n[\\text{PE}(pos, 2i) = \\sin\\bigl(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\bigr),\\quad \\text{PE}(pos, 2i+1) = \\cos\\bigl(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\bigr)]\nimport torch import torch.nn as nn import math class PositionalEncoding(nn.Module): def __init__(self, d_model, max_len=5000): super().__init__() pe = torch.zeros(max_len, d_model) position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # (max_len, 1) div_term = torch.exp(torch.arange(0, d_model, 2).float() * (math.log(10000.0) / d_model)) pe[:, 0::2] = torch.sin(position * div_term) # even dims pe[:, 1::2] = torch.cos(position * div_term) # odd dims pe = pe.unsqueeze(0) # (1, max_len, d_model) # Register as buffer so it’s saved with the model but not trainable self.register_buffer(\u0026#39;pe\u0026#39;, pe) def forward(self, x): # x: (batch, seq_len, d_model) seq_len = x.size(1) return x + self.pe[:, :seq_len, :] Shaping pe as (1, max_len, d_model) lets it broadcast-add to a batch of embeddings. Register it as a buffer so it moves with model.to(device) but isn’t updated by the optimizer.\nPutting It Together: Encoder Layer Each encoder layer does:\nSelf-Attention + Add \u0026amp; Norm: x \u0026lt;- LayerNorm(x + MHA(x, x, x)) Feed-Forward + Add \u0026amp; Norm: x \u0026lt;- LayerNorm(x + FFN(x)) import torch.nn as nn class EncoderLayer(nn.Module): def __init__(self, d_model, num_heads, dim_ff, dropout=0.1): super().__init__() self.self_attn = MultiHeadAttention(d_model, num_heads) self.norm1 = nn.LayerNorm(d_model) self.norm2 = nn.LayerNorm(d_model) self.dropout = nn.Dropout(dropout) # Feed-forward network (position-wise) self.fc1 = nn.Linear(d_model, dim_ff) self.fc2 = nn.Linear(dim_ff, d_model) def forward(self, x, mask=None): # 1) Multi-head self-attention attn_output, _ = self.self_attn(x, x, x, mask) x = x + self.dropout(attn_output) # Residual x = self.norm1(x) # 2) Feed-forward network ff_output = self.fc2(self.dropout(torch.relu(self.fc1(x)))) x = x + self.dropout(ff_output) # Residual x = self.norm2(x) return x If x is (batch, seq_len, d_model), the module preserves shapes. Dropout is applied after attention and inside the feed-forward path.\nReflections Understand by implementing: Writing code forces you to handle details (tensor shapes, masking). Quick dummy tests are invaluable. Scaling matters: Dividing by (\\sqrt{d_k}) keeps softmax well-behaved; omitting it caused overly peaky distributions in my tests. Reshaping is tricky but logical: Splitting/combining heads is the most error-prone part; draw shapes while coding. Positional encoding is neat: The sine/cosine patterns give a smooth notion of distance; the model infers relative order. Modularity: Once MultiHeadAttention and EncoderLayer work, stacking them is straightforward and encourages experimentation. Overall, the magic is that self-attention lets each position flexibly gather information from anywhere else, and multi-head attention enriches that process. If you’re learning this, don’t just read the paper—try coding the pieces. Small confusions often turn into big insights once resolved.\n","permalink":"http://localhost:1313/posts/attention_transformer_blog/","summary":"\u003cp\u003eTransformers have transformed the field of sequence modeling by replacing recurrent nets with self-attention. I was curious about how this works under the hood, so I dove into the “Attention Is All You Need” paper and built the components from scratch in PyTorch. In this post I share my understanding and hands-on insights. I’ll cover the motivation behind Transformers, break down the core ideas, and show PyTorch snippets for the key modules: scaled dot-product attention, multi-head attention, positional encoding, and an encoder layer.\u003c/p\u003e","title":"Transformers from Scratch: Implementing Attention Is All You Need"},{"content":"Introduction As a data scientist and AI engineer, I\u0026rsquo;ve spent countless hours working with various agentic frameworks like LangGraph, AutoGen, and CrewAI. While these tools are incredibly powerfull and make our lives easier, I often found myself wondering: what actually happens under the hood? What makes an AI agent tick? How do all these pieces come together to create something that can think, plan, and act autonomously?\nThat curiosity led me down a fascinating rabbit hole, and eventually to building an AI agent completly from scratch - no frameworks, no abstractions, just pure Python and a deep dive into the fundamental building blocks that make agents work.\nIn today\u0026rsquo;s rapidly evolving AI landscape, we\u0026rsquo;re witnessing an explosion of agentic systems. These aren\u0026rsquo;t just chatbots that respond to queries - they\u0026rsquo;re autonomous entities capable of complex reasoning, planning, and executing multi-step tasks. But here\u0026rsquo;s the thing: most of us are building on top of these sophisticated frameworks without truely understanding what\u0026rsquo;s happening beneath the surface.\nWhy does this matter? Because when you understand the fundamentals, you become a better AI engineer. You can debug issues more effectively, optimize performance better, and most importantly, you can innovate beyond the constraints of existing frameworks.\nSo let\u0026rsquo;s pull back the curtain and build an agent from the ground up. By the end of this post, you\u0026rsquo;ll have a clear understanding of what an AI agent really is, why we need them, and how to build one yourself.\nWhat is an AI Agent? Before we dive into the code, let\u0026rsquo;s establish a clear understanding of what we mean by an \u0026ldquo;AI agent.\u0026rdquo;\nAn AI agent is fundamentally different from a simple language model. While an LLM can generate text based on prompts, an agent can:\nThink about problems systematically Plan multi-step solutions Use tools to interact with the world Remember previous interactions and learnings Iterate until a goal is achieved Think of it this way:\nLLM: \u0026ldquo;Tell me about the weather\u0026rdquo; Agent: \u0026ldquo;Check the weather, plan my day accordingly, set reminders, and book restaurant reservations if it\u0026rsquo;s going to rain\u0026rdquo; The agent doesn\u0026rsquo;t just provide information - it takes action.\n##W The Agent Equation: Two Ways to Think About It\nThroughout my research and experimentation, I\u0026rsquo;ve found two complementary ways to conceptualize what an agent is:\nFormula 1: The Loop Perspective agent = llm + memory + tools + while loop This formula emphasizes the iterative nature of agents. The while loop is crucial - it\u0026rsquo;s what allows the agent to keep working until the task is complete, rather than giving a single response.\nFormula 2: The Capability Perspective agent = llm(read+write) + planning + tools + (condition+loop) This formula emphasizes the capabilities of agents. The LLM can both read (understand) and write (generate), planning provides structure, tools provide actions, and conditions with loops provide control flow.\nBoth formulas are correct - they just highlight different aspects of the same underlying system.\nWhy Build Agents from Scratch? You might be thinking: \u0026ldquo;Why re-invent the wheel? LangGraph and AutoGen work perfectly fine!\u0026rdquo;\nAnd you\u0026rsquo;re absolutley right - for production systems, you should definitely use these battle-tested frameworks. But here\u0026rsquo;s why building from scratch is invaluable:\n1. Deep Understanding When you build something yourself, you understand every component. This knowledge is incredibly valuable when debugging production issues or optimizing performance.\n2. Framework Independence Understanding the fundamentals means you\u0026rsquo;re not locked into any particular framework. You can evaluate new tools more effectively and migrate between them when needed.\n3. Innovation Opportunities The best innovations often come from understanding the basics deeply enough to see new possibilities. Many of today\u0026rsquo;s popular frameworks started as someone\u0026rsquo;s experiment in building agents from scratch.\n4. Debugging Superpowers When your production agent misbehaves, knowing what\u0026rsquo;s happening under the hood makes troubleshooting infinitely easier.\nThe Architecture: Building Blocks of Our Agent Let me walk you through the core components we\u0026rsquo;ll be building:\nComponent 1: The LLM Foundation Our agent starts with a simple LLM interface. Here\u0026rsquo;s how we set up the basic communication:\nfrom ai import openai def basic_completion(): completion = openai.chat.completions.create({ \u0026#34;messages\u0026#34;: [ {\u0026#34;role\u0026#34;: \u0026#34;developer\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;You are a helpful assistant.\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What is your name?\u0026#34;}, ], \u0026#34;model\u0026#34;: \u0026#34;llama3.2:latest\u0026#34;, }) return completion.get(\u0026#34;choices\u0026#34;, [None])[0] This is our foundation - a simple way to communicate with an LLM. But it\u0026rsquo;s just a one-shot interaction. To make it agentic, we need more.\nComponent 2: Adding Conditions and Judgment The next step is teaching our agent to evaluate its own responses:\ndef answer_with_judgment(question): # First: get an answer completion = openai.chat.completions.create({ \u0026#34;messages\u0026#34;: [{\u0026#34;role\u0026#34;: \u0026#34;developer\u0026#34;, \u0026#34;content\u0026#34;: question}], \u0026#34;model\u0026#34;: \u0026#34;llama3.2:latest\u0026#34;, }) answer = completion.get(\u0026#34;choices\u0026#34;, [{}])[0].get(\u0026#34;message\u0026#34;, {}).get(\u0026#34;content\u0026#34;) # Second: judge the completeness check = openai.chat.completions.create({ \u0026#34;model\u0026#34;: \u0026#34;llama3.2:latest\u0026#34;, \u0026#34;messages\u0026#34;: [{ \u0026#34;role\u0026#34;: \u0026#34;developer\u0026#34;, \u0026#34;content\u0026#34;: ( \u0026#34;You are a judge. Is this answer complete?\\n\\n\u0026#34; f\u0026#34;Question: {question}\\n\\nAnswer: {answer}\u0026#34; ), }], }) judge_response = check.get(\u0026#34;choices\u0026#34;, [{}])[0].get(\u0026#34;message\u0026#34;, {}).get(\u0026#34;content\u0026#34;, \u0026#34;\u0026#34;) is_complete = \u0026#34;yes\u0026#34; in judge_response.lower() or \u0026#34;complete\u0026#34; in judge_response.lower() return answer, is_complete Now our agent can evaluate its own work! This self-reflection capability is crucial for building reliable agents.\nComponent 3: Tool Integration This is where things get really interesting. Tools allow our agent to interact with the outside world:\ndef complete_with_tools(args): \u0026#34;\u0026#34;\u0026#34;Run completion and automatically handle tool calls.\u0026#34;\u0026#34;\u0026#34; completion = openai.chat.completions.create(args) choice0 = completion.get(\u0026#34;choices\u0026#34;, [{}])[0] message = choice0.get(\u0026#34;message\u0026#34;, {}) tool_calls = message.get(\u0026#34;tool_calls\u0026#34;) if tool_calls: # Execute each tool call args[\u0026#34;messages\u0026#34;].append(message) for tool_call in tool_calls: fn_name = tool_call.get(\u0026#34;function\u0026#34;, {}).get(\u0026#34;name\u0026#34;) fn_args = json.loads(tool_call.get(\u0026#34;function\u0026#34;, {}).get(\u0026#34;arguments\u0026#34;, \u0026#34;{}\u0026#34;)) # Execute the tool result = tool_functions[fn_name](fn_args) # Add result back to conversation args[\u0026#34;messages\u0026#34;].append({ \u0026#34;role\u0026#34;: \u0026#34;tool\u0026#34;, \u0026#34;tool_call_id\u0026#34;: tool_call.get(\u0026#34;id\u0026#34;, \u0026#34;\u0026#34;), \u0026#34;content\u0026#34;: result, }) # Continue the conversation return complete_with_tools(args) return completion This recusive approach allows our agent to use tools as needed and continue the conversation based on the results.\nComponent 4: Planning and Memory The final piece is giving our agent the ability to plan and remember:\n# Todo list functions for planning _todos = [] _done = [] def add_todos(params): \u0026#34;\u0026#34;\u0026#34;Add new todos to the planning list.\u0026#34;\u0026#34;\u0026#34; new_todos = params.get(\u0026#34;newTodos\u0026#34;, []) _todos.extend(new_todos) return f\u0026#34;Added {len(new_todos)} todos. Total: {len(_todos)}\u0026#34; def mark_todo_done(params): \u0026#34;\u0026#34;\u0026#34;Mark a todo as completed.\u0026#34;\u0026#34;\u0026#34; todo = params.get(\u0026#34;todo\u0026#34;) if todo in _todos: _todos.remove(todo) _done.append(todo) return f\u0026#34;Completed: {todo}\u0026#34; return f\u0026#34;Todo not found: {todo}\u0026#34; With these planning tools, our agent can break down complex tasks, track progress, and ensure nothing gets forgotten.\nPutting It All Together: The Complete Agent Here\u0026rsquo;s how all these components work together in our main agent loop:\ndef run_agent(goal): prompt = f\u0026#34;\u0026#34;\u0026#34; You are a helpful assistant. Before starting any work, make a plan using your todo list. Use available tools to accomplish tasks. Mark todos as done when complete. Summarize your actions when finished. Today is {datetime.now()} \u0026#34;\u0026#34;\u0026#34; completion = complete_with_tools({ \u0026#34;messages\u0026#34;: [ {\u0026#34;role\u0026#34;: \u0026#34;developer\u0026#34;, \u0026#34;content\u0026#34;: prompt}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: goal}, ], \u0026#34;model\u0026#34;: \u0026#34;llama3.2:latest\u0026#34;, \u0026#34;tool_choice\u0026#34;: \u0026#34;auto\u0026#34;, \u0026#34;tools\u0026#34;: available_tools, }) return completion.get(\u0026#34;choices\u0026#34;, [{}])[0].get(\u0026#34;message\u0026#34;, {}).get(\u0026#34;content\u0026#34;) This simple function creates an agent that can:\nUnderstand the goal Create a plan (using todo tools) Execute actions (using available tools) Track progress (marking todos complete) Provide a summary The Magic in Action When you run this agent with a goal like \u0026ldquo;I want to learn about building agents without a framework,\u0026rdquo; here\u0026rsquo;s what happens:\nPlanning Phase: The agent creates a todo list\n\u0026ldquo;Research agent fundamentals\u0026rdquo; \u0026ldquo;Find relevant resources\u0026rdquo; \u0026ldquo;Summarize key concepts\u0026rdquo; Execution Phase: The agent uses tools to:\nSearch for information Browse websites Take notes Tracking Phase: As each task completes, it marks todos as done\nSummary Phase: Finally, it provides a comprehensive report\nThe beautiful part? You can watch this entire process unfold in real-time through the console output.\nWhat We\u0026rsquo;ve Learned Building this agent from scratch has taught me several key insights:\n1. Simplicity is Powerful You don\u0026rsquo;t need complex frameworks to build effective agents. The core concepts are surprisingly straightforward.\n2. The Loop is Everything The while loop that continues until completion is what transforms an LLM from a chatbot into an agent.\n3. Self-Evaluation is Critical Agents that can judge their own work are far more reliable than those that can\u0026rsquo;t.\n4. Tools Extend Capabilities The real power comes not from the LLM itself, but from the tools it can use to interact with the world.\n5. Planning Makes Agents Reliable Agents that plan before acting are more systematic and less likely to miss important steps.\nWhy This Matters for Your Career Understanding these fundamentals makes you a better AI engineer in several ways:\nFramework Agnostic: You can work with any agentic framework becuase you understand the underlying principles Better Debugging: When production agents fail, you know where to look Innovation Ready: You can spot oportunities for new approaches and optimizations Technical Leadership: You can guide architectural decisions with deep understanding The Future of Agentic Systems We\u0026rsquo;re still in the early days of AI agents. While frameworks like LangGraph and AutoGen are fantastic, the field is evolving rapidly. New patterns, architectures, and approaches are emerging constantly.\nBy understanding the fundamentals - the core loop, tool integration, planning, and self-evaluation - you\u0026rsquo;re positioned to adapt to whatever comes next. You\u0026rsquo;re not just using AI; you\u0026rsquo;re truly understanding it.\nReady to Build Your Own? If this post has sparked your curiosity (and I hope it has!), I encourage you to try building your own agent from scratch. You\u0026rsquo;ll be amazed at how much you learn in the process.\nThe complete code for everything discussed in this post is available on GitHub: https://github.com/imanoop7/agent_from_scratch\nStart simple - build the basic LLM interface, add some conditions, integrate a tool or two, and before you know it, you\u0026rsquo;ll have a working agent. Then you can expand from there.\nRemember: every expert was once a beginner. The best way to truly understand AI agents is to build one yourself. So grab your code editor, fire up that local LLM, and start building!\nWhat will you teach your agent to do?\nHave you built your own agents from scratch? I\u0026rsquo;d love to hear about your experiences and what you learned along the way. Drop me a note and let\u0026rsquo;s discuss the fascinating world of agentic AI!\nCitation\rPlease cite this work as:\nAnoop Maurya. \"Building AI Agents from Scratch: Understanding the Core Components Behind the Magic\". Anoop Maurya (Sep 2025). http://localhost:1313/posts/agent_from_scratch/\rcopy\rOr use the BibTeX citation:\n@article{maurya2025sep,\rtitle = {Building AI Agents from Scratch: Understanding the Core Components Behind the Magic},\rauthor = {Anoop Maurya},\rjournal = {Anoop Maurya},\ryear = {2025},\rmonth = {September},\rurl = \"http://localhost:1313/posts/agent_from_scratch/\"\r}\rcopy\r","permalink":"http://localhost:1313/posts/agent_from_scratch/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eAs a data scientist and AI engineer, I\u0026rsquo;ve spent countless hours working with various agentic frameworks like LangGraph, AutoGen, and CrewAI. While these tools are incredibly powerfull and make our lives easier, I often found myself wondering: \u003cem\u003ewhat actually happens under the hood?\u003c/em\u003e What makes an AI agent tick? How do all these pieces come together to create something that can think, plan, and act autonomously?\u003c/p\u003e\n\u003cp\u003eThat curiosity led me down a fascinating rabbit hole, and eventually to building an AI agent completly from scratch - no frameworks, no abstractions, just pure Python and a deep dive into the fundamental building blocks that make agents work.\u003c/p\u003e","title":"Building AI Agents from Scratch: Understanding the Core Components Behind the Magic"},{"content":"Transformers have transformed the field of sequence modeling by replacing recurrent nets with self-attention. I was curious about how this works under the hood, so I dove into the “Attention Is All You Need” paper and built the components from scratch in PyTorch. In this post I share my understanding and hands-on insights. I’ll cover the motivation behind Transformers, break down the core ideas, and show PyTorch snippets for the key modules: scaled dot-product attention, multi-head attention, positional encoding, and an encoder layer.\nAlong the way I’ll mention a few hiccups and “aha!” moments I had – hopefully making this journey relatable to anyone wrapping their head around this for the first time.\nMotivation Before Transformers, sequence models like RNNs or LSTMs were standard for machine translation or language modeling. These models process data step-by-step, which is hard to parallelize and can struggle with long-range dependencies. The Transformer relies entirely on attention, allowing information from all time steps to be used at once.\nParallelism and efficiency: Self-attention lets us process all tokens at once, so training is much faster on modern hardware. Long-range context: Every token can attend to any other token, capturing long-distance relationships without vanishing gradients. Simplicity of components: The building blocks (attention, feed-forward layers, etc.) are relatively simple operations—feasible to code and inspect directly. Practical success: Many cutting-edge models (BERT, GPT) are based on Transformers, so learning it deeply pays off. Core Concepts The Transformer encoder is built from a few key ideas:\nScaled Dot-Product Attention: Computes attention scores between queries and keys, scales them, and uses softmax to weight values. Multi-Head Attention: Runs several attention “heads” in parallel so the model can jointly attend to different representation subspaces. Positional Encoding: Injects information about token positions, since attention alone is order-agnostic. Feed-Forward Network: A two-layer MLP applied to each position separately and identically. Add \u0026amp; Norm (Residual + LayerNorm): Residual connections and layer normalization for stable training and easy gradient flow. Stacking several encoder layers (typically 6–12) gives you a Transformer encoder.\nScaled Dot-Product Attention Given queries Q, keys K, and values V, scaled dot-product attention computes a weighted sum of the values, where weights come from the similarity of queries with keys. The scores are divided by (\\sqrt{d_k}) to keep values in a reasonable range.\nHigh level steps for a single query (\\mathbf{q}) and keys (\\mathbf{k}_j):\nCompute raw scores: (\\text{score}_j = \\mathbf{q} \\cdot \\mathbf{k}_j^\\top) Scale: (\\text{score}_j / \\sqrt{d_k}) Optionally mask some scores to (-\\infty) Softmax to probabilities: (\\alpha_j) Weighted sum of values: (\\sum_j \\alpha_j \\mathbf{v}_j) PyTorch implementation:\nimport torch import math def scaled_dot_product_attention(Q, K, V, mask=None): \u0026#34;\u0026#34;\u0026#34; Q, K, V: shape (batch, seq_len, d_k) mask: optional mask tensor of shape (batch, seq_len_q, seq_len_k) Returns: (output, attention_weights) \u0026#34;\u0026#34;\u0026#34; d_k = Q.size(-1) # Compute scaled dot products scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k) # Apply the mask (if provided) by setting scores to large negative where mask==0 if mask is not None: scores = scores.masked_fill(mask == 0, float(\u0026#39;-inf\u0026#39;)) # Softmax to get attention weights attn_weights = torch.softmax(scores, dim=-1) # Multiply by values output = torch.matmul(attn_weights, V) return output, attn_weights Here, scores has shape (batch, seq_q, seq_k) after the matmul. Multiplying by V then gives an output of shape (batch, seq_q, d_k). For masks (e.g., padding or causal), use masked_fill(-inf) before softmax so those positions get effectively zero weight.\nMulti-Head Attention One attention head gives one context representation. Multi-head attention runs several heads in parallel with separate linear projections, letting each head focus on different aspects (e.g., local vs. long-range patterns).\nIf d_model is the model dimension and we have h heads, each head uses d_k = d_model / h. There are learned projections Wq, Wk, Wv and an output projection Wo.\nimport torch import torch.nn as nn class MultiHeadAttention(nn.Module): def __init__(self, d_model, num_heads): super().__init__() assert d_model % num_heads == 0, \u0026#34;d_model must be divisible by num_heads\u0026#34; self.num_heads = num_heads self.d_k = d_model // num_heads # Linear layers to project inputs to queries, keys, values self.Wq = nn.Linear(d_model, d_model) self.Wk = nn.Linear(d_model, d_model) self.Wv = nn.Linear(d_model, d_model) # Output linear layer self.Wo = nn.Linear(d_model, d_model) def forward(self, Q, K, V, mask=None): batch_size = Q.size(0) # 1) Linear projections Q = self.Wq(Q) # (batch, seq_len, d_model) K = self.Wk(K) V = self.Wv(V) # 2) Split into heads by reshaping Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) # Now Q, K, V are (batch, heads, seq_len, d_k) # 3) Apply scaled dot-product attention on each head # Combine batch and heads dims for efficiency Q_flat = Q.reshape(-1, Q.size(2), self.d_k) # (batch*heads, seq_q, d_k) K_flat = K.reshape(-1, K.size(2), self.d_k) V_flat = V.reshape(-1, V.size(2), self.d_k) if mask is not None: # Expand mask to match the number of heads mask = mask.unsqueeze(1).repeat(1, self.num_heads, 1, 1) mask = mask.reshape(-1, mask.size(-2), mask.size(-1)) context, attn_weights = scaled_dot_product_attention(Q_flat, K_flat, V_flat, mask) # 4) Concatenate heads context = context.view(batch_size, self.num_heads, -1, self.d_k) context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k) # 5) Final linear layer output = self.Wo(context) # (batch, seq_len, d_model) return output, attn_weights The tensor reshaping is the trickiest part. After splitting into heads, I flatten (batch, heads) to reuse scaled_dot_product_attention, then reshape back and concatenate. Use .contiguous() before .view after a transpose.\nPositional Encoding Pure attention has no notion of order. We add positional encodings to token embeddings. The original paper used fixed sinusoidal encodings:\n[\\text{PE}(pos, 2i) = \\sin\\bigl(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\bigr),\\quad \\text{PE}(pos, 2i+1) = \\cos\\bigl(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\bigr)]\nimport torch import torch.nn as nn import math class PositionalEncoding(nn.Module): def __init__(self, d_model, max_len=5000): super().__init__() pe = torch.zeros(max_len, d_model) position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # (max_len, 1) div_term = torch.exp(torch.arange(0, d_model, 2).float() * (math.log(10000.0) / d_model)) pe[:, 0::2] = torch.sin(position * div_term) # even dims pe[:, 1::2] = torch.cos(position * div_term) # odd dims pe = pe.unsqueeze(0) # (1, max_len, d_model) # Register as buffer so it’s saved with the model but not trainable self.register_buffer(\u0026#39;pe\u0026#39;, pe) def forward(self, x): # x: (batch, seq_len, d_model) seq_len = x.size(1) return x + self.pe[:, :seq_len, :] Shaping pe as (1, max_len, d_model) lets it broadcast-add to a batch of embeddings. Register it as a buffer so it moves with model.to(device) but isn’t updated by the optimizer.\nPutting It Together: Encoder Layer Each encoder layer does:\nSelf-Attention + Add \u0026amp; Norm: x \u0026lt;- LayerNorm(x + MHA(x, x, x)) Feed-Forward + Add \u0026amp; Norm: x \u0026lt;- LayerNorm(x + FFN(x)) import torch.nn as nn class EncoderLayer(nn.Module): def __init__(self, d_model, num_heads, dim_ff, dropout=0.1): super().__init__() self.self_attn = MultiHeadAttention(d_model, num_heads) self.norm1 = nn.LayerNorm(d_model) self.norm2 = nn.LayerNorm(d_model) self.dropout = nn.Dropout(dropout) # Feed-forward network (position-wise) self.fc1 = nn.Linear(d_model, dim_ff) self.fc2 = nn.Linear(dim_ff, d_model) def forward(self, x, mask=None): # 1) Multi-head self-attention attn_output, _ = self.self_attn(x, x, x, mask) x = x + self.dropout(attn_output) # Residual x = self.norm1(x) # 2) Feed-forward network ff_output = self.fc2(self.dropout(torch.relu(self.fc1(x)))) x = x + self.dropout(ff_output) # Residual x = self.norm2(x) return x If x is (batch, seq_len, d_model), the module preserves shapes. Dropout is applied after attention and inside the feed-forward path.\nReflections Understand by implementing: Writing code forces you to handle details (tensor shapes, masking). Quick dummy tests are invaluable. Scaling matters: Dividing by (\\sqrt{d_k}) keeps softmax well-behaved; omitting it caused overly peaky distributions in my tests. Reshaping is tricky but logical: Splitting/combining heads is the most error-prone part; draw shapes while coding. Positional encoding is neat: The sine/cosine patterns give a smooth notion of distance; the model infers relative order. Modularity: Once MultiHeadAttention and EncoderLayer work, stacking them is straightforward and encourages experimentation. Overall, the magic is that self-attention lets each position flexibly gather information from anywhere else, and multi-head attention enriches that process. If you’re learning this, don’t just read the paper—try coding the pieces. Small confusions often turn into big insights once resolved.\n","permalink":"http://localhost:1313/posts/attention_transformer_blog/","summary":"\u003cp\u003eTransformers have transformed the field of sequence modeling by replacing recurrent nets with self-attention. I was curious about how this works under the hood, so I dove into the “Attention Is All You Need” paper and built the components from scratch in PyTorch. In this post I share my understanding and hands-on insights. I’ll cover the motivation behind Transformers, break down the core ideas, and show PyTorch snippets for the key modules: scaled dot-product attention, multi-head attention, positional encoding, and an encoder layer.\u003c/p\u003e","title":"Transformers from Scratch: Implementing Attention Is All You Need"},{"content":"Introduction As a data scientist and AI engineer, I\u0026rsquo;ve spent countless hours working with various agentic frameworks like LangGraph, AutoGen, and CrewAI. While these tools are incredibly powerfull and make our lives easier, I often found myself wondering: what actually happens under the hood? What makes an AI agent tick? How do all these pieces come together to create something that can think, plan, and act autonomously?\nThat curiosity led me down a fascinating rabbit hole, and eventually to building an AI agent completly from scratch - no frameworks, no abstractions, just pure Python and a deep dive into the fundamental building blocks that make agents work.\nIn today\u0026rsquo;s rapidly evolving AI landscape, we\u0026rsquo;re witnessing an explosion of agentic systems. These aren\u0026rsquo;t just chatbots that respond to queries - they\u0026rsquo;re autonomous entities capable of complex reasoning, planning, and executing multi-step tasks. But here\u0026rsquo;s the thing: most of us are building on top of these sophisticated frameworks without truely understanding what\u0026rsquo;s happening beneath the surface.\nWhy does this matter? Because when you understand the fundamentals, you become a better AI engineer. You can debug issues more effectively, optimize performance better, and most importantly, you can innovate beyond the constraints of existing frameworks.\nSo let\u0026rsquo;s pull back the curtain and build an agent from the ground up. By the end of this post, you\u0026rsquo;ll have a clear understanding of what an AI agent really is, why we need them, and how to build one yourself.\nWhat is an AI Agent? Before we dive into the code, let\u0026rsquo;s establish a clear understanding of what we mean by an \u0026ldquo;AI agent.\u0026rdquo;\nAn AI agent is fundamentally different from a simple language model. While an LLM can generate text based on prompts, an agent can:\nThink about problems systematically Plan multi-step solutions Use tools to interact with the world Remember previous interactions and learnings Iterate until a goal is achieved Think of it this way:\nLLM: \u0026ldquo;Tell me about the weather\u0026rdquo; Agent: \u0026ldquo;Check the weather, plan my day accordingly, set reminders, and book restaurant reservations if it\u0026rsquo;s going to rain\u0026rdquo; The agent doesn\u0026rsquo;t just provide information - it takes action.\n##W The Agent Equation: Two Ways to Think About It\nThroughout my research and experimentation, I\u0026rsquo;ve found two complementary ways to conceptualize what an agent is:\nFormula 1: The Loop Perspective agent = llm + memory + tools + while loop This formula emphasizes the iterative nature of agents. The while loop is crucial - it\u0026rsquo;s what allows the agent to keep working until the task is complete, rather than giving a single response.\nFormula 2: The Capability Perspective agent = llm(read+write) + planning + tools + (condition+loop) This formula emphasizes the capabilities of agents. The LLM can both read (understand) and write (generate), planning provides structure, tools provide actions, and conditions with loops provide control flow.\nBoth formulas are correct - they just highlight different aspects of the same underlying system.\nWhy Build Agents from Scratch? You might be thinking: \u0026ldquo;Why re-invent the wheel? LangGraph and AutoGen work perfectly fine!\u0026rdquo;\nAnd you\u0026rsquo;re absolutley right - for production systems, you should definitely use these battle-tested frameworks. But here\u0026rsquo;s why building from scratch is invaluable:\n1. Deep Understanding When you build something yourself, you understand every component. This knowledge is incredibly valuable when debugging production issues or optimizing performance.\n2. Framework Independence Understanding the fundamentals means you\u0026rsquo;re not locked into any particular framework. You can evaluate new tools more effectively and migrate between them when needed.\n3. Innovation Opportunities The best innovations often come from understanding the basics deeply enough to see new possibilities. Many of today\u0026rsquo;s popular frameworks started as someone\u0026rsquo;s experiment in building agents from scratch.\n4. Debugging Superpowers When your production agent misbehaves, knowing what\u0026rsquo;s happening under the hood makes troubleshooting infinitely easier.\nThe Architecture: Building Blocks of Our Agent Let me walk you through the core components we\u0026rsquo;ll be building:\nComponent 1: The LLM Foundation Our agent starts with a simple LLM interface. Here\u0026rsquo;s how we set up the basic communication:\nfrom ai import openai def basic_completion(): completion = openai.chat.completions.create({ \u0026#34;messages\u0026#34;: [ {\u0026#34;role\u0026#34;: \u0026#34;developer\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;You are a helpful assistant.\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What is your name?\u0026#34;}, ], \u0026#34;model\u0026#34;: \u0026#34;llama3.2:latest\u0026#34;, }) return completion.get(\u0026#34;choices\u0026#34;, [None])[0] This is our foundation - a simple way to communicate with an LLM. But it\u0026rsquo;s just a one-shot interaction. To make it agentic, we need more.\nComponent 2: Adding Conditions and Judgment The next step is teaching our agent to evaluate its own responses:\ndef answer_with_judgment(question): # First: get an answer completion = openai.chat.completions.create({ \u0026#34;messages\u0026#34;: [{\u0026#34;role\u0026#34;: \u0026#34;developer\u0026#34;, \u0026#34;content\u0026#34;: question}], \u0026#34;model\u0026#34;: \u0026#34;llama3.2:latest\u0026#34;, }) answer = completion.get(\u0026#34;choices\u0026#34;, [{}])[0].get(\u0026#34;message\u0026#34;, {}).get(\u0026#34;content\u0026#34;) # Second: judge the completeness check = openai.chat.completions.create({ \u0026#34;model\u0026#34;: \u0026#34;llama3.2:latest\u0026#34;, \u0026#34;messages\u0026#34;: [{ \u0026#34;role\u0026#34;: \u0026#34;developer\u0026#34;, \u0026#34;content\u0026#34;: ( \u0026#34;You are a judge. Is this answer complete?\\n\\n\u0026#34; f\u0026#34;Question: {question}\\n\\nAnswer: {answer}\u0026#34; ), }], }) judge_response = check.get(\u0026#34;choices\u0026#34;, [{}])[0].get(\u0026#34;message\u0026#34;, {}).get(\u0026#34;content\u0026#34;, \u0026#34;\u0026#34;) is_complete = \u0026#34;yes\u0026#34; in judge_response.lower() or \u0026#34;complete\u0026#34; in judge_response.lower() return answer, is_complete Now our agent can evaluate its own work! This self-reflection capability is crucial for building reliable agents.\nComponent 3: Tool Integration This is where things get really interesting. Tools allow our agent to interact with the outside world:\ndef complete_with_tools(args): \u0026#34;\u0026#34;\u0026#34;Run completion and automatically handle tool calls.\u0026#34;\u0026#34;\u0026#34; completion = openai.chat.completions.create(args) choice0 = completion.get(\u0026#34;choices\u0026#34;, [{}])[0] message = choice0.get(\u0026#34;message\u0026#34;, {}) tool_calls = message.get(\u0026#34;tool_calls\u0026#34;) if tool_calls: # Execute each tool call args[\u0026#34;messages\u0026#34;].append(message) for tool_call in tool_calls: fn_name = tool_call.get(\u0026#34;function\u0026#34;, {}).get(\u0026#34;name\u0026#34;) fn_args = json.loads(tool_call.get(\u0026#34;function\u0026#34;, {}).get(\u0026#34;arguments\u0026#34;, \u0026#34;{}\u0026#34;)) # Execute the tool result = tool_functions[fn_name](fn_args) # Add result back to conversation args[\u0026#34;messages\u0026#34;].append({ \u0026#34;role\u0026#34;: \u0026#34;tool\u0026#34;, \u0026#34;tool_call_id\u0026#34;: tool_call.get(\u0026#34;id\u0026#34;, \u0026#34;\u0026#34;), \u0026#34;content\u0026#34;: result, }) # Continue the conversation return complete_with_tools(args) return completion This recusive approach allows our agent to use tools as needed and continue the conversation based on the results.\nComponent 4: Planning and Memory The final piece is giving our agent the ability to plan and remember:\n# Todo list functions for planning _todos = [] _done = [] def add_todos(params): \u0026#34;\u0026#34;\u0026#34;Add new todos to the planning list.\u0026#34;\u0026#34;\u0026#34; new_todos = params.get(\u0026#34;newTodos\u0026#34;, []) _todos.extend(new_todos) return f\u0026#34;Added {len(new_todos)} todos. Total: {len(_todos)}\u0026#34; def mark_todo_done(params): \u0026#34;\u0026#34;\u0026#34;Mark a todo as completed.\u0026#34;\u0026#34;\u0026#34; todo = params.get(\u0026#34;todo\u0026#34;) if todo in _todos: _todos.remove(todo) _done.append(todo) return f\u0026#34;Completed: {todo}\u0026#34; return f\u0026#34;Todo not found: {todo}\u0026#34; With these planning tools, our agent can break down complex tasks, track progress, and ensure nothing gets forgotten.\nPutting It All Together: The Complete Agent Here\u0026rsquo;s how all these components work together in our main agent loop:\ndef run_agent(goal): prompt = f\u0026#34;\u0026#34;\u0026#34; You are a helpful assistant. Before starting any work, make a plan using your todo list. Use available tools to accomplish tasks. Mark todos as done when complete. Summarize your actions when finished. Today is {datetime.now()} \u0026#34;\u0026#34;\u0026#34; completion = complete_with_tools({ \u0026#34;messages\u0026#34;: [ {\u0026#34;role\u0026#34;: \u0026#34;developer\u0026#34;, \u0026#34;content\u0026#34;: prompt}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: goal}, ], \u0026#34;model\u0026#34;: \u0026#34;llama3.2:latest\u0026#34;, \u0026#34;tool_choice\u0026#34;: \u0026#34;auto\u0026#34;, \u0026#34;tools\u0026#34;: available_tools, }) return completion.get(\u0026#34;choices\u0026#34;, [{}])[0].get(\u0026#34;message\u0026#34;, {}).get(\u0026#34;content\u0026#34;) This simple function creates an agent that can:\nUnderstand the goal Create a plan (using todo tools) Execute actions (using available tools) Track progress (marking todos complete) Provide a summary The Magic in Action When you run this agent with a goal like \u0026ldquo;I want to learn about building agents without a framework,\u0026rdquo; here\u0026rsquo;s what happens:\nPlanning Phase: The agent creates a todo list\n\u0026ldquo;Research agent fundamentals\u0026rdquo; \u0026ldquo;Find relevant resources\u0026rdquo; \u0026ldquo;Summarize key concepts\u0026rdquo; Execution Phase: The agent uses tools to:\nSearch for information Browse websites Take notes Tracking Phase: As each task completes, it marks todos as done\nSummary Phase: Finally, it provides a comprehensive report\nThe beautiful part? You can watch this entire process unfold in real-time through the console output.\nWhat We\u0026rsquo;ve Learned Building this agent from scratch has taught me several key insights:\n1. Simplicity is Powerful You don\u0026rsquo;t need complex frameworks to build effective agents. The core concepts are surprisingly straightforward.\n2. The Loop is Everything The while loop that continues until completion is what transforms an LLM from a chatbot into an agent.\n3. Self-Evaluation is Critical Agents that can judge their own work are far more reliable than those that can\u0026rsquo;t.\n4. Tools Extend Capabilities The real power comes not from the LLM itself, but from the tools it can use to interact with the world.\n5. Planning Makes Agents Reliable Agents that plan before acting are more systematic and less likely to miss important steps.\nWhy This Matters for Your Career Understanding these fundamentals makes you a better AI engineer in several ways:\nFramework Agnostic: You can work with any agentic framework becuase you understand the underlying principles Better Debugging: When production agents fail, you know where to look Innovation Ready: You can spot oportunities for new approaches and optimizations Technical Leadership: You can guide architectural decisions with deep understanding The Future of Agentic Systems We\u0026rsquo;re still in the early days of AI agents. While frameworks like LangGraph and AutoGen are fantastic, the field is evolving rapidly. New patterns, architectures, and approaches are emerging constantly.\nBy understanding the fundamentals - the core loop, tool integration, planning, and self-evaluation - you\u0026rsquo;re positioned to adapt to whatever comes next. You\u0026rsquo;re not just using AI; you\u0026rsquo;re truly understanding it.\nReady to Build Your Own? If this post has sparked your curiosity (and I hope it has!), I encourage you to try building your own agent from scratch. You\u0026rsquo;ll be amazed at how much you learn in the process.\nThe complete code for everything discussed in this post is available on GitHub: https://github.com/imanoop7/agent_from_scratch\nStart simple - build the basic LLM interface, add some conditions, integrate a tool or two, and before you know it, you\u0026rsquo;ll have a working agent. Then you can expand from there.\nRemember: every expert was once a beginner. The best way to truly understand AI agents is to build one yourself. So grab your code editor, fire up that local LLM, and start building!\nWhat will you teach your agent to do?\nHave you built your own agents from scratch? I\u0026rsquo;d love to hear about your experiences and what you learned along the way. Drop me a note and let\u0026rsquo;s discuss the fascinating world of agentic AI!\nCitation\rPlease cite this work as:\nAnoop Maurya. \"Building AI Agents from Scratch: Understanding the Core Components Behind the Magic\". Anoop Maurya (Sep 2025). http://localhost:1313/posts/agent_from_scratch/\rcopy\rOr use the BibTeX citation:\n@article{maurya2025sep,\rtitle = {Building AI Agents from Scratch: Understanding the Core Components Behind the Magic},\rauthor = {Anoop Maurya},\rjournal = {Anoop Maurya},\ryear = {2025},\rmonth = {September},\rurl = \"http://localhost:1313/posts/agent_from_scratch/\"\r}\rcopy\r","permalink":"http://localhost:1313/posts/agent_from_scratch/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eAs a data scientist and AI engineer, I\u0026rsquo;ve spent countless hours working with various agentic frameworks like LangGraph, AutoGen, and CrewAI. While these tools are incredibly powerfull and make our lives easier, I often found myself wondering: \u003cem\u003ewhat actually happens under the hood?\u003c/em\u003e What makes an AI agent tick? How do all these pieces come together to create something that can think, plan, and act autonomously?\u003c/p\u003e\n\u003cp\u003eThat curiosity led me down a fascinating rabbit hole, and eventually to building an AI agent completly from scratch - no frameworks, no abstractions, just pure Python and a deep dive into the fundamental building blocks that make agents work.\u003c/p\u003e","title":"Building AI Agents from Scratch: Understanding the Core Components Behind the Magic"},{"content":"Transformers have transformed the field of sequence modeling by replacing recurrent nets with self-attention. I was curious about how this works under the hood, so I dove into the “Attention Is All You Need” paper and built the components from scratch in PyTorch. In this post I share my understanding and hands-on insights. I’ll cover the motivation behind Transformers, break down the core ideas, and show PyTorch snippets for the key modules: scaled dot-product attention, multi-head attention, positional encoding, and an encoder layer.\nAlong the way I’ll mention a few hiccups and “aha!” moments I had – hopefully making this journey relatable to anyone wrapping their head around this for the first time.\nMotivation Before Transformers, sequence models like RNNs or LSTMs were standard for machine translation or language modeling. These models process data step-by-step, which is hard to parallelize and can struggle with long-range dependencies. The Transformer relies entirely on attention, allowing information from all time steps to be used at once.\nParallelism and efficiency: Self-attention lets us process all tokens at once, so training is much faster on modern hardware. Long-range context: Every token can attend to any other token, capturing long-distance relationships without vanishing gradients. Simplicity of components: The building blocks (attention, feed-forward layers, etc.) are relatively simple operations—feasible to code and inspect directly. Practical success: Many cutting-edge models (BERT, GPT) are based on Transformers, so learning it deeply pays off. Core Concepts The Transformer encoder is built from a few key ideas:\nScaled Dot-Product Attention: Computes attention scores between queries and keys, scales them, and uses softmax to weight values. Multi-Head Attention: Runs several attention “heads” in parallel so the model can jointly attend to different representation subspaces. Positional Encoding: Injects information about token positions, since attention alone is order-agnostic. Feed-Forward Network: A two-layer MLP applied to each position separately and identically. Add \u0026amp; Norm (Residual + LayerNorm): Residual connections and layer normalization for stable training and easy gradient flow. Stacking several encoder layers (typically 6–12) gives you a Transformer encoder.\nScaled Dot-Product Attention Given queries Q, keys K, and values V, scaled dot-product attention computes a weighted sum of the values, where weights come from the similarity of queries with keys. The scores are divided by (\\sqrt{d_k}) to keep values in a reasonable range.\nHigh level steps for a single query (\\mathbf{q}) and keys (\\mathbf{k}_j):\nCompute raw scores: (\\text{score}_j = \\mathbf{q} \\cdot \\mathbf{k}_j^\\top) Scale: (\\text{score}_j / \\sqrt{d_k}) Optionally mask some scores to (-\\infty) Softmax to probabilities: (\\alpha_j) Weighted sum of values: (\\sum_j \\alpha_j \\mathbf{v}_j) PyTorch implementation:\nimport torch import math def scaled_dot_product_attention(Q, K, V, mask=None): \u0026#34;\u0026#34;\u0026#34; Q, K, V: shape (batch, seq_len, d_k) mask: optional mask tensor of shape (batch, seq_len_q, seq_len_k) Returns: (output, attention_weights) \u0026#34;\u0026#34;\u0026#34; d_k = Q.size(-1) # Compute scaled dot products scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k) # Apply the mask (if provided) by setting scores to large negative where mask==0 if mask is not None: scores = scores.masked_fill(mask == 0, float(\u0026#39;-inf\u0026#39;)) # Softmax to get attention weights attn_weights = torch.softmax(scores, dim=-1) # Multiply by values output = torch.matmul(attn_weights, V) return output, attn_weights Here, scores has shape (batch, seq_q, seq_k) after the matmul. Multiplying by V then gives an output of shape (batch, seq_q, d_k). For masks (e.g., padding or causal), use masked_fill(-inf) before softmax so those positions get effectively zero weight.\nMulti-Head Attention One attention head gives one context representation. Multi-head attention runs several heads in parallel with separate linear projections, letting each head focus on different aspects (e.g., local vs. long-range patterns).\nIf d_model is the model dimension and we have h heads, each head uses d_k = d_model / h. There are learned projections Wq, Wk, Wv and an output projection Wo.\nimport torch import torch.nn as nn class MultiHeadAttention(nn.Module): def __init__(self, d_model, num_heads): super().__init__() assert d_model % num_heads == 0, \u0026#34;d_model must be divisible by num_heads\u0026#34; self.num_heads = num_heads self.d_k = d_model // num_heads # Linear layers to project inputs to queries, keys, values self.Wq = nn.Linear(d_model, d_model) self.Wk = nn.Linear(d_model, d_model) self.Wv = nn.Linear(d_model, d_model) # Output linear layer self.Wo = nn.Linear(d_model, d_model) def forward(self, Q, K, V, mask=None): batch_size = Q.size(0) # 1) Linear projections Q = self.Wq(Q) # (batch, seq_len, d_model) K = self.Wk(K) V = self.Wv(V) # 2) Split into heads by reshaping Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) # Now Q, K, V are (batch, heads, seq_len, d_k) # 3) Apply scaled dot-product attention on each head # Combine batch and heads dims for efficiency Q_flat = Q.reshape(-1, Q.size(2), self.d_k) # (batch*heads, seq_q, d_k) K_flat = K.reshape(-1, K.size(2), self.d_k) V_flat = V.reshape(-1, V.size(2), self.d_k) if mask is not None: # Expand mask to match the number of heads mask = mask.unsqueeze(1).repeat(1, self.num_heads, 1, 1) mask = mask.reshape(-1, mask.size(-2), mask.size(-1)) context, attn_weights = scaled_dot_product_attention(Q_flat, K_flat, V_flat, mask) # 4) Concatenate heads context = context.view(batch_size, self.num_heads, -1, self.d_k) context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k) # 5) Final linear layer output = self.Wo(context) # (batch, seq_len, d_model) return output, attn_weights The tensor reshaping is the trickiest part. After splitting into heads, I flatten (batch, heads) to reuse scaled_dot_product_attention, then reshape back and concatenate. Use .contiguous() before .view after a transpose.\nPositional Encoding Pure attention has no notion of order. We add positional encodings to token embeddings. The original paper used fixed sinusoidal encodings:\n[\\text{PE}(pos, 2i) = \\sin\\bigl(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\bigr),\\quad \\text{PE}(pos, 2i+1) = \\cos\\bigl(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\bigr)]\nimport torch import torch.nn as nn import math class PositionalEncoding(nn.Module): def __init__(self, d_model, max_len=5000): super().__init__() pe = torch.zeros(max_len, d_model) position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # (max_len, 1) div_term = torch.exp(torch.arange(0, d_model, 2).float() * (math.log(10000.0) / d_model)) pe[:, 0::2] = torch.sin(position * div_term) # even dims pe[:, 1::2] = torch.cos(position * div_term) # odd dims pe = pe.unsqueeze(0) # (1, max_len, d_model) # Register as buffer so it’s saved with the model but not trainable self.register_buffer(\u0026#39;pe\u0026#39;, pe) def forward(self, x): # x: (batch, seq_len, d_model) seq_len = x.size(1) return x + self.pe[:, :seq_len, :] Shaping pe as (1, max_len, d_model) lets it broadcast-add to a batch of embeddings. Register it as a buffer so it moves with model.to(device) but isn’t updated by the optimizer.\nPutting It Together: Encoder Layer Each encoder layer does:\nSelf-Attention + Add \u0026amp; Norm: x \u0026lt;- LayerNorm(x + MHA(x, x, x)) Feed-Forward + Add \u0026amp; Norm: x \u0026lt;- LayerNorm(x + FFN(x)) import torch.nn as nn class EncoderLayer(nn.Module): def __init__(self, d_model, num_heads, dim_ff, dropout=0.1): super().__init__() self.self_attn = MultiHeadAttention(d_model, num_heads) self.norm1 = nn.LayerNorm(d_model) self.norm2 = nn.LayerNorm(d_model) self.dropout = nn.Dropout(dropout) # Feed-forward network (position-wise) self.fc1 = nn.Linear(d_model, dim_ff) self.fc2 = nn.Linear(dim_ff, d_model) def forward(self, x, mask=None): # 1) Multi-head self-attention attn_output, _ = self.self_attn(x, x, x, mask) x = x + self.dropout(attn_output) # Residual x = self.norm1(x) # 2) Feed-forward network ff_output = self.fc2(self.dropout(torch.relu(self.fc1(x)))) x = x + self.dropout(ff_output) # Residual x = self.norm2(x) return x If x is (batch, seq_len, d_model), the module preserves shapes. Dropout is applied after attention and inside the feed-forward path.\nReflections Understand by implementing: Writing code forces you to handle details (tensor shapes, masking). Quick dummy tests are invaluable. Scaling matters: Dividing by (\\sqrt{d_k}) keeps softmax well-behaved; omitting it caused overly peaky distributions in my tests. Reshaping is tricky but logical: Splitting/combining heads is the most error-prone part; draw shapes while coding. Positional encoding is neat: The sine/cosine patterns give a smooth notion of distance; the model infers relative order. Modularity: Once MultiHeadAttention and EncoderLayer work, stacking them is straightforward and encourages experimentation. Overall, the magic is that self-attention lets each position flexibly gather information from anywhere else, and multi-head attention enriches that process. If you’re learning this, don’t just read the paper—try coding the pieces. Small confusions often turn into big insights once resolved.\n","permalink":"http://localhost:1313/posts/attention_transformer_blog/","summary":"\u003cp\u003eTransformers have transformed the field of sequence modeling by replacing recurrent nets with self-attention. I was curious about how this works under the hood, so I dove into the “Attention Is All You Need” paper and built the components from scratch in PyTorch. In this post I share my understanding and hands-on insights. I’ll cover the motivation behind Transformers, break down the core ideas, and show PyTorch snippets for the key modules: scaled dot-product attention, multi-head attention, positional encoding, and an encoder layer.\u003c/p\u003e","title":"Transformers from Scratch: Implementing Attention Is All You Need"}]