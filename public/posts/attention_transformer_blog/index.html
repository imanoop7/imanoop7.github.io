<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Transformers from Scratch: Implementing Attention Is All You Need | Anoop Maurya</title><meta name=keywords content="transformers,pytorch,nlp"><meta name=description content="Beginner-to-intermediate guide to the Transformer with PyTorch code: attention, multi-head attention, positional encoding, and a minimal encoder."><meta name=author content="Anoop Maurya"><link rel=canonical href=https://imanoop7.github.io/posts/attention_transformer_blog/><link crossorigin=anonymous href=/assets/css/stylesheet.b8c5fe199b6ed7d82f8d796745cb75f1db5c0e80089bf3c900c156f7941e8838.css integrity="sha256-uMX+GZtu19gvjXlnRct18dtcDoAIm/PJAMFW95QeiDg=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/attention_transformer_blog/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="http://localhost:1313/posts/attention_transformer_blog/"><meta property="og:site_name" content="Anoop Maurya"><meta property="og:title" content="Transformers from Scratch: Implementing Attention Is All You Need"><meta property="og:description" content="Beginner-to-intermediate guide to the Transformer with PyTorch code: attention, multi-head attention, positional encoding, and a minimal encoder."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-09-15T12:00:00+00:00"><meta property="article:modified_time" content="2025-09-15T12:00:00+00:00"><meta property="article:tag" content="Transformers"><meta property="article:tag" content="Pytorch"><meta property="article:tag" content="Nlp"><meta name=twitter:card content="summary"><meta name=twitter:title content="Transformers from Scratch: Implementing Attention Is All You Need"><meta name=twitter:description content="Beginner-to-intermediate guide to the Transformer with PyTorch code: attention, multi-head attention, positional encoding, and a minimal encoder."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"Transformers from Scratch: Implementing Attention Is All You Need","item":"http://localhost:1313/posts/attention_transformer_blog/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Transformers from Scratch: Implementing Attention Is All You Need","name":"Transformers from Scratch: Implementing Attention Is All You Need","description":"Beginner-to-intermediate guide to the Transformer with PyTorch code: attention, multi-head attention, positional encoding, and a minimal encoder.","keywords":["transformers","pytorch","nlp"],"articleBody":"Transformers have transformed the field of sequence modeling by replacing recurrent nets with self-attention. I was curious about how this works under the hood, so I dove into the “Attention Is All You Need” paper and built the components from scratch in PyTorch. In this post I share my understanding and hands-on insights. I’ll cover the motivation behind Transformers, break down the core ideas, and show PyTorch snippets for the key modules: scaled dot-product attention, multi-head attention, positional encoding, and an encoder layer.\nAlong the way I’ll mention a few hiccups and “aha!” moments I had – hopefully making this journey relatable to anyone wrapping their head around this for the first time.\nMotivation Before Transformers, sequence models like RNNs or LSTMs were standard for machine translation or language modeling. These models process data step-by-step, which is hard to parallelize and can struggle with long-range dependencies. The Transformer relies entirely on attention, allowing information from all time steps to be used at once.\nParallelism and efficiency: Self-attention lets us process all tokens at once, so training is much faster on modern hardware. Long-range context: Every token can attend to any other token, capturing long-distance relationships without vanishing gradients. Simplicity of components: The building blocks (attention, feed-forward layers, etc.) are relatively simple operations—feasible to code and inspect directly. Practical success: Many cutting-edge models (BERT, GPT) are based on Transformers, so learning it deeply pays off. Core Concepts The Transformer encoder is built from a few key ideas:\nScaled Dot-Product Attention: Computes attention scores between queries and keys, scales them, and uses softmax to weight values. Multi-Head Attention: Runs several attention “heads” in parallel so the model can jointly attend to different representation subspaces. Positional Encoding: Injects information about token positions, since attention alone is order-agnostic. Feed-Forward Network: A two-layer MLP applied to each position separately and identically. Add \u0026 Norm (Residual + LayerNorm): Residual connections and layer normalization for stable training and easy gradient flow. Stacking several encoder layers (typically 6–12) gives you a Transformer encoder.\nScaled Dot-Product Attention Given queries Q, keys K, and values V, scaled dot-product attention computes a weighted sum of the values, where weights come from the similarity of queries with keys. The scores are divided by (\\sqrt{d_k}) to keep values in a reasonable range.\nHigh level steps for a single query (\\mathbf{q}) and keys (\\mathbf{k}_j):\nCompute raw scores: (\\text{score}_j = \\mathbf{q} \\cdot \\mathbf{k}_j^\\top) Scale: (\\text{score}_j / \\sqrt{d_k}) Optionally mask some scores to (-\\infty) Softmax to probabilities: (\\alpha_j) Weighted sum of values: (\\sum_j \\alpha_j \\mathbf{v}_j) PyTorch implementation:\nimport torch import math def scaled_dot_product_attention(Q, K, V, mask=None): \"\"\" Q, K, V: shape (batch, seq_len, d_k) mask: optional mask tensor of shape (batch, seq_len_q, seq_len_k) Returns: (output, attention_weights) \"\"\" d_k = Q.size(-1) # Compute scaled dot products scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k) # Apply the mask (if provided) by setting scores to large negative where mask==0 if mask is not None: scores = scores.masked_fill(mask == 0, float('-inf')) # Softmax to get attention weights attn_weights = torch.softmax(scores, dim=-1) # Multiply by values output = torch.matmul(attn_weights, V) return output, attn_weights Here, scores has shape (batch, seq_q, seq_k) after the matmul. Multiplying by V then gives an output of shape (batch, seq_q, d_k). For masks (e.g., padding or causal), use masked_fill(-inf) before softmax so those positions get effectively zero weight.\nMulti-Head Attention One attention head gives one context representation. Multi-head attention runs several heads in parallel with separate linear projections, letting each head focus on different aspects (e.g., local vs. long-range patterns).\nIf d_model is the model dimension and we have h heads, each head uses d_k = d_model / h. There are learned projections Wq, Wk, Wv and an output projection Wo.\nimport torch import torch.nn as nn class MultiHeadAttention(nn.Module): def __init__(self, d_model, num_heads): super().__init__() assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\" self.num_heads = num_heads self.d_k = d_model // num_heads # Linear layers to project inputs to queries, keys, values self.Wq = nn.Linear(d_model, d_model) self.Wk = nn.Linear(d_model, d_model) self.Wv = nn.Linear(d_model, d_model) # Output linear layer self.Wo = nn.Linear(d_model, d_model) def forward(self, Q, K, V, mask=None): batch_size = Q.size(0) # 1) Linear projections Q = self.Wq(Q) # (batch, seq_len, d_model) K = self.Wk(K) V = self.Wv(V) # 2) Split into heads by reshaping Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) # Now Q, K, V are (batch, heads, seq_len, d_k) # 3) Apply scaled dot-product attention on each head # Combine batch and heads dims for efficiency Q_flat = Q.reshape(-1, Q.size(2), self.d_k) # (batch*heads, seq_q, d_k) K_flat = K.reshape(-1, K.size(2), self.d_k) V_flat = V.reshape(-1, V.size(2), self.d_k) if mask is not None: # Expand mask to match the number of heads mask = mask.unsqueeze(1).repeat(1, self.num_heads, 1, 1) mask = mask.reshape(-1, mask.size(-2), mask.size(-1)) context, attn_weights = scaled_dot_product_attention(Q_flat, K_flat, V_flat, mask) # 4) Concatenate heads context = context.view(batch_size, self.num_heads, -1, self.d_k) context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k) # 5) Final linear layer output = self.Wo(context) # (batch, seq_len, d_model) return output, attn_weights The tensor reshaping is the trickiest part. After splitting into heads, I flatten (batch, heads) to reuse scaled_dot_product_attention, then reshape back and concatenate. Use .contiguous() before .view after a transpose.\nPositional Encoding Pure attention has no notion of order. We add positional encodings to token embeddings. The original paper used fixed sinusoidal encodings:\n[\\text{PE}(pos, 2i) = \\sin\\bigl(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\bigr),\\quad \\text{PE}(pos, 2i+1) = \\cos\\bigl(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\bigr)]\nimport torch import torch.nn as nn import math class PositionalEncoding(nn.Module): def __init__(self, d_model, max_len=5000): super().__init__() pe = torch.zeros(max_len, d_model) position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # (max_len, 1) div_term = torch.exp(torch.arange(0, d_model, 2).float() * (math.log(10000.0) / d_model)) pe[:, 0::2] = torch.sin(position * div_term) # even dims pe[:, 1::2] = torch.cos(position * div_term) # odd dims pe = pe.unsqueeze(0) # (1, max_len, d_model) # Register as buffer so it’s saved with the model but not trainable self.register_buffer('pe', pe) def forward(self, x): # x: (batch, seq_len, d_model) seq_len = x.size(1) return x + self.pe[:, :seq_len, :] Shaping pe as (1, max_len, d_model) lets it broadcast-add to a batch of embeddings. Register it as a buffer so it moves with model.to(device) but isn’t updated by the optimizer.\nPutting It Together: Encoder Layer Each encoder layer does:\nSelf-Attention + Add \u0026 Norm: x \u003c- LayerNorm(x + MHA(x, x, x)) Feed-Forward + Add \u0026 Norm: x \u003c- LayerNorm(x + FFN(x)) import torch.nn as nn class EncoderLayer(nn.Module): def __init__(self, d_model, num_heads, dim_ff, dropout=0.1): super().__init__() self.self_attn = MultiHeadAttention(d_model, num_heads) self.norm1 = nn.LayerNorm(d_model) self.norm2 = nn.LayerNorm(d_model) self.dropout = nn.Dropout(dropout) # Feed-forward network (position-wise) self.fc1 = nn.Linear(d_model, dim_ff) self.fc2 = nn.Linear(dim_ff, d_model) def forward(self, x, mask=None): # 1) Multi-head self-attention attn_output, _ = self.self_attn(x, x, x, mask) x = x + self.dropout(attn_output) # Residual x = self.norm1(x) # 2) Feed-forward network ff_output = self.fc2(self.dropout(torch.relu(self.fc1(x)))) x = x + self.dropout(ff_output) # Residual x = self.norm2(x) return x If x is (batch, seq_len, d_model), the module preserves shapes. Dropout is applied after attention and inside the feed-forward path.\nReflections Understand by implementing: Writing code forces you to handle details (tensor shapes, masking). Quick dummy tests are invaluable. Scaling matters: Dividing by (\\sqrt{d_k}) keeps softmax well-behaved; omitting it caused overly peaky distributions in my tests. Reshaping is tricky but logical: Splitting/combining heads is the most error-prone part; draw shapes while coding. Positional encoding is neat: The sine/cosine patterns give a smooth notion of distance; the model infers relative order. Modularity: Once MultiHeadAttention and EncoderLayer work, stacking them is straightforward and encourages experimentation. Overall, the magic is that self-attention lets each position flexibly gather information from anywhere else, and multi-head attention enriches that process. If you’re learning this, don’t just read the paper—try coding the pieces. Small confusions often turn into big insights once resolved.\n","wordCount":"1284","inLanguage":"en","datePublished":"2025-09-15T12:00:00Z","dateModified":"2025-09-15T12:00:00Z","author":{"@type":"Person","name":"Anoop Maurya"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/attention_transformer_blog/"},"publisher":{"@type":"Organization","name":"Anoop Maurya","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Anoop Maurya (Alt + H)">Anoop Maurya</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/posts/ title=Posts><span>Posts</span></a></li><li><a href=http://localhost:1313/archives/ title=Archives><span>Archives</span></a></li><li><a href=http://localhost:1313/categories/ title=Categories><span>Categories</span></a></li><li><a href=http://localhost:1313/tags/ title=Tags><span>Tags</span></a></li><li><a href=http://localhost:1313/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=http://localhost:1313/faq/ title=FAQ><span>FAQ</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Transformers from Scratch: Implementing Attention Is All You Need</h1><div class=post-description>Beginner-to-intermediate guide to the Transformer with PyTorch code: attention, multi-head attention, positional encoding, and a minimal encoder.</div><div class=post-meta><span title='2025-09-15 12:00:00 +0000 +0000'>September 15, 2025</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;1284 words&nbsp;·&nbsp;Anoop Maurya</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><ul><li><a href=#motivation>Motivation</a></li><li><a href=#core-concepts>Core Concepts</a></li><li><a href=#scaled-dot-product-attention>Scaled Dot-Product Attention</a></li><li><a href=#multi-head-attention>Multi-Head Attention</a></li><li><a href=#positional-encoding>Positional Encoding</a></li><li><a href=#putting-it-together-encoder-layer>Putting It Together: Encoder Layer</a></li><li><a href=#reflections>Reflections</a></li></ul></li></ul></nav></div></details></div><div class=post-content><p>Transformers have transformed the field of sequence modeling by replacing recurrent nets with self-attention. I was curious about how this works under the hood, so I dove into the “Attention Is All You Need” paper and built the components from scratch in PyTorch. In this post I share my understanding and hands-on insights. I’ll cover the motivation behind Transformers, break down the core ideas, and show PyTorch snippets for the key modules: scaled dot-product attention, multi-head attention, positional encoding, and an encoder layer.</p><p>Along the way I’ll mention a few hiccups and “aha!” moments I had – hopefully making this journey relatable to anyone wrapping their head around this for the first time.</p><h3 id=motivation>Motivation<a hidden class=anchor aria-hidden=true href=#motivation>#</a></h3><p>Before Transformers, sequence models like RNNs or LSTMs were standard for machine translation or language modeling. These models process data step-by-step, which is hard to parallelize and can struggle with long-range dependencies. The Transformer relies entirely on attention, allowing information from all time steps to be used at once.</p><ul><li><strong>Parallelism and efficiency</strong>: Self-attention lets us process all tokens at once, so training is much faster on modern hardware.</li><li><strong>Long-range context</strong>: Every token can attend to any other token, capturing long-distance relationships without vanishing gradients.</li><li><strong>Simplicity of components</strong>: The building blocks (attention, feed-forward layers, etc.) are relatively simple operations—feasible to code and inspect directly.</li><li><strong>Practical success</strong>: Many cutting-edge models (BERT, GPT) are based on Transformers, so learning it deeply pays off.</li></ul><h3 id=core-concepts>Core Concepts<a hidden class=anchor aria-hidden=true href=#core-concepts>#</a></h3><p>The Transformer encoder is built from a few key ideas:</p><ul><li><strong>Scaled Dot-Product Attention</strong>: Computes attention scores between queries and keys, scales them, and uses softmax to weight values.</li><li><strong>Multi-Head Attention</strong>: Runs several attention “heads” in parallel so the model can jointly attend to different representation subspaces.</li><li><strong>Positional Encoding</strong>: Injects information about token positions, since attention alone is order-agnostic.</li><li><strong>Feed-Forward Network</strong>: A two-layer MLP applied to each position separately and identically.</li><li><strong>Add & Norm (Residual + LayerNorm)</strong>: Residual connections and layer normalization for stable training and easy gradient flow.</li></ul><p>Stacking several encoder layers (typically 6–12) gives you a Transformer encoder.</p><h3 id=scaled-dot-product-attention>Scaled Dot-Product Attention<a hidden class=anchor aria-hidden=true href=#scaled-dot-product-attention>#</a></h3><p>Given queries Q, keys K, and values V, scaled dot-product attention computes a weighted sum of the values, where weights come from the similarity of queries with keys. The scores are divided by (\sqrt{d_k}) to keep values in a reasonable range.</p><p>High level steps for a single query (\mathbf{q}) and keys (\mathbf{k}_j):</p><ol><li>Compute raw scores: (\text{score}_j = \mathbf{q} \cdot \mathbf{k}_j^\top)</li><li>Scale: (\text{score}_j / \sqrt{d_k})</li><li>Optionally mask some scores to (-\infty)</li><li>Softmax to probabilities: (\alpha_j)</li><li>Weighted sum of values: (\sum_j \alpha_j \mathbf{v}_j)</li></ol><p>PyTorch implementation:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> math
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>scaled_dot_product_attention</span>(Q, K, V, mask<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Q, K, V: shape (batch, seq_len, d_k)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    mask: optional mask tensor of shape (batch, seq_len_q, seq_len_k)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Returns: (output, attention_weights)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    d_k <span style=color:#f92672>=</span> Q<span style=color:#f92672>.</span>size(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    <span style=color:#75715e># Compute scaled dot products</span>
</span></span><span style=display:flex><span>    scores <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>matmul(Q, K<span style=color:#f92672>.</span>transpose(<span style=color:#f92672>-</span><span style=color:#ae81ff>2</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)) <span style=color:#f92672>/</span> math<span style=color:#f92672>.</span>sqrt(d_k)
</span></span><span style=display:flex><span>    <span style=color:#75715e># Apply the mask (if provided) by setting scores to large negative where mask==0</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> mask <span style=color:#f92672>is</span> <span style=color:#f92672>not</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>        scores <span style=color:#f92672>=</span> scores<span style=color:#f92672>.</span>masked_fill(mask <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>, float(<span style=color:#e6db74>&#39;-inf&#39;</span>))
</span></span><span style=display:flex><span>    <span style=color:#75715e># Softmax to get attention weights</span>
</span></span><span style=display:flex><span>    attn_weights <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>softmax(scores, dim<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    <span style=color:#75715e># Multiply by values</span>
</span></span><span style=display:flex><span>    output <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>matmul(attn_weights, V)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> output, attn_weights
</span></span></code></pre></div><p>Here, <code>scores</code> has shape <code>(batch, seq_q, seq_k)</code> after the matmul. Multiplying by <code>V</code> then gives an output of shape <code>(batch, seq_q, d_k)</code>. For masks (e.g., padding or causal), use <code>masked_fill(-inf)</code> before softmax so those positions get effectively zero weight.</p><h3 id=multi-head-attention>Multi-Head Attention<a hidden class=anchor aria-hidden=true href=#multi-head-attention>#</a></h3><p>One attention head gives one context representation. Multi-head attention runs several heads in parallel with separate linear projections, letting each head focus on different aspects (e.g., local vs. long-range patterns).</p><p>If <code>d_model</code> is the model dimension and we have <code>h</code> heads, each head uses <code>d_k = d_model / h</code>. There are learned projections <code>Wq, Wk, Wv</code> and an output projection <code>Wo</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>MultiHeadAttention</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self, d_model, num_heads):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span><span style=color:#a6e22e>__init__</span>()
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>assert</span> d_model <span style=color:#f92672>%</span> num_heads <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>, <span style=color:#e6db74>&#34;d_model must be divisible by num_heads&#34;</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>num_heads <span style=color:#f92672>=</span> num_heads
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>d_k <span style=color:#f92672>=</span> d_model <span style=color:#f92672>//</span> num_heads
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Linear layers to project inputs to queries, keys, values</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>Wq <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(d_model, d_model)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>Wk <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(d_model, d_model)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>Wv <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(d_model, d_model)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Output linear layer</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>Wo <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(d_model, d_model)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, Q, K, V, mask<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>        batch_size <span style=color:#f92672>=</span> Q<span style=color:#f92672>.</span>size(<span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 1) Linear projections</span>
</span></span><span style=display:flex><span>        Q <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>Wq(Q)  <span style=color:#75715e># (batch, seq_len, d_model)</span>
</span></span><span style=display:flex><span>        K <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>Wk(K)
</span></span><span style=display:flex><span>        V <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>Wv(V)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 2) Split into heads by reshaping</span>
</span></span><span style=display:flex><span>        Q <span style=color:#f92672>=</span> Q<span style=color:#f92672>.</span>view(batch_size, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, self<span style=color:#f92672>.</span>num_heads, self<span style=color:#f92672>.</span>d_k)<span style=color:#f92672>.</span>transpose(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>        K <span style=color:#f92672>=</span> K<span style=color:#f92672>.</span>view(batch_size, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, self<span style=color:#f92672>.</span>num_heads, self<span style=color:#f92672>.</span>d_k)<span style=color:#f92672>.</span>transpose(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>        V <span style=color:#f92672>=</span> V<span style=color:#f92672>.</span>view(batch_size, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, self<span style=color:#f92672>.</span>num_heads, self<span style=color:#f92672>.</span>d_k)<span style=color:#f92672>.</span>transpose(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>        <span style=color:#75715e># Now Q, K, V are (batch, heads, seq_len, d_k)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 3) Apply scaled dot-product attention on each head</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Combine batch and heads dims for efficiency</span>
</span></span><span style=display:flex><span>        Q_flat <span style=color:#f92672>=</span> Q<span style=color:#f92672>.</span>reshape(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, Q<span style=color:#f92672>.</span>size(<span style=color:#ae81ff>2</span>), self<span style=color:#f92672>.</span>d_k)  <span style=color:#75715e># (batch*heads, seq_q, d_k)</span>
</span></span><span style=display:flex><span>        K_flat <span style=color:#f92672>=</span> K<span style=color:#f92672>.</span>reshape(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, K<span style=color:#f92672>.</span>size(<span style=color:#ae81ff>2</span>), self<span style=color:#f92672>.</span>d_k)
</span></span><span style=display:flex><span>        V_flat <span style=color:#f92672>=</span> V<span style=color:#f92672>.</span>reshape(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, V<span style=color:#f92672>.</span>size(<span style=color:#ae81ff>2</span>), self<span style=color:#f92672>.</span>d_k)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> mask <span style=color:#f92672>is</span> <span style=color:#f92672>not</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>            <span style=color:#75715e># Expand mask to match the number of heads</span>
</span></span><span style=display:flex><span>            mask <span style=color:#f92672>=</span> mask<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>1</span>)<span style=color:#f92672>.</span>repeat(<span style=color:#ae81ff>1</span>, self<span style=color:#f92672>.</span>num_heads, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>            mask <span style=color:#f92672>=</span> mask<span style=color:#f92672>.</span>reshape(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, mask<span style=color:#f92672>.</span>size(<span style=color:#f92672>-</span><span style=color:#ae81ff>2</span>), mask<span style=color:#f92672>.</span>size(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        context, attn_weights <span style=color:#f92672>=</span> scaled_dot_product_attention(Q_flat, K_flat, V_flat, mask)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 4) Concatenate heads</span>
</span></span><span style=display:flex><span>        context <span style=color:#f92672>=</span> context<span style=color:#f92672>.</span>view(batch_size, self<span style=color:#f92672>.</span>num_heads, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, self<span style=color:#f92672>.</span>d_k)
</span></span><span style=display:flex><span>        context <span style=color:#f92672>=</span> context<span style=color:#f92672>.</span>transpose(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>)<span style=color:#f92672>.</span>contiguous()<span style=color:#f92672>.</span>view(batch_size, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, self<span style=color:#f92672>.</span>num_heads <span style=color:#f92672>*</span> self<span style=color:#f92672>.</span>d_k)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 5) Final linear layer</span>
</span></span><span style=display:flex><span>        output <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>Wo(context)  <span style=color:#75715e># (batch, seq_len, d_model)</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> output, attn_weights
</span></span></code></pre></div><p>The tensor reshaping is the trickiest part. After splitting into heads, I flatten <code>(batch, heads)</code> to reuse <code>scaled_dot_product_attention</code>, then reshape back and concatenate. Use <code>.contiguous()</code> before <code>.view</code> after a transpose.</p><h3 id=positional-encoding>Positional Encoding<a hidden class=anchor aria-hidden=true href=#positional-encoding>#</a></h3><p>Pure attention has no notion of order. We add positional encodings to token embeddings. The original paper used fixed sinusoidal encodings:</p><p>[\text{PE}(pos, 2i) = \sin\bigl(\frac{pos}{10000^{2i/d_{\text{model}}}}\bigr),\quad \text{PE}(pos, 2i+1) = \cos\bigl(\frac{pos}{10000^{2i/d_{\text{model}}}}\bigr)]</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> math
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>PositionalEncoding</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self, d_model, max_len<span style=color:#f92672>=</span><span style=color:#ae81ff>5000</span>):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span><span style=color:#a6e22e>__init__</span>()
</span></span><span style=display:flex><span>        pe <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>zeros(max_len, d_model)
</span></span><span style=display:flex><span>        position <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>0</span>, max_len, dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>float)<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>1</span>)  <span style=color:#75715e># (max_len, 1)</span>
</span></span><span style=display:flex><span>        div_term <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>exp(torch<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>0</span>, d_model, <span style=color:#ae81ff>2</span>)<span style=color:#f92672>.</span>float() <span style=color:#f92672>*</span> (math<span style=color:#f92672>.</span>log(<span style=color:#ae81ff>10000.0</span>) <span style=color:#f92672>/</span> d_model))
</span></span><span style=display:flex><span>        pe[:, <span style=color:#ae81ff>0</span>::<span style=color:#ae81ff>2</span>] <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>sin(position <span style=color:#f92672>*</span> div_term)  <span style=color:#75715e># even dims</span>
</span></span><span style=display:flex><span>        pe[:, <span style=color:#ae81ff>1</span>::<span style=color:#ae81ff>2</span>] <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>cos(position <span style=color:#f92672>*</span> div_term)  <span style=color:#75715e># odd dims</span>
</span></span><span style=display:flex><span>        pe <span style=color:#f92672>=</span> pe<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>0</span>)  <span style=color:#75715e># (1, max_len, d_model)</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Register as buffer so it’s saved with the model but not trainable</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>register_buffer(<span style=color:#e6db74>&#39;pe&#39;</span>, pe)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        <span style=color:#75715e># x: (batch, seq_len, d_model)</span>
</span></span><span style=display:flex><span>        seq_len <span style=color:#f92672>=</span> x<span style=color:#f92672>.</span>size(<span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> x <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>pe[:, :seq_len, :]
</span></span></code></pre></div><p>Shaping <code>pe</code> as <code>(1, max_len, d_model)</code> lets it broadcast-add to a batch of embeddings. Register it as a buffer so it moves with <code>model.to(device)</code> but isn’t updated by the optimizer.</p><h3 id=putting-it-together-encoder-layer>Putting It Together: Encoder Layer<a hidden class=anchor aria-hidden=true href=#putting-it-together-encoder-layer>#</a></h3><p>Each encoder layer does:</p><ol><li>Self-Attention + Add & Norm: <code>x &lt;- LayerNorm(x + MHA(x, x, x))</code></li><li>Feed-Forward + Add & Norm: <code>x &lt;- LayerNorm(x + FFN(x))</code></li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>EncoderLayer</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self, d_model, num_heads, dim_ff, dropout<span style=color:#f92672>=</span><span style=color:#ae81ff>0.1</span>):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span><span style=color:#a6e22e>__init__</span>()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>self_attn <span style=color:#f92672>=</span> MultiHeadAttention(d_model, num_heads)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>norm1 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>LayerNorm(d_model)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>norm2 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>LayerNorm(d_model)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>dropout <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Dropout(dropout)
</span></span><span style=display:flex><span>        <span style=color:#75715e># Feed-forward network (position-wise)</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>fc1 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(d_model, dim_ff)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>fc2 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(dim_ff, d_model)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x, mask<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>        <span style=color:#75715e># 1) Multi-head self-attention</span>
</span></span><span style=display:flex><span>        attn_output, _ <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>self_attn(x, x, x, mask)
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> x <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>dropout(attn_output)  <span style=color:#75715e># Residual</span>
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>norm1(x)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 2) Feed-forward network</span>
</span></span><span style=display:flex><span>        ff_output <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>fc2(self<span style=color:#f92672>.</span>dropout(torch<span style=color:#f92672>.</span>relu(self<span style=color:#f92672>.</span>fc1(x))))
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> x <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>dropout(ff_output)  <span style=color:#75715e># Residual</span>
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>norm2(x)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> x
</span></span></code></pre></div><p>If <code>x</code> is <code>(batch, seq_len, d_model)</code>, the module preserves shapes. Dropout is applied after attention and inside the feed-forward path.</p><h3 id=reflections>Reflections<a hidden class=anchor aria-hidden=true href=#reflections>#</a></h3><ul><li><strong>Understand by implementing</strong>: Writing code forces you to handle details (tensor shapes, masking). Quick dummy tests are invaluable.</li><li><strong>Scaling matters</strong>: Dividing by (\sqrt{d_k}) keeps softmax well-behaved; omitting it caused overly peaky distributions in my tests.</li><li><strong>Reshaping is tricky but logical</strong>: Splitting/combining heads is the most error-prone part; draw shapes while coding.</li><li><strong>Positional encoding is neat</strong>: The sine/cosine patterns give a smooth notion of distance; the model infers relative order.</li><li><strong>Modularity</strong>: Once <code>MultiHeadAttention</code> and <code>EncoderLayer</code> work, stacking them is straightforward and encourages experimentation.</li></ul><p>Overall, the magic is that self-attention lets each position flexibly gather information from anywhere else, and multi-head attention enriches that process. If you’re learning this, don’t just read the paper—try coding the pieces. Small confusions often turn into big insights once resolved.</p></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/transformers/>Transformers</a></li><li><a href=http://localhost:1313/tags/pytorch/>Pytorch</a></li><li><a href=http://localhost:1313/tags/nlp/>Nlp</a></li></ul><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Transformers from Scratch: Implementing Attention Is All You Need on x" href="https://x.com/intent/tweet/?text=Transformers%20from%20Scratch%3a%20Implementing%20Attention%20Is%20All%20You%20Need&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fattention_transformer_blog%2f&amp;hashtags=transformers%2cpytorch%2cnlp"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Transformers from Scratch: Implementing Attention Is All You Need on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fattention_transformer_blog%2f&amp;title=Transformers%20from%20Scratch%3a%20Implementing%20Attention%20Is%20All%20You%20Need&amp;summary=Transformers%20from%20Scratch%3a%20Implementing%20Attention%20Is%20All%20You%20Need&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2fattention_transformer_blog%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Transformers from Scratch: Implementing Attention Is All You Need on reddit" href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fposts%2fattention_transformer_blog%2f&title=Transformers%20from%20Scratch%3a%20Implementing%20Attention%20Is%20All%20You%20Need"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Transformers from Scratch: Implementing Attention Is All You Need on facebook" href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fposts%2fattention_transformer_blog%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Transformers from Scratch: Implementing Attention Is All You Need on whatsapp" href="https://api.whatsapp.com/send?text=Transformers%20from%20Scratch%3a%20Implementing%20Attention%20Is%20All%20You%20Need%20-%20http%3a%2f%2flocalhost%3a1313%2fposts%2fattention_transformer_blog%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Transformers from Scratch: Implementing Attention Is All You Need on telegram" href="https://telegram.me/share/url?text=Transformers%20from%20Scratch%3a%20Implementing%20Attention%20Is%20All%20You%20Need&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fattention_transformer_blog%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Transformers from Scratch: Implementing Attention Is All You Need on ycombinator" href="https://news.ycombinator.com/submitlink?t=Transformers%20from%20Scratch%3a%20Implementing%20Attention%20Is%20All%20You%20Need&u=http%3a%2f%2flocalhost%3a1313%2fposts%2fattention_transformer_blog%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>Anoop Maurya</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>